{"title": "Delta SharingによるDatabricks間でのファイルやノートブックの共有", "text": "前見た時には、Delta Sharingで共有できるのはDeltaテーブルだけだったのですが、今日になってファイルやノートブックも共有できることに気づきました(遅い)。\n\nhttps://docs.databricks.com/ja/data-sharing/index.html\n\nいずれもDatabricks間での共有ですが、クラウドやアカウントを越えて共有できるのでユースケースは結構あると思います。\n\n# 共有の設定\n\n共有側(プロバイダー)をワークスペースA、利用側(受信者)をワークスペースBとします。\n\n## 共有の作成：ワークスペースAでの作業\n\nカタログエクスプローラで共有を作成します。Delta Sharingの**自分が共有**にアクセスし、**データを共有**をクリックして新規共有を作成します。\n![Screenshot 2023-12-27 at 18.32.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1c51d2d9-5dca-0912-5b86-c3aecb8473b7.png)\n\n**アセットを追加**をクリックします。左のペインからテーブルやボリュームを選択して保存します。\n![Screenshot 2023-12-27 at 18.34.15.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c21a416d-adcf-cd65-ceb7-e21791254abb.png)\n\nこれでテーブルとボリュームが共有に追加されました。\n![Screenshot 2023-12-27 at 18.35.00.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bcef94d3-6280-4381-6ed3-4c4a318679dc.png)\n\nさらに右上の**アセットを管理**をクリックし、**ノートブックファイルを追加します**を選択します。\n![Screenshot 2023-12-27 at 18.35.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9b032c48-4776-f84e-b18c-0caabd3dfdd5.png)\n\nノートブックを選択して保存します。\n![Screenshot 2023-12-27 at 18.36.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2641f003-1174-d6f8-450f-b8520e0e2022.png)\n![Screenshot 2023-12-27 at 18.36.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/227ee433-a4ec-9b2a-07a5-1f36c3ee772a.png)\n\n## 受信者の作成：ワークスペースBでの作業\n\n別のクラウドやアカウントにあるDatabricksと共有する際には、受信側の[共有識別子](https://docs.databricks.com/ja/data-sharing/recipient.html#get-access-in-the-databricks-to-databricks-model)が必要となります。共有識別子は`<cloud>:<region>:<uuid>`の形式となっており、以前は特定するのが面倒でしたが今では受信側のカタログエクスプローラで容易にコピーできます。\n\nワークスペースBのカタログエクスプローラにアクセスし、**Delta Sharing > 自分と共有**にアクセスします。画面上部に**共有識別子**のコピーボタンがあるのでこれをクリックしてコピーします。\n![Screenshot 2023-12-27 at 18.40.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e2419d62-f2ca-d95e-89d9-8bb9c291e389.png)\n\n## 受信者の作成：ワークスペースAでの作業\n\n提供側の**Delta Sharing > 自分が共有**にアクセスし、上で作成した共有オブジェクトにアクセスし、**受信者**タブをクリックします。**受信者を追加**をクリックします。\n![Screenshot 2023-12-27 at 18.43.30.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/60c98e76-af9e-74ae-3eb2-d1d36910931c.png)\n\n**+ 新規受信者を作成** をクリックします。\n![Screenshot 2023-12-27 at 18.43.58.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e9bd93e8-98a7-8d25-1f75-4c4f9b004b3d.png)\n\n受信者名を入力し、上でコピーした共有識別子を貼り付けて、**受信者を作成および追加**をクリックします。\n![Screenshot 2023-12-27 at 18.44.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a2ea7e0c-d943-b9b9-aefc-6d2752b2bd4c.png)\n\n**追加**をクリックします。\n![Screenshot 2023-12-27 at 18.46.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0caa8dee-4800-bc1b-7d1b-6a45ddbee653.png)\n\nこれで提供者側の作業は完了です。\n![Screenshot 2023-12-27 at 18.46.41.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e1859d29-6ea4-7767-4298-027dbe673843.png)\n\n# 受信者側の作業\n\n以降はワークスペースBでの作業となります。\n\n## ファイル(ボリューム)へのアクセス\n\nワークスペースBのカタログエクスプローラにアクセスし、**Delta Sharing > 自分と共有**にアクセスします。プロバイダーに提供者一覧が表示されます。提供者名は提供側に確認してください。\n![Screenshot 2023-12-27 at 18.50.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7a90f612-e3ba-0579-3f13-69e98733717d.png)\n\n提供側の共有が表示されます。カタログを作成をクリックします。\n![Screenshot 2023-12-27 at 18.51.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c01f3663-54f1-9d74-d2a0-3efeee6dcd30.png)\n\n受信者側におけるカタログ名を入力し、作成をクリックします。\n![Screenshot 2023-12-27 at 18.52.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/27f68922-e6d1-c9d1-3306-3c2bb2b538f4.png)\n\nこれで共有されたテーブルやボリュームにアクセスできるようになります。\n![Screenshot 2023-12-27 at 18.53.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/41f68da6-769c-85dd-b89e-b344605007d2.png)\n![Screenshot 2023-12-27 at 18.53.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3ec94dfb-35d7-f002-d8bd-3742a33a9b36.png)\n\n## ノートブックへのアクセス\n\n上で作成したカタログにアクセスし、**その他のアセット**をクリックすると共有されているノートブックが表示されます。\n![Screenshot 2023-12-27 at 18.55.17.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c4d0d6a2-9b41-deb3-4144-eacb60322531.png)\n\nクリックすることで内容を確認することができ、右上の**クローン作成**で任意の場所にコピーすることができます。\n![Screenshot 2023-12-27 at 18.56.06.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/aef7adfb-02ec-d097-9ca4-87910cb8bc44.png)\n\nテーブルやファイルを共有する際には、サンプルとなるノートブックがあった方がコラボレーションが円滑になるかと思います。是非ご活用ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksのAI PlaygroundによるLLMの動作確認", "text": "こちらのプレビュー機能です。\n\nhttps://docs.databricks.com/ja/machine-learning/foundation-models/supported-models.html#chat-with-supported-llms-using-ai-playground\n\n:::note info\n**プレビュー**\n本機能は[パブリックプレビュー](https://docs.databricks.com/release-notes/release-types.html)です。記事の執筆時点では日本リージョンではまだ利用できません。\n:::\n\nDatabricks Foundation Models APIで利用できるLLMの動作確認をクイックに行いたい場合に活用できるのがAI Playgroundです。\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IQtxL7qmi_8?si=n_aq2nFzzUrsNAHQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n有効化されているワークスペースであれば、サイドメニューに**Playground**が表示されます。ロボ。\n![Screenshot 2023-12-24 at 10.44.06.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f18b519c-1fdc-5a66-fc5a-a1ac2d24797b.png)\n\n画面構成はこのようになっています。\n![Screenshot 2023-12-24 at 10.44.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bd0fe7d7-fc71-1f24-f1d1-9ac5edd52de0.png)\n\n左上のドロップダウンリストからLLMを選択します。\n![Screenshot 2023-12-24 at 10.45.21.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1f89a5be-c0fc-edb1-687a-5c9cfa4ec170.png)\n\n右上の設定ボタンでパラメータを変更できます。\n![Screenshot 2023-12-24 at 10.45.53.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/836216d2-9312-7b45-2d40-fd5e428be4d6.png)\n\n**+** ボタンを押すとチャットウィンドウが追加できるので、レスポンスの比較を行うことができます。\n![Screenshot 2023-12-24 at 10.46.22.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1065126b-65f2-e039-608c-d3f30d7f38a8.png)\n\n**Sync chats**にチェックがついていれば、プロンプトが両方のウィンドウに反映されるので、容易にレスポンスを比較できます。便利。\n![Screenshot 2023-12-24 at 10.47.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5fd6e969-f4d8-3a65-b6f5-1e4667c8064c.png)\n\nウィンドウ右上の **< >** をクリックすると、モデルエンドポイントへのリクエストのcurlコマンドが表示されます。プログラムに組み込むなどのケースで役立ちそうです。\n![Screenshot 2023-12-24 at 10.49.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e4206923-9d53-7a76-c77a-b7a80350dada.png)\n\nそして、**System Prompt**は編集できます。以下のようにレスポンスを日本語にすることも可能です。\n\n> あなたはDatabricksのエキスパートです。日本語でレスポンスをしてください。\n\n![Screenshot 2023-12-24 at 10.51.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6408e890-98d7-745c-7208-d8ffb12ca380.png)\n\nLLMを用いたシステムの構築は自然言語による試行錯誤が多く発生することになるかと思います。期待した挙動をさせるにはどのようなプロンプトが良いのかを検証するには便利なツールかと思います。日本で利用できるようになりましたら是非お試しください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "ZealのCO-ODEデータを用いたDatabricksにおけるデータ分析", "text": "Zeal様がDatabricksマーケットプレイスのプロバイダーになることが、こちらのニュースリリースで発表されました。\n\nhttps://www.zdh.co.jp/topics/20230914/\n\nこちらのデータセットです。\n\nhttps://www.zdh.co.jp/products-services/external-data/co-ode/\n\n> オープンデータや外部データを収集・加工して配信しているサービス\n> ～自社データ×外部データでデータ分析の高度化を実現～\n自社データと外部データを掛け合わせると、要因分析・原因分析や将来予測を立てやすくなります。\n\n![Screenshot 2023-12-22 at 16.28.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d892b0d7-3d1b-c719-b0ce-75c5ea479ab1.png)\n\nDatabricksマーケットプレイスからアクセスを申請します。申請する前に[こちら](https://www.zdh.co.jp/products-services/external-data/co-ode/)で料金体系をご確認ください。\n\n承認されると以下のようなメールが届きます。\n![Screenshot 2023-12-22 at 14.19.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/20da0dc9-3fc4-c145-f14d-0c4ec4bf1345.png)\n\nマーケットプレイスの画面では**即時アクセス権を取得**ボタンが表示されます。\n![Screenshot 2023-12-22 at 14.19.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/81f6c5f0-dfcd-703e-af5a-64779bca5f28.png)\n\n取り込む際にはカタログ名を指定します。\n![Screenshot 2023-12-22 at 14.24.28.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/34bdb7be-a9cf-1e27-0d36-2be465deaa77.png)\n\n取り込みが完了すると、カタログエクスプローラでテーブルにアクセスできるようになります。\n![Screenshot 2023-12-22 at 14.25.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/13142da7-a294-bd30-27db-557edcd8f297.png)\n\n日本のデータなのが嬉しいですね。\n![Screenshot 2023-12-22 at 14.25.30.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/925aaa48-7e52-4461-169c-962bfd343ebd.png)\n\n今回はこちらの人口データを使います。\n\nhttps://marketplace.databricks.com/details/bc3c9f86-6797-484e-82e5-016f09de8b7b/Zeal-Corporation_-\n\nノートブックで確認します。\n```sql\nSELECT * FROM zeal_corporation_population.coode_sample.population_2020;\n```\n\n[プロファイラ](https://docs.databricks.com/ja/visualizations/index.html#create-a-new-data-profile)でデータの傾向を把握します。\n![Screenshot 2023-12-22 at 16.52.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/75610d44-2568-c4f1-914d-52e5d21eb401.png)\n\n都道府県は1つしか無いので2020年時点の東京都のみのデータであることがわかります。その他の列を見ていくと、世帯や人口、職業などに関する定量データが含まれていることがわかります。\n\n今回はクイックに[Lakeview](https://docs.databricks.com/ja/dashboards/lakeview.html)でダッシュボードを作ります。ところどころ[日本語で指示](https://qiita.com/taka_yayoi/items/4b3c4fe62e183cdf7d84)したりしてます。\n![Screenshot 2023-12-22 at 16.54.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/460ca005-e662-f779-d21c-5e25b7583b11.png)\n\n上の二つのグラフからは世田谷区や品川区の人口増加が著しいことがわかります。特に増減数のグラフからは23区の増加とそれ以外の地域の減少が顕著であることがわかります。\n\n右下のグラフでは、人口数の少ない地域での高齢化を確認することができまます。こうなると3年後の今年のデータも見てみたくなりますね。\n![chart (1).png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5e3db7c0-e6cc-07ea-8665-187dfa2313fb.png)\n\n分析のエクササイズには適したデータだと思います。この他のデータも見てみようと思いますし、データエンジニアリングして更なる観点で分析したいとも思いました。是非活用をご検討ください！詳細は[こちらのページ](https://www.zdh.co.jp/products-services/external-data/co-ode/)下部にあるお問い合わせフォームからお願いします。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Azure DatabricksにおけるUnity Catalog有効化のウォークスルー", "text": "以前、こちらの記事を書きましたが、アカウントストレージやマネージドIDのセットアップ手順を端折ってましたので、きちんとウォークスルーします。自分のAzureの知識チェックも兼ねます。\n\nhttps://qiita.com/taka_yayoi/items/9b56995b979fa4e32b4e\n\nこちらの記事も参考になります。\n\nhttps://qiita.com/ryoma-nagata/items/25bcf9e89940d88239cb\n\nなお、最近ではUnity Catalogは自動で有効化されるようになっています。こちらでは手動での有効化が必要な場合の手順をまとめています。\n\nhttps://learn.microsoft.com/ja-jp/azure/databricks/data-governance/unity-catalog/get-started#enablement\n\n> Databricks は、2023 年 11 月 9 日、アカウント全体で順次ロールアウトする Unity Catalog の新しいワークスペースの自動有効化を開始しました。\n\n# Unity Catalogの有効化\n\nマニュアルはこちらにありますが、字が多くて正直取っ付きにくいです。なので、可能な限りスクリーンショットで説明していきます。\n\nhttps://learn.microsoft.com/ja-jp/azure/databricks/data-governance/unity-catalog/create-metastore\n\n全体的な流れは以下の通りです。\n\n1. Azure Databricksワークスペースの作成\n1. Unity Catalog用ストレージアカウント、コンテナーの作成\n1. Unity Catalog用マネージドIDの作成\n1. DatabricksアカウントコンソールでのUnity Catalogメタストアの作成\n1. メタストアをAzure Databricksワークスペースにアタッチ\n1. 動作確認\n\n:::note warn\n**重要！**\nAzure DatabricksワークスペースとUnity Catalogメタストアは同じリージョンである必要があります。また、ストレージアカウントもメタストアと同じリージョンにすることが推奨となります。\n:::\n\n# Azure Databricksワークスペースの作成\n\nAzure PortalでDatabricksを検索して、数クリックで作成できます。今回はJapan Westでワークスペースを作成しました。\n\nhttps://learn.microsoft.com/ja-jp/azure/databricks/getting-started/\n\n![Screenshot 2023-12-22 at 10.15.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/143062f6-abc2-6126-e7de-793f6abc8318.png)\n\n# Unity Catalog用ストレージアカウント、コンテナーの作成\n\n**ストレージアカウントとは？**\n\n> AWSにおけるS3バケットのようなもの。\n\n**コンテナーとは？**\n\n> バケットに作るディレクトリ。\n\n## ストレージアカウントの作成\n\nAzure Portalから**ストレージアカウント**にアクセスします。**+ 作成** をクリックします。\n![Screenshot 2023-12-22 at 10.15.21.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a75aaedd-fe5b-a0d4-8cc1-8ecd79afeb3c.png)\n\nリソースグループを選択し、ストレージアカウント名を入力します。地域はワークスペースと同じにしましょう。パフォーマンスは性能要件とコスト見合いで選択します。ここでは**Standard**にします。\n![Screenshot 2023-12-22 at 10.24.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/89bcc9d9-ce4e-dab0-da73-a6ccf319dd0b.png)\n\n**階層型名前空間**の階層型名前空間を有効にするにチェックをつけます。\n![Screenshot 2023-12-22 at 11.00.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/194d2e18-f8e7-653c-8f98-993aff610106.png)\n\n他はデフォルトのままにします。\n![Screenshot 2023-12-22 at 10.28.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7f1f3324-5b22-ba2e-3590-138f0053de91.png)\n![Screenshot 2023-12-22 at 10.28.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c7f33cbf-b839-7e84-4654-d0e92c93b92d.png)\n![Screenshot 2023-12-22 at 10.29.01.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/872de93c-d370-649b-aa18-d74aad8e92bd.png)\n![Screenshot 2023-12-22 at 10.29.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f7e9dce5-4fcf-eb4f-88e1-de5cd4e206c2.png)\n\n設定を確認してストレージアカウントを作成します。\n![Screenshot 2023-12-22 at 11.00.45.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5f353e1f-2323-7594-6a4f-64700c5924b3.png)\n\n![Screenshot 2023-12-22 at 10.29.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/254023da-f9f9-7884-033e-8d47def3c91f.png)\n\n## コンテナーの作成\n\n作成したストレージアカウントにアクセスします。**コンテナー**をクリックして **+コンテナー** をクリックします。名前をつけて**作成**をクリックします。\n![Screenshot 2023-12-22 at 10.31.14.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b51ad022-aaad-6dc8-5a5b-5df5f32a82f7.png)\n![Screenshot 2023-12-22 at 10.31.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/069b06e1-7c37-6b31-678c-f848508c5a37.png)\n\n作成したストレージアカウント、コンテナ、パスを`<コンテナ名>@<ストレージアカウント名>.dfs.core.windows.net/<パス>`の形式でメモしておきます。\n\n# Unity Catalog用マネージドIDの作成\n\n**マネージドIDとは？**\n\n> サービスプリンシパル(人に紐づかないID)で、Unity Catalogがユーザーに代わってストレージアカウントにアクセスするために使用する。\n\nhttps://learn.microsoft.com/ja-jp/azure/databricks/data-governance/unity-catalog/azure-managed-identities#config-managed-id\n\n## マネージドIDの作成\n\nAzure Portalの **+リソースの作成** をクリックします。\n![Screenshot 2023-12-22 at 11.11.41.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a54de73e-2224-2b64-9c4d-8817efe2b733.png)\n\nAzure Databricksで検索して表示される**Access Connector for Azure Databricks**を選択します。\n![Screenshot 2023-12-22 at 11.13.13.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/112e7be0-3eea-eae2-9d3d-23660c405d46.png)\n\n**作成**をクリックします。\n![Screenshot 2023-12-22 at 11.14.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/23bd3155-6406-d2e4-37cb-66a6ae6358f2.png)\n\n名前をつけて、リージョンを選択します。リージョンはストレージアカウント(とDatabricksワークスペース)と同じにします。**確認と作成**をクリックします。\n![Screenshot 2023-12-22 at 11.16.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1fdf6a6a-fafd-fa21-c103-afbac33bdd72.png)\n\n**作成**をクリックします。\n![Screenshot 2023-12-22 at 11.17.04.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/4d1e9ba9-22a9-9bcc-e424-c7fb18f1e75b.png)\n\n**リソースに移動**をクリックします。\n![Screenshot 2023-12-22 at 11.17.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a7368079-9e6a-d5bd-ea50-25f22d610c6b.png)\n\n**リソースID**をコピーしておきます。カーソルを移動するとコピーアイコンが表示されます。\n![Screenshot 2023-12-22 at 11.18.35.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e78ace0b-a04a-b888-d830-11c3e03b0a93.png)\n\n## マネージドIDにストレージアカウントへのアクセスを許可\n\n[ストレージアカウントの作成](#ストレージアカウントの作成)で作成したストレージアカウントにアクセスします。**アクセス制御**をクリックし **+追加** をクリックして**ロールの割り当ての追加**を選択します。\n![Screenshot 2023-12-22 at 11.36.15.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/63434f78-3681-a7d7-a0ee-4cdbc3b2da96.png)\n\n`ストレージ BLOB データ共同作成者`で検索して選択します。**次へ**をクリックします。\n![Screenshot 2023-12-22 at 11.39.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e6ce4ce4-691d-2e59-ef1b-d96f5a894ed2.png)\n\nアクセスの割り当て先で**マネージドID**を選択し、**+メンバーを選択**をクリックします。\n![Screenshot 2023-12-22 at 11.41.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/237c8343-793d-23e0-d6ff-4187b1ece682.png)\n\n**Azure Databricks 用のアクセス コネクタ**を選択し、[マネージドIDの作成](#マネージドidの作成)で作成したマネージドIDを選択して**選択**をクリックします。\n![Screenshot 2023-12-22 at 11.49.39.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8471393f-479e-7208-76bc-233a1647c57b.png)\n\n**レビューと割り当て**をクリックします。\n![Screenshot 2023-12-22 at 11.50.16.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bc2b102f-a067-dd72-58f9-0d5ccef157c5.png)\n\nもう一度**レビューと割り当て**をクリックします。\n![Screenshot 2023-12-22 at 11.50.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8538eee0-eead-6f5a-5541-1fb9e636dd5e.png)\n\n## マネージドIDにファイルイベントへのアクセス権を付与\n\nマネージドIDにファイルイベントへのアクセスを許可することで、ファイル処理の効率が向上します。\n\n上の手順と同様にマネージドIDに`ストレージ キュー データ共同作成者`を割り当てます。\n![Screenshot 2023-12-22 at 11.53.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/01fd6e9c-fc3f-885d-a981-867b0d986349.png)\n\n## ユーザーに代わってAzure Databricksにファイルイベントを構成するアクセス権を付与\n\n**リソースグループとは？**\n\n> 名前の通り、Azureのリソースをグルーピングできる機能。リソースグループを削除すると、そこに属するすべてのリソースが削除される。[参考記事](https://tech-blog.cloud-config.jp/2019-06-06-evening-azure-challenge-002/)。\n\nこの手順により、Azure Databricksでファイルイベントを自動的に設定できます。\n\n上の手順と同様にマネージドIDに`ストレージ アカウント共同作成者`を割り当てます。\n![Screenshot 2023-12-22 at 11.56.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c5c00dd8-42f3-daad-b9ef-797f3aed43b9.png)\n\nストレージアカウントが属するリソースグループにアクセスします。\n![Screenshot 2023-12-22 at 12.02.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3cd3319e-db57-cdf3-166c-35d42fe6e28b.png)\n\n**アクセス制御**をクリックし **+追加** をクリックして**ロールの割り当ての追加**を選択します。リソースグループに`EventGrid EventSubscription 共同作成者`を割り当てます。\n![Screenshot 2023-12-22 at 12.03.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ce95c055-a1ba-fc14-cc4c-9ca714100587.png)\n\nマネージドIDを追加します。\n![Screenshot 2023-12-22 at 12.03.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/67779a7a-1341-5c12-103d-1ce4e9011cf4.png)\n![Screenshot 2023-12-22 at 12.06.41.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/caa52f06-a948-4311-ab62-198ede5543ec.png)\n\n# DatabricksアカウントコンソールでのUnity Catalogメタストアの作成\n\nDatabricksワークスペースにログインして、画面右上に表示される自分のメールアドレスをクリックして、**アカウントを管理**をクリックして、アカウントコンソールにアクセスします。\n![Screenshot 2023-12-22 at 12.11.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/fac9c36b-4c07-acff-7817-03e790f80021.png)\n\n**データ**をクリックします。\n![Screenshot 2023-12-22 at 12.13.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e4d86cb0-006f-eec6-96df-661614972710.png)\n\n**メタストアを作成する**をクリックします。\n![Screenshot 2023-12-22 at 12.17.13.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/44d0c4b3-3af0-23e3-41ad-4b312c9f9c1d.png)\n\n- メタストアに名前を付け、リージョンはストレージアカウントと同じものを指定します。私の場合は`japanwest`となります。\n- ADLS Gen 2パスには`<コンテナ名>@<ストレージアカウント名>.dfs.core.windows.net/<パス>`のフォーマットで指定します。私の場合は、`uc-container@japanwestucstorage.dfs.core.windows.net/`となります。\n- アクセスコネクターIDには[マネージドIDの作成](#マネージドidの作成)で作成したマネージドIDのリソースIDを指定します。\n\n**作成**をクリックします。\n![Screenshot 2023-12-22 at 12.23.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8937b270-e8d5-8f30-a636-74cceb4de6b9.png)\n\nメタストアが作成されました。\n![Screenshot 2023-12-22 at 12.25.14.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0683bc49-a092-2525-de7f-3a2929325250.png)\n\n# メタストアをAzure Databricksワークスペースにアタッチ\n\n上の画面ではすでにアタッチするワークスペースが表示されているので、左端のチェックボックスをチェックします。**割り当てる**をクリックします。\n![Screenshot 2023-12-22 at 12.26.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/4227e92c-4a84-829c-1b37-1813ca523d53.png)\n\n確認メッセージが表示されるので、**有効化**をクリックします。\n![Screenshot 2023-12-22 at 12.28.41.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e0dfeea1-2042-b4f5-00cb-0d29b9c1dbbf.png)\n\nおめでとうメッセージが出たら成功です。\n![Screenshot 2023-12-22 at 12.29.30.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8bc4a7f5-2848-b442-3310-342ff3f7c5b6.png)\n\n# 動作確認\n\n上でメタストアをアタッチしたワークスペースにアクセスして、左のメニューから**カタログ**にアクセスします。カタログ一覧に`main`が表示されていることを確認します。これがUnity Catalogのデフォルトのカタログです。\n![Screenshot 2023-12-22 at 12.31.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/271aaf4b-06ff-cb1f-2383-ed32dbcf3e8b.png)\n\n左のメニューから**クラスター**を選択し、**Personal Computeで作成**をクリックします。\n![Screenshot 2023-12-22 at 12.34.14.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c7fc56f2-3891-1791-5809-b42863f16665.png)\n\n**コンピューティングを作成**をクリックします。これで、Unity Catalog対応のクラスターが作成されます。\n![Screenshot 2023-12-22 at 12.35.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/df0b31ed-5ed3-09b6-6a92-13487b5fe9e9.png)\n\nSQLノートブックを作成し、上記クラスターにアタッチして以下のコマンドを実行します。\n\n```sql\nSHOW CATALOGS;\n```\n\nカタログ一覧が表示されます。\n![Screenshot 2023-12-22 at 12.39.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/706814c8-d92e-d51c-daa1-f44b22e72efa.png)\n\n以下のコマンドを実行してスキーマ(データベース)を作成します。\n\n```sql\nUSE CATALOG main;\nCREATE SCHEMA uc_test;\n```\n\nカタログエクスプローラでスキーマを確認できます。\n![Screenshot 2023-12-22 at 12.41.22.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d70e110d-00b8-9de9-3835-71fd04cae771.png)\n\n以上で終了です。お疲れ様でした！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksアシスタントを用いた日本語によるLakeviewダッシュボードの作成", "text": "「未来は手のひらの上に」と思わざるを得ません。\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ye-8tHVY92c?si=QV1prAd86rljDWE8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\nこちらの機能です。\n\nhttps://docs.databricks.com/ja/dashboards/lakeview-create-w-db-assistant.html\n\n元々Lakeviewはノーコードでダッシュボードを作れる機能でしたが、[Databricksアシスタント](https://www.databricks.com/jp/blog/introducing-databricks-assistant)と組み合わされることで、自然言語でダッシュボードを作ることができます！\n\nテーブルは指定します。日本のCOVID-19感染者数のデータです。(そのうち、このステップも不要になりそう)\n![Screenshot 2023-12-21 at 16.02.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7c962165-5a55-c43b-bde0-02c3900eca01.png)\n\nCanvasに移動してビジュアライゼーションを追加するとプロンプトを入力できるようになっています。\n![Screenshot 2023-12-21 at 16.03.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/95c25120-4a6b-f1ab-d94c-f2090dbe36af.png)\n\n> 都道府県別比率\n\nと入力します。\n\n勝手に円グラフになりました！\n![Screenshot 2023-12-21 at 16.03.15.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6ee09e0e-39bc-31c4-ed77-217f539659a7.png)\n\n次は、\n\n> 月毎の感染者数の変化\n\n![Screenshot 2023-12-21 at 16.03.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/75a79971-86c2-7cec-aaf9-f685b55debd4.png)\n![Screenshot 2023-12-21 at 16.03.35.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/34836026-c26b-1c5d-1193-0f33b144bd32.png)\n\n今度は折れ線グラフです！(興奮しすぎ)\n\nますます、自然言語が新たなインタフェースになってきてます。ぜひお試しください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks推論テーブルとレイクハウスモニタリングによる大規模言語モデルの監視", "text": "こちらのウォークスルーです。\n\nhttps://docs.databricks.com/ja/machine-learning/model-serving/inference-tables.html\n\n以前書いたこちらの記事の続編でもあります。\n\nhttps://qiita.com/taka_yayoi/items/6ef148d2c4286b51b947\n\nhttps://qiita.com/taka_yayoi/items/b481502fad773f475e23\n\nOpenAI APIを使うチャットbotをDatabricksで動作させる際に、推論テーブル(inference table)を有効化すると、モデルがデプロイされているモデルサービングエンドポイントの入出力を追跡することができます。\n\nモデルサービングエンドポイントを編集して推論テーブルを有効化し、保存先テーブルを指定します。\n![Screenshot 2023-12-20 at 18.25.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3ea25525-6ccd-b10d-9347-503b6244f345.png)\n\nDatabricksで稼働するチャットbotに問い合わせすると、モデルサービングエンドポイントを呼び出す形となり、その際に推論テーブルにリクエスト、レスポンスが記録されます。\n![Screenshot 2023-12-20 at 18.26.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c854cfce-559f-0d4b-6f5a-cc84137def61.png)\n\nカタログエクスプローラでデータを確認できます。\n![Screenshot 2023-12-20 at 18.29.16.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2c3f80fb-8114-01d8-ded0-7c1e65a9f88b.png)\n\nこれは生データですので加工します。こちらにサンプルノートブックがあります。アプリケーションの入出力に応じて微調整する必要があるかもしれません。\n\nhttps://docs.databricks.com/ja/machine-learning/model-serving/inference-tables.html#starter-notebook-for-monitoring-text-quality-from-endpoints-serving-llms\n\nノートブックを実行することで、チャットボットへの入出力にアクセスできるようになります。\n![Screenshot 2023-12-20 at 18.32.06.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bf38ed3c-6ac3-3377-dad9-a0675acd1a0f.png)\n\n上記ノートブックにはレイクハウスモニタリングのコードも入っていますので、実行すると上記テーブルに対するモニターが設定されます。\n\n自動生成されるダッシュボードでは、問い合わせ数や内容、レスポンス、レスポンスのtoxicity、perplexityなどを確認することができます。\n![Screenshot 2023-12-20 at 18.21.34.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/177e2efb-6d18-ddf3-e14a-7ac11335313d.png)\n![Screenshot 2023-12-20 at 18.34.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5d7c64bd-e5d9-f95b-4a6b-c98f8a55e8e0.png)\n\n企業で運用するチャットbotにおいては、LLMに対する入力とレスポンスのモニタリング、それに基づく対策、改善が重要となります。(まだ日本リージョンでは利用できませんが)リリースされたらぜひお試しください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksのカタログエクスプローラでER図がサポートされます", "text": "こちらのアップデートです。\n\nhttps://docs.databricks.com/ja/release-notes/product/2023/december.html#entity-relationship-diagram-for-primary-keys-and-foreign-keys\n\n> カタログエクスプローラで、Entity Relationship Diagram (ERD)のグラフとして主キーと外部キーの関係を参照できます。詳細については、[エンティティ関係図の表示](https://docs.databricks.com/ja/catalog-explorer/entity-relationship-diagram.html)をご覧ください。\n\n以下のようなリレーションを持つテーブルをUnity Catalog配下に作成します。\n\n```sql\nCREATE TABLE T(pk1 INTEGER NOT NULL, pk2 INTEGER NOT NULL,\n                CONSTRAINT t_pk PRIMARY KEY(pk1, pk2));\nCREATE TABLE S(pk INTEGER NOT NULL PRIMARY KEY,\n                fk1 INTEGER, fk2 INTEGER,\n                CONSTRAINT s_t_fk FOREIGN KEY(fk1, fk2) REFERENCES T);\n```\n\nテーブルSはテーブルTの複合主キーを外部キーとして参照しています。\n\nカタログエクスプローラに移動します。**列**タブにアクセスします。**View relationships**ボタンが表示されるのでクリックします。\n![Screenshot 2023-12-20 at 8.34.06.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bb513d75-e19e-f887-d87b-4eab80c0e34b.png)\n\nER図が表示されます。\n![Screenshot 2023-12-20 at 8.33.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c58482c5-d983-1d7f-514f-38c1b61eb28b.png)\n\nすでに提供されている[データリネージ](https://qiita.com/taka_yayoi/items/875f6684e26f13c6e48b)はデータがどのように作成されたのかを示す情報でしたが、ER図によってテーブル間の論理的な関係性を容易に把握できるようになります。本機能は順次リリースされますので、是非ご活用ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "MLflowのOpenAIフレーバーにおけるAPIキー管理方法の変更", "text": "こちらを読んでいたら以下のような記述が。\n\nhttps://mlflow.org/docs/latest/python_api/openai/index.html\n\n> Specifying secrets for model serving with `MLFLOW_OPENAI_SECRET_SCOPE` is deprecated. Use [secrets-based environment variables](https://docs.databricks.com/ja/machine-learning/model-serving/store-env-variable-model-serving.html) instead.\n\nこれまでは、OpenAIのAPIキーはDatabricksシークレットのスコープ名を環境変数[`MLFLOW_OPENAI_SECRET_SCOPE`に指定](https://qiita.com/taka_yayoi/items/a058484e6c0abbfbc476)する形で引き渡していましたが、これが非推奨になるとのこと。代わりにシークレットベースの環境変数を指定しろとのこと。なので、早速試してみます。\n\n```py\n%pip install tiktoken\n%pip install openai\n```\n\n```py\nimport os\nimport mlflow\nimport openai\n\nmodel_name = \"takaakiyayoi_catalog.rag_chatbot.open_ai\"\n\nmlflow.set_registry_uri(\"databricks-uc\")\n\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model=\"gpt-3.5-turbo\",\n        task=openai.ChatCompletion,\n        messages=[{\"role\": \"system\", \"content\": \"You are an MLflow expert\"}],\n        artifact_path=\"model\",\n        registered_model_name = model_name,\n    )\n```\n\nMLflowでトラッキングするとともに、Unity Catalogに登録します。\n![Screenshot 2023-12-19 at 21.39.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d7a2f006-00ff-5c12-927c-73a158280beb.png)\n\nREST APIを使ってモデルサービングエンドポイントを作成しますが、この際に`environment_vars`にシークレットを参照する環境変数を設定します。以下の例ではスコープ名が`demo-token-takaaki.yayoi`でキー名が`openai`のシークレットにOpenAIのAPIキーを格納しています。\n\n```py\nimport requests\n\nsecret_scope_name = \"demo-token-takaaki.yayoi\"\nsecret_key_name = \"openai\"\ndatabricks_host = \"<Databricksワークスペースのホスト名>\"\n\nendpoint_name = \"my_serving_endpoint_taka\"\nmodel_name = model_name\nmodel_version = 1\ndatabricks_api_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\n# Create your endpoint\ndata = {\n    \"name\": endpoint_name,\n    \"config\": {\n        \"served_models\": [\n            {\n                \"model_name\": model_name,\n                \"model_version\": model_version,\n                \"workload_size\": \"Small\",\n                \"scale_to_zero_enabled\": True,\n                \"environment_vars\": {\n                    \"OPENAI_API_KEY\": f\"{{{{secrets/{secret_scope_name}/{secret_key_name}}}}}\"\n                }\n            }\n        ]\n    }\n}\n\nheaders = {\n    \"Context-Type\": \"text/json\",\n    \"Authorization\": f\"Bearer {databricks_api_token}\"\n}\n\nresponse = requests.post(\n    url=f\"https://{databricks_host}/api/2.0/serving-endpoints\",\n    json=data,\n    headers=headers\n)\nprint(\"Response status:\", response.status_code)\nprint(\"Response text:\", response.text, \"\\n\")\n```\n\nしばらくするとエンドポイントが稼働し始めます。\n![Screenshot 2023-12-19 at 21.42.30.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/14b9a6a7-df5e-0d82-61c3-664e094dd55a.png)\n\n動作確認のために(いつものように)streamlitで画面を作ります。\n\n```py:/Workspace/Users/takaaki.yayoi@databricks.com/20231219_openai/streamlit.py\nimport streamlit as st \nimport numpy as np \nfrom PIL import Image\nimport base64\nimport io\n\nimport os\nimport requests\nimport numpy as np\nimport pandas as pd\n\nimport json\n\nfrom databricks.sdk.runtime import dbutils\n\nst.header('OpenAI Chatbot on Databrikcs')\n\ndef score_model(prompt):\n  # 1. パーソナルアクセストークンを設定してください\n  token = dbutils.secrets.get(\"demo-token-takaaki.yayoi\", \"token\")\n\n  # 2. モデルエンドポイントのURLを設定してください\n  url = '<モデルサービングエンドポイントのURL>'\n  headers = {'Authorization': f'Bearer {token}'}\n  \n  data_json_str = f\"\"\"\n  {{\"inputs\":[\n{{\"role\": [\"user\"], \"content\": [\"{prompt}\"]}}\n]\n}}\n\"\"\"\n\n  data_json = json.loads(data_json_str)\n  response = requests.request(method='POST', headers=headers, url=url, json=data_json)\n\n  if response.status_code != 200:\n    raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n  return response.json()\n\nprompt = st.text_input(\"プロンプト\")\n\nif prompt != \"\":\n  response = score_model(prompt)\n  st.write(response['predictions'][0])\n```\n\n元のノートブックに戻ってstreamlitをインストールします。\n\n```py\n%pip install streamlit watchdog\ndbutils.library.restartPython()\n```\n\n```py\nfrom dbruntime.databricks_repl_context import get_context\n\ndef front_url(port):\n    \"\"\"\n    フロントエンドを実行するための URL を返す\n\n    Returns\n    -------\n    proxy_url : str\n        フロントエンドのURL\n    \"\"\"\n    ctx = get_context()\n    proxy_url = f\"https://{ctx.browserHostName}/driver-proxy/o/{ctx.workspaceId}/{ctx.clusterId}/{port}/\"\n\n    return proxy_url\n\nPORT = 1501\n\n# Driver ProxyのURLを表示\nprint(front_url(PORT))\n\n# 利便性のためにリンクをHTML表示\ndisplayHTML(f\"<a href='{front_url(PORT)}' target='_blank' rel='noopener noreferrer'>別ウインドウで開く</a>\")\n```\n\nstreamlitを起動します。\n\n```py\nstreamlit_file = \"/Workspace/Users/takaaki.yayoi@databricks.com/20231219_openai/streamlit.py\"\n!streamlit run {streamlit_file} --server.port {PORT}\n```\n\n上のセルを実行した際に表示されるリンクをクリックするとGUIにアクセスできます。\n\nということで動作確認もできました！\n![Screenshot 2023-12-19 at 21.45.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/588ceeec-1ee5-61a4-f144-ae97cc036e82.png)\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks Feature Storeによる特徴量とモデルの一元管理", "text": "こちらのノートブックをウォークスルーします。\n\nhttps://docs.databricks.com/ja/machine-learning/feature-store/example-notebooks.html#basic-feature-engineering-in-unity-catalog-example-notebook\n\n日本語に翻訳したものはこちらです。\n\nhttps://github.com/taka-yayoi/public_repo_2/blob/master/feature_store_in_uc/feature-store-with-uc-basic-example.py\n\n# Databricks Feature Storeとは\n\n特徴量の一元管理を行うためのリポジトリです。似たような取り組みを行っているにも関わらず、複数人が特徴量を準備するのは無駄となります。このような無駄を排除し、特徴量の再利用を促進することができます。\n\nDatabricksのFeature Storeを活用することで、以下のメリットを享受することができます。\n\n- 検索容易性: Feature StoreのUIを通じて特徴量の検索、参照が可能となります。\n- リネージ: Databricksで特徴量を作成すると、特徴量テーブルの作成で使用されたデータソースが保存され、容易に確認することができるようになります。また、使用されたノートブック、ジョブ、エンドポイントなどを確認することができます。\n- モデルのスコアリングとサービングとの統合: Feature Storeの特徴量を使用してモデルをトレーニングすると、モデルには特徴量のメタデータも一緒にパッケージングされます。モデルをバッチ推論やオンライン推論で使用する際には、自動的にFeature Storeから特徴量が取得されます。\n\n![](https://docs.databricks.com/ja/_images/feature-store-flow-gcp.png)\n\n# Unity Catakigにおける特徴量エンジニアリングの基本的なサンプル\n\nこちらのノートブックではUnity Catalogに統合された[Feature Store](https://docs.databricks.com/ja/machine-learning/feature-store/uc/feature-tables-uc.html)と[モデルの管理機能](https://docs.databricks.com/ja/machine-learning/manage-model-lifecycle/index.html)もウォークスルーします。\n\nこのノートブックでは、MLモデルのトレーニングとバッチ推論を行うために、推論時にのみ利用できる特徴量を含むUnity Catalogの特徴量の作成、格納、管理を行うためのUnity Catalogの特徴量エンジニアリングの使い方を説明します。このサンプルでは、さまざまな静的なワインの特徴量やリアルタイムのインプットとMLモデルを用いてワインの品質を予測することがゴールとなります。\n\nこのノートブックでは以下の方法を説明します:\n\n- 機械学習モデル向けのトレーニングデータセットを構築するための特徴量テーブルの作成\n- 新バージョンのモデルを作成するために、特徴量テーブルを編集し、アップデートされたテーブルを使用\n- 特徴量とモデルがどのような関係にあるのかを特定するためにDatabricksの特徴量UIを使用\n- 自動特徴量検索を用いたバッチスコアリングの実行\n\n## 要件\n\n- Databricks機械学習ランタイム13.2以降\n  - Databricks機械学習ランタイムにアクセスできない場合には、Databricksランタイム13.2以降でこのノートブックを実行することができます。この際には、ノートブックの最初で`%pip install databricks-feature-engineering`を実行します。\n\n```py\nimport pandas as pd\n\nfrom pyspark.sql.functions import monotonically_increasing_id, expr, rand\nimport uuid\n\nfrom databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n\nimport mlflow\nimport mlflow.sklearn\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n```\n\n## データセットのロード\n\n以下のセルのコードでは、データセットをロードして少々のデータ準備を行います: それぞれの観測値に対してユニークなIDを作成し、カラム名から空白を除外します。ユニークIDのカラム(`wine_id`)は特徴量テーブルの主キーとなり、特徴量の検索に使用されます。\n\n```py\nraw_data = spark.read.load(\"/databricks-datasets/wine-quality/winequality-red.csv\",format=\"csv\",sep=\";\",inferSchema=\"true\",header=\"true\" )\n\ndef addIdColumn(dataframe, id_column_name):\n    \"\"\"データフレームに id カラムを追加\"\"\"\n    columns = dataframe.columns\n    new_df = dataframe.withColumn(id_column_name, monotonically_increasing_id())\n    return new_df[[id_column_name] + columns]\n\ndef renameColumns(df):\n    \"\"\"UCのFeature Engineeringと互換性を持つようにカラム名を変更\"\"\"\n    renamed_df = df\n    for column in df.columns:\n        renamed_df = renamed_df.withColumnRenamed(column, column.replace(' ', '_'))\n    return renamed_df\n\n# 関数の実行\nrenamed_df = renameColumns(raw_data)\ndf = addIdColumn(renamed_df, 'wine_id')\n\n# 特徴量テーブルに含めないターゲットカラム ('quality') を削除します\nfeatures_df = df.drop('quality')\ndisplay(features_df)\n```\n![Screenshot 2023-12-18 at 16.42.16.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ecda3acb-a1e8-3075-6747-998e137db406.png)\n\n## 新規のカタログの作成、あるいは既存カタログの再利用\n\n新規にカタログを作成するには、メタストアに対する`CREATE CATALOG`権限が必要です。既存のカタログを使用する場合には、カタログに対する`USE CATALOG`権限が必要です。\n\n```py\ncatalog_name = \"takaakiyayoi_catalog\"\n\n# 新規カタログを作成:\n# spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n# spark.sql(f\"USE CATALOG {catalog_name}\")\n\n# あるいは、既存カタログを再利用:\nspark.sql(f\"USE CATALOG {catalog_name}\")\n```\n\n## カタログに新規スキーマを作成\n\nカタログに新規スキーマを作成するには、カタログに対する`CREATE SCHEMA`権限が必要です。\n\n```py\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS wine_db\")\nspark.sql(\"USE SCHEMA wine_db\")\n\n# それぞれの実行ごとにユニークなテーブル名を作成。複数回ノートブックを実行する際のエラーを回避します。\ntable_name = f\"{catalog_name}.wine_db.wine_db_\" + str(uuid.uuid4())[:6]\nprint(table_name)\n```\n\n```\ntakaakiyayoi_catalog.wine_db.wine_db_807ef4\n```\n\n## 特徴量テーブルの作成\n\n最初のステップでは`FeatureEngineeringClient`を作成します。\n\n```py\nfe = FeatureEngineeringClient()\n\n# ノートブックでfeature engineering client APIの関数のヘルプを取得できます :\n# help(fe.<function_name>)\n\n# 例:\n# help(fe.create_table)\n```\n\n特徴量テーブルを作成します。完全なAPIリファレンスについては([AWS](https://docs.databricks.com/machine-learning/feature-store/python-api.html)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/feature-store/python-api)|[GCP](https://docs.gcp.databricks.com/machine-learning/feature-store/python-api.html))をご覧ください。\n\n```py\nfe.create_table(\n    name=table_name,\n    primary_keys=[\"wine_id\"],\n    df=features_df,\n    schema=features_df.schema,\n    description=\"ワインの特徴量\"\n)\n```\n\n特徴量テーブルが作成されました。\n![Screenshot 2023-12-18 at 16.45.39.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/50f2c565-fc7e-a4c7-bb7e-e0a1848bc3b8.png)\n\n## Unity Catalogの特徴量エンジニアリングを用いたモデルのトレーニング\n\n特徴量テーブルには予測ターゲットは含まれません。しかし、トレーニングデータセットには予測ターゲットの値が必要です。また、モデルが推論で使用されるまで利用できない特徴量が存在する場合があります。\n\nこの例では、推論時にのみ観測できるワインの特性を表現する特徴量 **`real_time_measurement`** を使用します。この特徴量はトレーニングで使用され、推論時にはワインの特徴量の値として提供されます。\n\n以下の例では`real_time_measurement`としてダミーデータを追加してます。\n\n```py\n## inference_data_df には、 wine_id (主キー)、quality (予測ターゲット)、リアルタイムの特徴量が含まれます\ninference_data_df = df.select(\"wine_id\", \"quality\", (10 * rand()).alias(\"real_time_measurement\"))\ndisplay(inference_data_df)\n```\n![Screenshot 2023-12-18 at 16.46.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b1731d77-edf3-a549-c30a-e6f0a703e25f.png)\n\n特徴量テーブルから特徴量を検索するために指定された`lookup_key`とオンライン特徴量`real_time_measurement`を使用するトレーニングデータセットを構築するために`FeatureLookup`を使用します。`feature_names`パラメータを指定しない場合には、主キーを除くすべての特徴量が返却されます。\n\n```py\ndef load_data(table_name, lookup_key):\n    # FeatureLookupで`feature_names`パラメータを指定しない場合、主キーを除くすべての特徴量が返却されます\n    model_feature_lookups = [FeatureLookup(table_name=table_name, lookup_key=lookup_key)]\n\n    # fe.create_training_setはinference_data_dfと主キーがマッチするmodel_feature_lookupsの特徴量を検索します\n    training_set = fe.create_training_set(df=inference_data_df, feature_lookups=model_feature_lookups, label=\"quality\", exclude_columns=\"wine_id\")\n    training_pd = training_set.load_df().toPandas()\n\n    # トレーニングデータセット、テストデータセットを作成\n    X = training_pd.drop(\"quality\", axis=1)\n    y = training_pd[\"quality\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    return X_train, X_test, y_train, y_test, training_set\n\n# トレーニングデータセットとテストデータセットを作成\nX_train, X_test, y_train, y_test, training_set = load_data(table_name, \"wine_id\")\nX_train.head()\n```\n\n`fe.create_training_set`の`df`に指定された`inference_data_df`と`model_feature_lookups`に指定された検索条件に従って取得された特徴量テーブルが結合されたトレーニングデータセットを取得することができます。\n\n![Screenshot 2023-12-18 at 16.47.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/36df146a-f616-436b-cff0-c681ea34ca6f.png)\n\nトレーニングしたモデルがUnity Catalog配下で管理されるように設定します。\n\n```py\nfrom mlflow.tracking.client import MlflowClient\n\n# Unity CatalogのモデルにアクセスするようにMLflowクライアントを設定\nmlflow.set_registry_uri(\"databricks-uc\")\n\nmodel_name = f\"{catalog_name}.wine_db.wine_model\"\n\nclient = MlflowClient()\n\ntry:\n    client.delete_registered_model(model_name) # 作成済みの場合にはモデルを削除\nexcept:\n    None\n```\n\n次のセルのコードはscikit-learnのRandomForestRegressorモデルをトレーニングし、UCのFeature Engineeringを用いてモデルを記録します。\n\nこのコードはトレーニングのパラメータと結果を追跡するためのMLflowエクスペリメントをスタートします。モデルのオートロギングをオフ(`mlflow.sklearn.autolog(log_models=False)`)にしていることに注意してください。これは、モデルは`fe.log_model`を用いて記録されるためです。\n\n```py\n# MLflowオートロギングを無効化して、UCのFeature Engineeringを用いてモデルを記録\nmlflow.sklearn.autolog(log_models=False)\n\ndef train_model(X_train, X_test, y_train, y_test, training_set, fe):\n    ## モデルのフィッティングと記録\n    with mlflow.start_run() as run:\n\n        rf = RandomForestRegressor(max_depth=3, n_estimators=20, random_state=42)\n        rf.fit(X_train, y_train)\n        y_pred = rf.predict(X_test)\n\n        mlflow.log_metric(\"test_mse\", mean_squared_error(y_test, y_pred))\n        mlflow.log_metric(\"test_r2_score\", r2_score(y_test, y_pred))\n\n        fe.log_model(\n            model=rf,\n            artifact_path=\"wine_quality_prediction\",\n            flavor=mlflow.sklearn,\n            training_set=training_set,\n            registered_model_name=model_name,\n        )\n\ntrain_model(X_train, X_test, y_train, y_test, training_set, fe)\n```\n\nモデルが記録されます。\n![Screenshot 2023-12-18 at 16.50.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6747aacb-e4ba-c898-3b8d-bcdce56bb34b.png)\n\n記録されたモデルを参照するには、このノートブックのMLflowエクスペリメントページに移動します。エクスペリメントページにアクセスするには、左のナビゲーションバーのエクスペリメントアイコンをクリックします: <img src=\"https://docs.databricks.com/_static/images/icons/experiments-icon.png\"/>\n\nリストからノートブックのエクスペリメントを探します。ノートブックと同じ名前になっており、この場合`feature-store-with-uc-basic-example`となります。\n\nエクスペリメントページを表示するにはエクスペリメント名をクリックします。このページの**Artifacts**セクションには、`fe.log_model`を呼び出した際に作成された、パッケージングされたUCのFeature Engineeringモデルが表示されます。\n\n<img src=\"https://docs.databricks.com/_static/images/machine-learning/feature-store/basic-fs-nb-artifact.png\"/>\n\nまた、このモデルは自動的にUnity Catalogに登録されます。\n![Screenshot 2023-12-18 at 16.52.30.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3978fbeb-9161-ec80-412d-8c138654c0f7.png)\n\n## バッチスコアリング\n\n推論において新規データに対して、パッケージングされたFeature Engineering in UCモデルを適用するには、`score_batch`を使用します。入力データには主キーのカラム`wine_id`とリアルタイムの特徴量である`real_time_measurement`のみが必要となります。モデルは自動で特徴量テーブルからすべてのその他の特徴量を検索します。\n\n```py\n# ヘルパー関数\ndef get_latest_model_version(model_name):\n    latest_version = 1\n    mlflow_client = MlflowClient()\n    for mv in mlflow_client.search_model_versions(f\"name='{model_name}'\"):\n        version_int = int(mv.version)\n        if version_int > latest_version:\n            latest_version = version_int\n    return latest_version\n```\n\n```py\n## シンプルにするために、この例では予測の入力データとしてinference_data_dfを使います\nbatch_input_df = inference_data_df.drop(\"quality\") # ラベルカラムの削除\n\nlatest_model_version = get_latest_model_version(model_name)\n\npredictions_df = fe.score_batch(model_uri=f\"models:/{model_name}/{latest_model_version}\", df=batch_input_df)\n\ndisplay(predictions_df[\"wine_id\", \"prediction\"])\n```\n![Screenshot 2023-12-18 at 16.53.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a0fc1b92-8e07-3e03-9275-4a715ccc956c.png)\n\n## 特徴量テーブルの修正\n\n新たな特徴量を追加することでデータフレームを修正したとします。特徴量テーブルを更新するには`mode=\"merge\"`で`fe.write_table`を使用します。\n\n```py\n## 特徴量を保持するデータフレームの修正\nso2_cols = [\"free_sulfur_dioxide\", \"total_sulfur_dioxide\"]\nnew_features_df = (features_df.withColumn(\"average_so2\", expr(\"+\".join(so2_cols)) / 2))\n\ndisplay(new_features_df)\n```\n![Screenshot 2023-12-18 at 16.54.53.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/74da3568-327a-837d-3abf-5cba95b1decf.png)\n\n`fe.write_table`で`mode=\"merge\"`を指定して特徴量テーブルを更新します。\n\n```py\nfe.write_table(\n    name=table_name,\n    df=new_features_df,\n    mode=\"merge\"\n)\n```\n\n特徴量テーブルから特徴量を読み込むには`fe.read_table()`を使用します。\n\n```py\n# 最新バージョンの特徴量テーブルを表示します\n# 現行バージョンで削除された特徴量は表示されますが、値はnullとなります\ndisplay(fe.read_table(name=table_name))\n```\n![Screenshot 2023-12-18 at 16.56.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a5faf85d-97d9-d10d-2f28-747d4a65b080.png)\n\n## 更新された特徴量テーブルを用いた新たなモデルバージョンのトレーニング\n\n```py\ndef load_data(table_name, lookup_key):\n    model_feature_lookups = [FeatureLookup(table_name=table_name, lookup_key=lookup_key)]\n\n    # fe.create_training_set は inference_data_df とキーがマッチする model_feature_lookups 特徴量を検索します\n    training_set = fe.create_training_set(df=inference_data_df, feature_lookups=model_feature_lookups, label=\"quality\", exclude_columns=\"wine_id\")\n    training_pd = training_set.load_df().toPandas()\n\n    # トレーニングデータセットとテストデータセットの作成\n    X = training_pd.drop(\"quality\", axis=1)\n    y = training_pd[\"quality\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    return X_train, X_test, y_train, y_test, training_set\n\nX_train, X_test, y_train, y_test, training_set = load_data(table_name, \"wine_id\")\nX_train.head()\n```\n![Screenshot 2023-12-18 at 16.56.53.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ac46aa1a-d01b-36e9-6151-43b3409caf45.png)\n\nトレーニングデータセットを構築する際、特徴量を検索するために指定された `key` を使用します。\n\n```py\ndef train_model(X_train, X_test, y_train, y_test, training_set, fe):\n    ## モデルのフィッティングと記録\n    with mlflow.start_run() as run:\n\n        rf = RandomForestRegressor(max_depth=3, n_estimators=20, random_state=42)\n        rf.fit(X_train, y_train)\n        y_pred = rf.predict(X_test)\n\n        mlflow.log_metric(\"test_mse\", mean_squared_error(y_test, y_pred))\n        mlflow.log_metric(\"test_r2_score\", r2_score(y_test, y_pred))\n\n        fe.log_model(\n            model=rf,\n            artifact_path=\"feature-store-model\",\n            flavor=mlflow.sklearn,\n            training_set=training_set,\n            registered_model_name=model_name,\n        )\n\ntrain_model(X_train, X_test, y_train, y_test, training_set, fe)\n```\n\n**`score_batch`** を用いて特徴量に最新バージョンの登録MLflowモデルを適用します。\n\n```py\n## シンプルにするために、この例では予測の入力データとしてinference_data_dfを使います\nbatch_input_df = inference_data_df.drop(\"quality\") # ラベルカラムの削除\nlatest_model_version = get_latest_model_version(model_name)\npredictions_df = fe.score_batch(model_uri=f\"models:/{model_name}/{latest_model_version}\", df=batch_input_df)\ndisplay(predictions_df[\"wine_id\",\"prediction\"])\n```\n![Screenshot 2023-12-18 at 16.58.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d925476f-d5ea-1f3c-9160-7be34f7a17f5.png)\n\n## 特徴量テーブルの権限コントロールと削除\n\n- Unity Catalogの特徴量テーブルに誰がアクセスできるのかをコントロールするには、カタログエクスプローラのテーブル詳細ページにある**Permissions**ボタンを使います。\n- Unity Catalog特徴量テーブルを削除するには、カタログエクスプローラのテーブル詳細ページにあるケバブメニューをクリックし、**Delete**を選択します。UIを用いてUnity Catalog特徴量テーブルを削除すると、対応するDeltaテーブルも削除されます。\n\n# カタログエクスプローラによるリネージの確認\n\n特徴量テーブルと機械学習モデルの両方がUnity Catalogで管理されることで、リネージも追跡されます。特徴量テーブルの**依存関係**を開くと、この特徴量テーブルがどのモデルによって使用されているのかを確認することができます。\n![Screenshot 2023-12-18 at 17.01.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f594492f-8e15-8304-1142-1117eb6fd769.png)\n![Screenshot 2023-12-18 at 17.02.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ab3c7f45-be4e-2691-caa9-efe0757468f5.png)\n\nこれまでは、特徴量テーブルも機械学習モデルも各自の環境で開発して終わりのケースが多かったと思うのですが、このように、これらを一元管理することでモデルのトレーニングで使用された特徴量の追跡や、特徴量テーブルのコンシューマーを容易に特定できるようになります。是非ご活用ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksレイクハウスモニタリングによるテーブル品質の監視", "text": "サーバレスが有効化されているリージョンではテーブルや機械学習モデルの監視を行えるレイクハウスモニタリングが利用できるようになっています。\n\nhttps://qiita.com/taka_yayoi/items/5aaddc9aa7b06dc87f12\n\n時系列(time series)分析の[サンプルノートブック](https://docs.databricks.com/ja/lakehouse-monitoring/create-monitor-api.html#notebook-example-time-series-profile)をウォークスルーします。時間とともにデータが変化していくテーブルを監視するタイプです。\n\n# レイクハウスモニタリングサンプルノートブック: 時系列分析\n\n**ユーザー要件**\n- Unity Catalogにアクセスできるクラスターでのコマンド実行権限が必要です。\n- 最低でも一つのカタログに対する`USE CATALOG`権限が必要です。最低でも一つのスキーマに対する`USE SCHEMA`権限が必要です。このノートブックではテーブルを`main.default`スキーマに作成します。`main.default`スキーマへの権限がない場合には、権限を持つカタログとスキーマに変更してください。\n\n**システム要件:**\n- ワークスペースでUnity Catalogが有効化されている必要があります。\n- Databricks Runtime 12.2LTS以降\n- シングルユーザーあるいは共有クラスター\n\nこのノートブックでは時系列モニターの作成方法を説明します。\n\nレイクハウスモニタリングの詳細に関してはドキュメントを参照ください。([AWS](https://docs.databricks.com/ja/lakehouse-monitoring/index.html)|[Azure](https://learn.microsoft.com/ja-jp/azure/databricks/lakehouse-monitoring/))\n\n## セットアップ\n* クラスター設定の検証\n* Pythonクライアントのインストール\n* カタログ、スキーマ、テーブル名の定義\n\n```py\n# クラスター設定のチェック。このセルが失敗する場合、ノートブックの右上のクラスターセレクターで、Databricks Runtime 12.2 LTS　以降が稼働しているクラスターを設定、選択してください。\nimport os\n\nassert float(os.environ.get(\"DATABRICKS_RUNTIME_VERSION\", 0)) >= 12.2, \"Please configure your cluster to use Databricks Runtime 12.2 LTS or above.\"\n```\n\n```py\n%pip install \"https://ml-team-public-read.s3.amazonaws.com/wheels/data-monitoring/a4050ef7-b183-47a1-a145-e614628e3146/databricks_lakehouse_monitoring-0.4.4-py3-none-any.whl\"\n```\n\n```py\n# 新規にインストールしたwheelを用いて環境をリセットするためにこのステップが必要です。\ndbutils.library.restartPython()\n```\n\n```py\n# カタログに対する`USE CATALOG`権限とスキーマに対する`USE SCHEMA`権限が必要です。\n# 必要に応じて、ここでカタログとスキーマを変更して下さい。\n\nCATALOG = \"takaakiyayoi_catalog\"\nSCHEMA = \"monitoring\"\n```\n\nテーブル名にユーザー名を埋め込みます。\n\n```py\nusername = spark.sql(\"SELECT current_user()\").first()[\"current_user()\"]\nusername_prefixes = username.split(\"@\")[0].split(\".\")\n```\n\n```py\nunique_suffix = \"_\".join([username_prefixes[0], username_prefixes[1][0:2]])\nTABLE_NAME = f\"{CATALOG}.{SCHEMA}.wine_ts_{unique_suffix}\"\nBASELINE_TABLE = f\"{CATALOG}.{SCHEMA}.wine_ts_baseline_{unique_suffix}\"\nTIMESTAMP_COL = \"timestamp\"\n```\n\n```py\nspark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\nspark.sql(f\"DROP TABLE IF EXISTS {BASELINE_TABLE}\")\n```\n\n## ユーザージャーニー\n1. テーブルの作成: 生のデータを読み込みプライマリテーブル(監視されるテーブル)とベーステーブル(期待する品質基準を満たす既知のデータを格納)を作成。\n2. プライマリテーブルに対するモニターを作成。\n3. メトリクステーブルの調査。\n4. テーブルを変更してメトリクスを更新。メトリクステーブルの調査。\n5. [オプション] モニターの削除。\n\n### 1. Unity Catalogでプライマリテーブルと(オプション)ベースラインテーブルを作成\n* これらのテーブルはUnity Catalog配下のDeltaテーブルであり、ノートブックを実行するユーザーが所有者である必要があります。\n* 監視されるテーブルは「プライマリテーブル」とも呼ばれます。  \n* ベースラインテーブルは監視されるテーブルと同じスキーマを持つ必要があります。\n\nこのサンプルでは`winequality`データセットを使用します。\n\n```py\nimport pandas as pd\n\nwhite_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-white.csv\", sep=\";\")\nred_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-red.csv\", sep=\";\")\n\n# カテゴリーの追加\nwhite_wine[\"type\"] = \"white\"\nred_wine[\"type\"] = \"red\"\ndata_pdf = pd.concat([white_wine, red_wine], axis=0)\n\n# カラム名の整形\ndata_pdf.columns = data_pdf.columns.str.replace(\" \", \"_\")\n```\n\n#### データを分割します。`baseline_df`はベースラインテーブルです。 `ts1_df`はプライマリテーブルであり、`ts2_df`は模擬的に未来のデータを示すものです。\n\n```py\ndata_df = spark.createDataFrame(data_pdf)\nbaseline_df, ts1_df, ts2_df = data_df.randomSplit(weights=[0.20, 0.40, 0.40], seed=42)\n```\n\n#### 時系列データをシミュレートするために異なるタイムスタンプを作成します。\n\n```py\nfrom datetime import timedelta, datetime\nfrom pyspark.sql import functions as F\n\n# 異なるタイムスタンプを持つデータをシミュレート\ntimestamp_0 = datetime.now() # ベースラインデータ\ntimestamp_1 = (datetime.now() + timedelta(1)).timestamp() # 1日後\ntimestamp_2 = (datetime.now() + timedelta(2)).timestamp() \n\nbaseline_df = baseline_df.withColumn(\"timestamp\", F.lit(timestamp_0).cast(\"timestamp\"))\nts1_df = ts1_df.withColumn(\"timestamp\", F.lit(timestamp_1).cast(\"timestamp\"))\nts2_df = ts2_df.withColumn(\"timestamp\", F.lit(timestamp_2).cast(\"timestamp\"))\n```\n\n```py\nbaseline_df.display()\n```\n![Screenshot 2023-12-16 at 16.52.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d6b41c9f-df1f-294d-d470-e66be6bba1ff.png)\n\n#### Unity CatalogにベースラインのDeltaテーブルとプライマリのDeltaテーブルを作成\n\n```py\n# Unity Catalogにベースラインテーブルを書き込み\n\n(baseline_df\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"overwriteSchema\",True)\n  .option(\"delta.enableChangeDataFeed\", \"true\")\n  .saveAsTable(f\"{BASELINE_TABLE}\")\n)\n```\n```py\n# Unity Catalogにプライマリテーブルを書き込み。これが監視されるテーブルとなります。このノートブックの後半で、このテーブルに未来のタイムスタンプを持つデータを追加します。 \n\n(ts1_df\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"overwriteSchema\",True)\n  .option(\"delta.enableChangeDataFeed\", \"true\")\n  .saveAsTable(f\"{TABLE_NAME}\")\n)\n```\n\nテーブルが作成されます。\n![Screenshot 2023-12-16 at 16.52.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7f719efb-a4d6-d954-280c-6fd770facdbe.png)\n\n行数を確認します。\n\n```py\nspark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").display()\n```\n![Screenshot 2023-12-16 at 16.53.04.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/fbbeb1eb-33d8-94a7-af72-d8169d598b34.png)\n\n2. モニターの作成\nこのノートブックでは、`TimeSeries`タイプの分析を説明します。他の分析タイプに関しては、レイクハウスモニタリングのドキュメントを参照ください。([AWS](https://docs.databricks.com/ja/lakehouse-monitoring/index.html)|[Azure](https://docs.databricks.com/ja/lakehouse-monitoring/index.html))\n\n**ビジネスやユースケース観点から除外されるべきすべてのカラムを除外するようにしてください。**\n\n```py\nimport databricks.lakehouse_monitoring as lm\n```\n\n```py\n# 分析するウィンドウのサイズ\nGRANULARITIES = [\"1 day\"]                       \n\n# データをスライスするエクスプレッション\nSLICING_EXPRS = [\"type='Red'\"]  \n```\n\n```py\nprint(f\"Creating monitor for {TABLE_NAME}\")\n\ninfo = lm.create_monitor(\n  table_name=TABLE_NAME,\n  profile_type=lm.TimeSeries(\n    timestamp_col=TIMESTAMP_COL,\n    granularities=GRANULARITIES\n  ),\n  slicing_exprs=SLICING_EXPRS,\n  baseline_table_name=BASELINE_TABLE,\n  output_schema_name=f\"{CATALOG}.{SCHEMA}\"\n)\n```\n\nカタログエクスプローラからプライマリテーブルにアクセスし、**品質**タブをクリックするとモニターを確認できます。\n![Screenshot 2023-12-16 at 16.53.30.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8a374dd7-2f33-faf9-be1a-fc9d8ab86f3e.png)\n![Screenshot 2023-12-16 at 16.53.38.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ac812ecc-0df5-5e13-1e24-f6fe9f9ed039.png)\n\n```py\nimport time\n\n\n# モニターの作成を待ちます\nwhile info.status == lm.MonitorStatus.PENDING:\n  info = lm.get_monitor(table_name=TABLE_NAME)\n  time.sleep(10)\n\nassert(info.status == lm.MonitorStatus.ACTIVE)\n```\n\nまた、**更新履歴を表示**をクリックすることでメトリクスの更新処理の状態を確認できます。\n![Screenshot 2023-12-16 at 16.53.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/4af614cc-c456-0c7b-7273-04cd22ae3103.png)\n\nモニターを作成すると、自動でメトリクスの更新処理が起動します。\n\n```py\n# 作成時にメトリックの更新が自動でトリガーされます\nrefreshes = lm.list_refreshes(table_name=TABLE_NAME)\nassert(len(refreshes) > 0)\n\nrun_info = refreshes[0]\nwhile run_info.state in (lm.RefreshState.PENDING, lm.RefreshState.RUNNING):\n  run_info = lm.get_refresh(table_name=TABLE_NAME, refresh_id=run_info.refresh_id)\n  time.sleep(30)\n\nassert(run_info.state == lm.RefreshState.SUCCESS)\n```\n\nモニターを作成するとメトリクスを確認するためのダッシュボードが自動生成されます。ダッシュボードを開くにはセルのアウトプットでハイライトされているダッシュボードのリンクをクリックします。カタログエクスプローラのUIからダッシュボードに移動することもできます。\n![Screenshot 2023-12-16 at 17.24.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/271269f9-6d18-7ef8-7bd5-4af2257ed72c.png)\n\nなお、ダッシュボードやダッシュボードで使用されるクエリーはホームディレクトリ配下の`databricks_lakehouse_monitoring`に格納されます。\n![Screenshot 2023-12-16 at 16.54.28.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f4065d99-857c-93b4-c0cf-8fcce4d34718.png)\n\nダッシュボードにはデータの統計情報などが表示されます。\n![Screenshot 2023-12-16 at 17.01.35.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/fad02c5c-fc1b-07d1-f99e-a42bedba9c19.png)\n![Screenshot 2023-12-16 at 17.01.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e462f1e2-4913-d198-3a1c-b55581594c93.png)\n\n以下のメソッドでモニターの情報を表示できます。\n\n```py\nlm.get_monitor(table_name=TABLE_NAME)\n```\n![Screenshot 2023-12-16 at 17.26.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6248b02e-f12a-9756-b11b-219caf5b9387.png)\n\n## 3. メトリクステーブルの調査\n\nデフォルトではメトリクステーブルはdefaultデータベースに格納されます。\n\n`create_monitor`の呼び出しで2つの新規テーブル作成されます: プロファイルメトリクステーブルとドリフトメトリクステーブルです。\n\nこれらの2つのテーブルは分析ジョブのアウトプットを記録します。このテーブルでは監視されるプライマリテーブルと同じ名前を使用し、サフィックス`_profile_metrics`と`_drift_metrics`を使用します。\n\n### プロファイルメトリクステーブルのオリエンテーション\n\nプロファイルメトリクステーブルにはサフィックス`_profile_metrics`が追加されます。テーブルに表示される統計情報の一覧に関しては、ドキュメントを参照ください([AWS](https://docs.databricks.com/ja/lakehouse-monitoring/monitor-output.html#profile-metrics-table)|[Azure](https://learn.microsoft.com/ja-jp/azure/databricks/lakehouse-monitoring/monitor-output#profile-metrics-table))。\n\n- プライマリテーブルのすべてのカラムに対して、ベースラインテーブルとプライマリテーブルに対するサマリーの統計情報を表示します。カラム`log_type`はプライマリテーブルの統計情報を示す`INPUT`、ベースラインテーブルの統計情報を示す`BASELINE`を表示しています。プライマリテーブルのカラムはカラム`column_name`で識別できます。\n- `TimeSeries`タイプの分析では、`granularity`カラムには行に対する粒度が表示されます。ベースラインテーブルの統計情報では`granularity`カラムの値は`null`になります。\n- このテーブルでは、それぞれの時間ウィンドウにおけるそれぞれのスライスキーのそれぞれの値に対する統計情報とテーブル全体に対する統計情報が表示されます。テーブル全体の統計情報は`slice_key` = `slice_value` = `null`で特定することができます。\n- プライマリテーブルでは、`window`カラムに当該行に対応する時間ウィンドウが表示されます。ベースラインテーブルの統計情報では、`window`カラムは`null`になります。\n- いくつかの統計情報は特定のカラムではなくテーブル全体に対して計算されます。`column_name`カラムの`:table`によって、これらの統計情報を特定できます。\n\n```py\n# プロファイルメトリクステーブルを表示\nprofile_table = f\"{TABLE_NAME}_profile_metrics\"\ndisplay(spark.sql(f\"SELECT * FROM {profile_table}\"))\n```\n![Screenshot 2023-12-16 at 17.02.10.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/33866865-39f5-1870-a9a0-66c9e286b56d.png)\n\n### ドリフトメトリクステーブルのオリエンテーション\n\nドリフトメトリクステーブルにはサフィックス`_drift_metrics`が追加されます。テーブルに表示される統計情報の一覧に関しては、ドキュメントを参照ください([AWS](https://docs.databricks.com/ja/lakehouse-monitoring/monitor-output.html#drift-metrics-table)|[Azure](https://learn.microsoft.com/ja-jp/azure/databricks/lakehouse-monitoring/monitor-output#drift-metrics-table))。\n\n- プライマリテーブルのすべてのカラムに対して、ドリフトテーブルではテーブルの現在の値と以前の分析実行時の値、ベースラインテーブルと比較した一連のメトリクスを表示します。ベースラインテーブルに対するドリフトでは、カラム`drift_type`に`BASELINE`が表示され、以前の時間ウィンドウに対するドリフトでは、`CONSECUTIVE`が表示されます。プロファイルテーブルと同じように、プライマリテーブルのカラムはカラム`column_name`で特定できます。\n  - この時点では、このモニターの最初の実行であるため、比較する以前のウィンドウがありません。このため、`drift_type`が`CONSECUTIVE`である行はありません。\n- `TimeSeries`タイプの分析では、`granularity`カラムには行に対する粒度が表示されます。\n- このテーブルでは、それぞれの時間ウィンドウにおけるそれぞれのスライスキーのそれぞれの値に対する統計情報とテーブル全体に対する統計情報が表示されます。テーブル全体の統計情報は`slice_key` = `slice_value` = `null`で特定することができます。\n- `window`カラムには当該行に対応する時間ウィンドウが表示されます。`window_cmp`カラムには比較対象のウィンドウが表示されます。ベースラインテーブルとの比較の場合には、`window_cmp`は`null`になります。\n- いくつかの統計情報は特定のカラムではなくテーブル全体に対して計算されます。`column_name`カラムの`:table`によって、これらの統計情報を特定できます。\n\n```py\n# ドリフトメトリクステーブルの表示\ndrift_table = f\"{TABLE_NAME}_drift_metrics\"\ndisplay(spark.sql(f\"SELECT * FROM {drift_table}\"))\n```\n\n![Screenshot 2023-12-16 at 17.02.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/028a16ac-9c71-1c2f-b7ad-23f5f660a295.png)\n\n## 4. テーブルに新規データを追加し、メトリクスをリフレッシュ\n\n### テーブルに新規データを追加\n\n以下のセルではプライマリテーブルに、シミュレートした未来のデータ`ts2_df`を追加します。\n\n```py\n(ts2_df\n  .write.format(\"delta\").mode(\"append\") \n  .option(\"mergeSchema\",True) \n  .option(\"delta.enableChangeDataFeed\", \"true\") \n  .saveAsTable(f\"{TABLE_NAME}\")\n)\n```\n\n行数を確認します。\n\n```py\nspark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").display()\n```\n![Screenshot 2023-12-16 at 17.03.05.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6181a471-4196-7730-6907-fd5928e2be48.png)\n\n### メトリクスのリフレッシュ\n\n```py\nrun_info = lm.run_refresh(table_name=TABLE_NAME)\nwhile run_info.state in (lm.RefreshState.PENDING, lm.RefreshState.RUNNING):\n  run_info = lm.get_refresh(table_name=TABLE_NAME, refresh_id=run_info.refresh_id)\n  time.sleep(30)\n\nassert(run_info.state == lm.RefreshState.SUCCESS)\n```\n\n2回目のリフレッシュが実行されます。\n![Screenshot 2023-12-16 at 17.03.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/cf6fde90-ef75-609e-abdf-e1c26892bde5.png)\n\nリフレッシュが完了したら、変化を確認するためにモニタリングダッシュボードを開きます。\n![Screenshot 2023-12-16 at 17.12.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3c4d4caa-1154-02f7-5d89-869ef3c5f7d8.png)\n![Screenshot 2023-12-16 at 17.12.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bc17b16a-1887-6444-8426-9dfdf5edcb95.png)\n\n時間ウィンドウ間の変化を確認することができます。\n![Screenshot 2023-12-16 at 17.13.05.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/660b2daf-a148-ed04-be9e-933e9a2b67c2.png)\n![Screenshot 2023-12-16 at 17.13.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c5bdbf9e-ae53-46ea-5e52-2f9f91b80f9f.png)\n\n## [オプション] モニターの削除\n\nモニターをクリーンアップするには以下のコードのコメントを解除してください。一つのテーブルには単一のモニターのみを追加できます。\n\n```py\n# lm.delete_monitor(table_name=TABLE_NAME)\n```\n\nテーブルの統計情報をモニタリングするためには、これまでには手組みで色々なロジックを構成しないといけませんでしたが、例クハウスモニタリングを活用いただくことで簡単にテーブルの状態を監視できるようになります。(日本リージョンで使えるようになったら)是非お試しください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksレイクハウスモニタリングのご紹介", "text": "[Introduction to Databricks Lakehouse Monitoring \\| Databricks on AWS](https://docs.databricks.com/en/lakehouse-monitoring/index.html) [2023/12/6時点]の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n:::note info\n**プレビュー**\n本機能は以下のリージョンでパブリックプレビューです: eu-central-1, eu-west-1, us-east-1, us-east-2, us-west-2, ap-southeast-2。一覧されているリージョンですべてのワークスペースがサポートされている訳ではありません。レイクハウスモニタリングを使用する際に「Your workspace is hosted on infrastructure that cannot support serverless compute.」というエラーが出る場合には、別のワークスペースを試すか、アカウントチームに問い合わせください。\n:::\n\n本書ではDatabricksレイクハウスモニタリングを説明します。お使いのデータに対するモニタリングのメリットをカバーし、コンポーネントの概要とDatabricksレイクハウスモニタリングの使い方を説明します。\n\nDatabricksレイクハウスモニタリングによって、お使いのアカウントのすべてのテーブルのデータの統計的な属性や品質を監視することができます。また、モデルの入力と予測結果を格納する推論テーブルを監視することで、機械学習モデルとモデルサービングエンドポイントのパフォーマンスを追跡するためにこの機能を活用することができます。以下の図では、DatabricksにおけるデータとMLパイプラインを経由するデータフローと、データ品質とモデルパフォーマンスを継続的に追跡するためにどのようにモニタリングを活用するのかを示しています。\n![](https://docs.databricks.com/en/_images/lakehouse-monitoring-overview.png)\n\n# なぜDatabricksレイクハウスモニタリングを使うのか？\n\nあなたのデータから有用な洞察を導き出すためには、お使いのデータに対して信頼できるようになっている必要があります。お使いのデータをモニタリングすることで、時間が経過したとしてもデータの品質と一貫性を追跡、確認する助けとなる定量的な指標を得ることができます。お使いのテーブルのデータ分布や対応するモデルのパフォーマンスの変化を検知した際、Databricksレイクハウスモニタリングは変化を捕捉、警告することができるので、原意の特定の助けとなります。\n\nDatabricksレイクハウスモニタリングは、以下のような質問に回答する助けとなります:\n\n- データの統一性はどのようなものか、そしてそれはどのように時間とともに変化しているのか？例えば、現在のデータにおけるnull値やゼルの値の割合は、それは増加しているのか？\n- 統計的な分布はどのようになっているか、そしてそれは時間とともにどのように屁kんかしているのか？例えば、数値カラムの90thパーセンタイルは何か？あるいは、カテゴリー値カラムの値の分布はどうなっているか、それは昨日とどのように異なっているか？\n- 現在のデータと既知のベースライン、あるいは次の時間ウィンドウのデータの間にドリフトが生じているか？\n- データのサブセットやスライスの統計的な分布やドリフトはどのようなものか？\n- MLモデルの入力と予測の時間的なシフトはどのようになっているか？\n- モデルパフォーマンスの時間的なトレンドはどのようになっているか？モデルバージョンAはバージョンBよりもパフォーマンスが優れているか？\n\nさらに、Databricksレイクハウスモニタリングを用いることで、観測結果の時間粒度をコントロールしたり、カスタムメトリクスを設定することができます。\n\n# 要件\n\nDatabricksレイクハウスモニタリングを使うには以下の要件があります:\n\n- お使いのワークスペースでUnity Cataログが有効化されており、Databricks SQLにアクセスできる必要があります。\n- Deltaマネージドテーブル、外部テーブル、ビュー、マテリアライズドビューのモニタリングのみがサポートされています。\n\n:::note info\n**注意**\nDatabricksレイクハウスモニタリングはサーバレスジョブコンピュートを使用します。レイクハウスモニタリングの費用の追跡に関しては、[Lakehouse Monitoringの使用量を確認する](https://docs.databricks.com/ja/lakehouse-monitoring/expense.html)をご覧ください。\n:::\n\n# Databricks上でのレイクハウスモニタリングの動作原理\n\nDatabricksのテーブルを監視するには、テーブルにアタッチされるモニターを作成します。機械学習モデルのパフォーマンスを監視するには、モデルの入力と対応する予測結果を保持する推論テーブルにモニターをアタッチします。\n\nDatabricks上でのレイクハウスモニタリングは、以下のタイプの分析を提供します: 時系列、スナップショット、推論です。\n\n| プロファイルタイプ | 説明 |\n|:--|:--|\n| 時系列  | 時間ウィンドウごとのデータ分布を比較します。時間とともにお使いのデータ分布がどのように変化するのかを比較するためにメトリクスをどの粒度(1日など)で計算するのかを指定します。このタイプのプロファイルではタイムスタンプの列が必要となります。  |\n| スナップショット  | 時系列とは異なり、スナップショットプロファイルは、テーブルのすべてのコンテンツが時間とともにどのように変化しているかを監視します。テーブルのすべてのデータに対してメトリクスが計算され、モニターがリフレッシュされる都度、テーブルの状態を監視します。  |\n| 推論  | 機械学習分類、回帰モデルによる予測値の出力を含むテーブルが対象となります。このテーブルには、タイムスタンプ、モデルID、モデルへの入力(特徴量)、モデルの予測結果を含むカラム、ユニークな観測IDや正解ラベルを含むオプションのカラムが含まれます。モデルへの入力には使用されませんが公正性、バイアスの調査やその他のモニタリングで有用な場合があるデモグラフィック情報のようなメタデータを含めることができます。推論プロファイルは時系列プロファイルと似ていますが、モデル品質メトリクスも含まれます。  |\n\nこのセクションでは、Databricksレイクハウスモニタリングで使用される入力テーブルと生成されるメトリックテーブルを簡単に説明します。以下の図では、入力テーブル、メトリックテーブル、モニター、ダッシュボード間の関係を示しています。\n![](https://docs.databricks.com/en/_images/lakehouse-monitoring.png)\n\n## プライマリテーブルとベースラインテーブル\n\n「プライマリテーブル」と呼ばれる監視対象のテーブルに加え、オプションでドリフト計測や値の時間変化のリファレンスとして使用できるベースラインテーブルを指定することができます。ベースラインテーブルは、お使いのデータがどのようなものかを期待できるサンプルがある場合には有用です。期待されるデータ値や分布と比較することでドリフトが計算されることになります。\n\nベースラインテーブルには、統計的な分布、個々のカラムの分布、欠損値、その他の特性において入力データに期待される品質を反映するデータセットを含める必要があります。監視されるテーブルのスキーマと一致しなくてはなりません。例外は、時系列や推論プロファイルで使用されるテーブルのタイムスタンプの絡むです。プライマリテーブルやベースラインに含まれていないカラムが存在する場合、モニタリングでは出力メトリクスを計算するためにベストエフォートのヒューリスティックを用います。\n\nスナップショットプロファイルを使うモニターでは、ベースラインテーブルには許容できる品質基準を表現する分布を持つデータのスナップショットが含まれている必要があります。例えば、評点分布のデータにおいては、評点が均等に分布つしている以前のクラスをベースラインに設定するかもしれません。\n\n時系列プロファイルを使用するモニターでは、ベースラインテーブルには品質基準を表現する分布となっている時間ウィンドウを表現するデータが含まれている必要があります。例えば、気候データにおいては、気温が期待する通常の気温に近い週、月、年にベースラインを設定するかもしれません。\n\n推論プロファイルを使用するモニターでは、ベースラインに好適な選択肢は、監視されるモデルのトレーニング、検証に用いたデータとなります。このようにすることで、モデルのトレーニング、検証に用いたデータと比較してデータがドリフトを起こしている際に、ユーザーがアラートを受け取ることができます。このテーブルには、プライマリテーブルと同じ特徴量のカラムが含まれる必要があり、データ一貫性を持って集計されるようにプライマリテーブルのInferenceLogに指定されたのと同じ`model_id_col`を追加で含める必要があります。理想的には、モデル品質メトリクスを比較できるようにするために、モデルの評価に使用されたテストセット、検証セットを用いるべきです。\n\n## メトリックテーブルとダッシュボード\n\nテーブルのモニターは2つのメトリックテーブルとダッシュボードを作成します。メトリックの値はテーブル全体に対して計算され、対象はモニターを作成時に指定する時間ウィンドウやデータのサブセット(あるいはスライス)が対象となります。さらに、推論の分析では、それぞれのモデルIDごとにメトリクスが計算されます。メトリックテーブルの詳細については、[メトリクステーブルを監視する](https://docs.databricks.com/ja/lakehouse-monitoring/monitor-output.html)をご覧ください。\n\n- プロファイルメトリクステーブルにはサマリーの統計情報が格納されます。[プロファイルメトリクステーブルのスキーマ](https://docs.databricks.com/ja/lakehouse-monitoring/monitor-output.html#profile-metrics-table-schema)をご覧ください。\n- ドリフトメトリクステーブルには、時間経過によるドリフトに関する統計情報が格納されます。ベースラインテーブルが指定されると、ベースラインの値に対してもドリフトが監視されます。[ドリフトメトリクステーブルのスキーマ](https://docs.databricks.com/ja/lakehouse-monitoring/monitor-output.html#drift-metrics-table-schema)をご覧ください。\n\nメトリックテーブルはDeltaテーブルであり、指定したUnity Catalogのスキーマに格納されます。DatabricksのUIを用いてこれらのテーブルを参照し、Databricks SQLを用いてクエリーを行い、これらをベースとしたダッシュボードやアラートを作成することができます。\n\nそれぞれのモニターに対して、監視結果を可視化し、表示する助けになるダッシュボードをDatabricksは自動で作成します。このダッシュボードは他の[Databricks SQLのダッシュボード](https://docs.databricks.com/ja/sql/user/dashboards/index.html)と同じように完全にカスタマイズすることができます。\n\n# Databricksでレイクハウスモニタリングを使い始める\n\n使い始めるには以下のドキュメントをご覧ください:\n\n- [Databricks UIを使用したモニターの作成](https://docs.databricks.com/ja/lakehouse-monitoring/create-monitor-ui.html)\n- [APIを使用したモニターの作成](https://docs.databricks.com/ja/lakehouse-monitoring/create-monitor-api.html)\n- [メトリクステーブルのモニタリング](https://docs.databricks.com/ja/lakehouse-monitoring/monitor-output.html)\n- [生成されたSQLダッシュボードを使用する](https://docs.databricks.com/ja/lakehouse-monitoring/monitor-dashboard.html)\n- [アラートの監視](https://docs.databricks.com/ja/lakehouse-monitoring/monitor-alerts.html)\n- [Databricksレイクハウスモニタリングでカスタムメトリクスを使用する](https://docs.databricks.com/ja/lakehouse-monitoring/custom-metrics.html)\n- [モデルのモニタリングとデバッグのための推論テーブル](https://docs.databricks.com/ja/machine-learning/model-serving/inference-tables.html)\n- [分類モデルの公平性とバイアスを監視する](https://docs.databricks.com/ja/lakehouse-monitoring/fairness-bias.html)\n- [Lakehouse Monitoring API Reference — Lakehouse Monitoring documentation](https://api-docs.databricks.com/python/lakehouse-monitoring/latest/index.html)のマテリアル\n- [サンプルノートブック](https://docs.databricks.com/ja/lakehouse-monitoring/create-monitor-api.html#example-notebooks)\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Delta Lakeチュートリアルのウォークスルー", "text": "復習も兼ねてこちらをウォークスルーします。\n\nhttps://docs.databricks.com/ja/delta/tutorial.html\n\n「Delta Lakeって結局なんだって」となることもあり、こちらの本でも復習してみました。無料でダウンロードできます。\n\nhttps://www.databricks.com/resources/ebook/delta-lake-running-oreilly\n\nDelta Lakeを理解するには[レイクハウスの概念](https://www.databricks.com/jp/product/data-lakehouse)の理解も重要です。\n\nデータウェアハウス、データレイクに次ぐアーキテクチャであるレイクハウス、上記書籍のP14に説明があります。\n\n> レイクハウスとは、ACIDトランザクション、データのバージョン管理、監査、インデックス、キャッシュ、クエリーの最適化のような分析DBMSの管理機能やパフォーマンス機能も提供する低コストかつ直接アクセス可能なストレージをベースにして構築されたデータ管理システムです。\n\nですので、レイクハウスのデータストアはストレージ、一般的にはクラウドのオブジェクトストレージです。データをオブジェクトストレージに格納しながらもDBMSの機能を活用できます。\n\nでは、このような仕組みをどのように実装するのか？となるわけですが、Databricksでは[Delta Lake](https://delta.io/)を用いてこれを実現しています。\n\nDelta Lakeの説明も上記書籍のP18にあります。\n\n> Delta Lakeは、データレイクのストレージフォーマットとメタデータ、キャッシュ機能、インデックス機能を組み合わせたオープンなテーブルフォーマットです。これらは、ACIDトランザクションやその他の管理機能を提供するための抽象化レベルを提供します。<中略>Delta Lakeは、ACIDトランザクション、スケーラブルなメタデータ操作、バッチからストリーミングをカバーする統合処理モデル、完全の監査履歴、SQLのデータ操作言語(DML)のサポートを提供します。\n\nDelta Lake形式でデータを格納することで、上述の機能を活用することができ、データはオブジェクトストレージに格納しておきながらも、DBMSで必要となる機能を利用できるようになります。これによって、上述のレイクハウスを実現しているわけです。\n\nこのことから、以下のチュートリアルではテーブルを操作していく流れとなります。\n\n# テーブルの作成\n\n```py:Python\n# ソースからのデータロード\ndf = spark.read.load(\"/databricks-datasets/learning-spark-v2/people/people-10m.delta\")\n\n# テーブルへのデータの書き込み\ntable_name = \"takaakiyayoi_catalog.delta_tutorial.people_10m\"\ndf.write.saveAsTable(table_name)\n```\n\nテーブルが作成されました。\n![Screenshot 2023-12-15 at 17.31.05.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e93ea1a2-9718-bd15-95cf-ccfad7dc4c39.png)\n\n# テーブルのメタデータの表示\n\nSQLの`DESCRIBE DETAIL`を使います。\n\n```py:Python\ndisplay(spark.sql('DESCRIBE DETAIL takaakiyayoi_catalog.delta_tutorial.people_10m'))\n```\n![Screenshot 2023-12-15 at 17.33.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c98b6ef8-1309-bae0-c0c0-0777e7bcfc98.png)\n\n# テーブルのUPSERT\n\nUPSERTはUPDATE + INSERTです。更新対象のテーブルにマッチするテーブルがあればUPDATE、マッチしない場合にはINSERTを行います。\n\n以下の例では最初のブロックで一時ビュー`people_updates`を作成しています。一時ビューとはSparkセッションでのみ有効なビューです。その後のブロックで`MERGE INTO`でUPSERTを実行します。\n\n```sql:SQL\n%sql\nCREATE OR REPLACE TEMP VIEW people_updates (\n  id, firstName, middleName, lastName, gender, birthDate, ssn, salary\n) AS VALUES\n  (9999998, 'Billy', 'Tommie', 'Luppitt', 'M', '1992-09-17T04:00:00.000+0000', '953-38-9452', 55250),\n  (9999999, 'Elias', 'Cyril', 'Leadbetter', 'M', '1984-05-22T04:00:00.000+0000', '906-51-2137', 48500),\n  (10000000, 'Joshua', 'Chas', 'Broggio', 'M', '1968-07-22T04:00:00.000+0000', '988-61-6247', 90000),\n  (20000001, 'John', '', 'Doe', 'M', '1978-01-14T04:00:00.000+000', '345-67-8901', 55500),\n  (20000002, 'Mary', '', 'Smith', 'F', '1982-10-29T01:00:00.000+000', '456-78-9012', 98250),\n  (20000003, 'Jane', '', 'Doe', 'F', '1981-06-25T04:00:00.000+000', '567-89-0123', 89900);\n\nMERGE INTO takaakiyayoi_catalog.delta_tutorial.people_10m\nUSING people_updates\nON people_10m.id = people_updates.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *;\n```\n\n```sql:SQL\n%sql\nCREATE OR REPLACE TEMP VIEW people_updates (\n  id, firstName, middleName, lastName, gender, birthDate, ssn, salary\n) AS VALUES\n  (9999998, 'Billy', 'Tommie', 'Luppitt', 'M', '1992-09-17T04:00:00.000+0000', '953-38-9452', 55250),\n  (9999999, 'Elias', 'Cyril', 'Leadbetter', 'M', '1984-05-22T04:00:00.000+0000', '906-51-2137', 48500),\n  (10000000, 'Joshua', 'Chas', 'Broggio', 'M', '1968-07-22T04:00:00.000+0000', '988-61-6247', 90000),\n  (20000001, 'John', '', 'Doe', 'M', '1978-01-14T04:00:00.000+0000', '345-67-8901', 55500),\n  (20000002, 'Mary', '', 'Smith', 'F', '1982-10-29T01:00:00.000+0000', '456-78-9012', 98250),\n  (20000003, 'Jane', '', 'Doe', 'F', '1981-06-25T04:00:00.000+0000', '567-89-0123', 89900);\n\nMERGE INTO takaakiyayoi_catalog.delta_tutorial.people_10m\nUSING people_updates\nON people_10m.id = people_updates.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *;\n```\n\n確認のために以下のSQLを実行します。\n\n```sql:SQL\n%sql\nSELECT\n  firstName,\n  middleName,\n  lastName,\n  salary\nFROM\n  takaakiyayoi_catalog.delta_tutorial.people_10m\nWHERE\n  id in (\n    9999998,\n    9999999,\n    10000000,\n    20000001,\n    20000002,\n    20000003\n  )\n```\n\n3行のみがヒットします。\n![Screenshot 2023-12-15 at 17.58.39.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c0463e67-0528-2990-2aeb-3a0f66667c2f.png)\n\n上の`MERGE`を実行します。そして、再度SELECTを実行します。\n\n```sql:SQL\n%sql\nSELECT\n  firstName,\n  middleName,\n  lastName,\n  salary\nFROM\n  takaakiyayoi_catalog.delta_tutorial.people_10m\nWHERE\n  id in (\n    9999998,\n    9999999,\n    10000000,\n    20000001,\n    20000002,\n    20000003\n  )\n```\n\n既存のレコードのsalaryが更新され、新たに3行が追加されました。\n\n![Screenshot 2023-12-15 at 18.19.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c293da21-9f1b-df56-ca3b-1016868721b0.png)\n\n# テーブルの読み込み\n\n```py:Python\npeople_df = spark.read.table(table_name)\n\ndisplay(people_df)\n```\n![Screenshot 2023-12-15 at 18.22.13.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/59b5547a-29f2-6811-3c03-6f35645bf6df.png)\n\nSQLでも同じ結果を得ることができます。\n\n```sql:SQL\n%sql\nSELECT * FROM takaakiyayoi_catalog.delta_tutorial.people_10m;\n```\n\n\n# テーブルの更新\n\n`UPDATE`で行けます。\n\n```sql:SQL\n%sql\nUPDATE takaakiyayoi_catalog.delta_tutorial.people_10m SET gender = 'Female' WHERE gender = 'F';\nUPDATE takaakiyayoi_catalog.delta_tutorial.people_10m SET gender = 'Male' WHERE gender = 'M';\n```\n\n`gender`列が置き換えられました。\n![Screenshot 2023-12-15 at 18.26.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7a064871-bb32-8807-4f07-130a1a9edf4c.png)\n\n# テーブル履歴の表示\n\nDelta Lakeのテーブルでは更新処理がすべて記録されます。`DESCRIBE HISTORY`で履歴を表示できます。\n\n```sql\n%sql\nDESCRIBE HISTORY takaakiyayoi_catalog.delta_tutorial.people_10m\n```\n![Screenshot 2023-12-15 at 18.28.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c5e02f74-fac7-0e81-75ee-042e7805d389.png)\n\n# 特定バージョンのテーブルのクエリー(タイムトラベル)\n\nバージョン番号を指定してクエリーを行うことができます。\n\n```sql:SQL\n%sql\nSELECT\n  *\nFROM\n  takaakiyayoi_catalog.delta_tutorial.people_10m VERSION AS OF 0\nWHERE\n  id in (\n    9999998,\n    9999999,\n    10000000,\n    20000001,\n    20000002,\n    20000003\n  )\n```\n\n[UPSERT](#テーブルのupsert)前の状態が表示されます。\n![Screenshot 2023-12-15 at 18.30.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/db162196-94f6-877d-28f7-7200e5ae3ae8.png)\n\n# Optimizeによるテーブルの最適化\n\nDelta Lakeの実態はオブジェクトストレージに保存されるファイルです。これに対して様々な更新処理を行うとファイルは小さいものに断片化していきます。この状態では複数ファイルにアクセスすることになり、検索性能の悪化につながります。\n\nテーブルの更新で断片化したファイルをコンパクトにまとめることで検索性能を改善できます。`OPTIMIZE`コマンドを使います。\n\n```sql:SQL\n%sql\nOPTIMIZE takaakiyayoi_catalog.delta_tutorial.people_10m;\n```\n\n# Z-orderによる検索性能改善\n\nさらに検索性能を改善するために、検索キーの値が近しいファイルを局所化することができます。これがZ-orderingです。\n\n```sql:SQL\n%sql\nOPTIMIZE takaakiyayoi_catalog.delta_tutorial.people_10m\nZORDER BY (gender)\n```\n![Screenshot 2023-12-15 at 18.35.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bf0f64e4-3c7d-dfa8-f40d-a7d1378079a7.png)\n\n`OPTIMIZE`コマンドも処理に時間を要しますので、深夜のバッチなどで動かすことが推奨となります。\n\n# Vacuumによるクリーンアップ\n\nDelta Lakeテーブルを更新していくと、参照されないファイルなども発生します。これら古くなったファイルを削除するためには`Vacuum`コマンドを使用します。これによって、ストレージコストを削減できます。\n\n```sql:SQL\n%sql\nVACUUM takaakiyayoi_catalog.delta_tutorial.people_10m\n```\n\n(理論上)容量無制限で低コストなオブジェクトストレージ上で様々なデータを効率にハンドリングできるようになるDelta Lake、ぜひお試しください！DatabricksではデフォルトフォーマットがDelta Lakeなので何も気にせずに活用いただけます。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksでMLflowとLLMを用いたRAGシステムの評価(日本語編)", "text": "こちらの日本語編です。\n\nhttps://qiita.com/taka_yayoi/items/110818e5691be47fd96a\n\n前半は同じです。\n\n```py\n%pip install chromadb==0.4.15\ndbutils.library.restartPython()\n```\n\n```py\nimport os\nos.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(\"demo-token-takaaki.yayoi\", \"openai\")\n```\n\n```py\nimport pandas as pd\n\nimport mlflow\n```\n\n```py\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.llms import OpenAI\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n```\n\nDatabricksの日本語マニュアルのページを指定します。\n\n```py\nloader = WebBaseLoader(\"https://docs.databricks.com/ja/introduction/index.html\")\n\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\n\nembeddings = OpenAIEmbeddings()\ndocsearch = Chroma.from_documents(texts, embeddings)\n\nqa = RetrievalQA.from_chain_type(\n    llm=OpenAI(temperature=0),\n    chain_type=\"stuff\",\n    retriever=docsearch.as_retriever(),\n    return_source_documents=True,\n)\n```\n\n```py\ndef model(input_df):\n    answer = []\n    for index, row in input_df.iterrows():\n        answer.append(qa(row[\"questions\"]))\n\n    return answer\n```\n\n評価用データセットを作成します。ここの文字列を長くするとLLMの最長トークン数に引っかかってエラーになってしまいました。どうしたものか。\n\n```py\neval_df = pd.DataFrame(\n    {\n        \"questions\": [\n            \"Databricksとは？\",\n            \"価格は？\",\n        ],\n    }\n)\n```\n\n例も日本語で指定します。\n\n```py\nfrom mlflow.metrics.genai import faithfulness, EvaluationExample\n\n# この問題の文脈における faithfulness の良い例と悪い例を作成します\nfaithfulness_examples = [\n    EvaluationExample(\n        input=\"Databricksとは？\",\n        output=\"Databricksは自動車です。\",\n        score=1,\n        justification=\"アウトプットはコンテキストに示されている情報を用いておらず間違った回答をしています。\",\n        grading_context={\n            \"context\": \"Databricksは、エンタープライズグレードのデータ分析とAIソリューションを大規模に構築、デプロイ、共有、保守するための、統合されたオープンなアナリティクスプラットフォームです。 Databricksデータインテリジェンスプラットフォームは、クラウドアカウントのクラウドストレージおよびセキュリティと統合し、ユーザーに代わってクラウドインフラストラクチャを管理およびデプロイします。\"\n        },\n    ),\n    EvaluationExample(\n        input=\"Databricksとは？\",\n        output=\"Databricksはデータ分析とAIソリューションの開発・運用を実現するデータインテリジェンスプラットフォームです。\",\n        score=5,\n        justification=\"アウトプットはコンテキストに示されている情報を用いてDatabricksを説明しています。\",\n        grading_context={\n            \"context\": \"Databricksは、エンタープライズグレードのデータ分析とAIソリューションを大規模に構築、デプロイ、共有、保守するための、統合されたオープンなアナリティクスプラットフォームです。 Databricksデータインテリジェンスプラットフォームは、クラウドアカウントのクラウドストレージおよびセキュリティと統合し、ユーザーに代わってクラウドインフラストラクチャを管理およびデプロイします。\"\n        },\n    ),\n]\n\nfaithfulness_metric = faithfulness(model=\"openai:/gpt-4\", examples=faithfulness_examples)\nprint(faithfulness_metric)\n```\n\n```\nEvaluationMetric(name=faithfulness, greater_is_better=True, long_name=faithfulness, version=v1, metric_details=\nTask:\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called faithfulness based on the input and output.\nA definition of faithfulness and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must also justify your score.\n\nExamples could be included below for reference. Make sure to use them as references and to\nunderstand them before completing the task.\n\nInput:\n{input}\n\nOutput:\n{output}\n\n{grading_context_columns}\n\nMetric definition:\nFaithfulness is only evaluated with the provided output and provided context, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not present in the output.\n\nGrading rubric:\nFaithfulness: Below are the details for different scores:\n- Score 1: None of the claims in the output can be inferred from the provided context.\n- Score 2: Some of the claims in the output can be inferred from the provided context, but the majority of the output is missing from, inconsistent with, or contradictory to the provided context.\n- Score 3: Half or more of the claims in the output can be inferred from the provided context.\n- Score 4: Most of the claims in the output can be inferred from the provided context, with very little information that is not directly supported by the provided context.\n- Score 5: All of the claims in the output are directly supported by the provided context, demonstrating high faithfulness to the provided context.\n\nExamples:\n\nInput:\nDatabricksとは？\n\nOutput:\nDatabricksはデータ分析とAIソリューションの開発・運用を実現するデータインテリジェンスプラットフォームです。\n\nAdditional information used by the model:\nkey: context\nvalue:\nDatabricksは、エンタープライズグレードのデータ分析とAIソリューションを大規模に構築、デプロイ、共有、保守するための、統合されたオープンなアナリティクスプラットフォームです。 Databricksデータインテリジェンスプラットフォームは、クラウドアカウントのクラウドストレージおよびセキュリティと統合し、ユーザーに代わってクラウドインフラストラクチャを管理およびデプロイします。\n\nscore: 5\njustification: アウトプットはコンテキストに示されている情報を用いてDatabricksを説明しています。\n        \n\nYou must return the following fields in your response one below the other:\nscore: Your numerical score for the model's faithfulness based on the rubric\njustification: Your step-by-step reasoning about the model's faithfulness score\n    )\n```\n\n```py\nfrom mlflow.metrics.genai import relevance, EvaluationExample\n\n\nrelevance_metric = relevance(model=\"openai:/gpt-4\")\nprint(relevance_metric)\n```\n\n評価します。\n\n```py\nresults = mlflow.evaluate(\n    model,\n    eval_df,\n    model_type=\"question-answering\",\n    evaluators=\"default\",\n    predictions=\"result\",\n    extra_metrics=[faithfulness_metric, relevance_metric, mlflow.metrics.latency()],\n    evaluator_config={\n        \"col_mapping\": {\n            \"inputs\": \"questions\",\n            \"context\": \"source_documents\",\n        }\n    },\n)\nprint(results.metrics)\n```\n\n```py\ndisplay(results.tables[\"eval_results_table\"])\n```\n\n回答は一部英語になっていますが、適切に評価されているようです。\n![Screenshot 2023-12-14 at 18.17.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/17ccedd9-0b9f-79e4-170f-5b824cc5446b.png)\n\n特に2つ目の質問に関しては、元の情報に価格が含まれていないことから適当なことを返すことなしに、`I don't know.`を返しています。\n![Screenshot 2023-12-14 at 18.16.54.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/46b8e05b-0ca0-680d-3c86-0d4a967293f4.png)\n\nこれに対しても、LLMが以下の通り公正な回答を行なっていると評価して5点をつけています。\n> The output \"I don't know\" does not make any factual claims, so it does not contradict or misrepresent the context provided. Therefore, it is fully faithful to the context, even though it does not utilize the information in the context.\n\n> アウトプット「わかりません」では、いかなる架空の主張を行なっていませんので、矛盾や指定されたコンテキストの誤読はありません。このため、コンテキストの情報は活用していないとしても、完全に公正なものです。\n\nしかし、これってLLMシステムを定量的に評価できるようになったってことなんですよね。すごい。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksでMLflowとLLMを用いたRAGシステムの評価", "text": "こちらのノートブックをウォークスルーします。Retrieval Augumented Generation(RAG)システムの評価にLLMを使うような時代になったとは。\n\nhttps://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/index.html#rag-evaluation-notebook-using-gpt-4-as-judge\n\nこのノートブックでは、MLflowを用いてどのように様々なRAGシステムを評価するのかをデモンストレーションします。[LLM-as-a-judge](https://arxiv.org/abs/2306.05685)の手法を用いています。\n\nchromadbをインストールします。あとで[エラー](https://github.com/langchain-ai/langchain/issues/13061)に遭遇するので明示的にバージョンを指定します。\n\n```py\n%pip install chromadb==0.4.15\ndbutils.library.restartPython()\n```\n\nOpenAI (あるいは Azure OpenAI) の環境変数を設定します。\n\n```py\nimport os\nos.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(\"demo-token-takaaki.yayoi\", \"openai\")\n\n# If using Azure OpenAI\n# os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n# os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n# os.environ[\"OPENAI_API_KEY\"] = \"https://<>.<>.<>.com\"\n# os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = \"deployment-name\"\n```\n\n```py\nimport pandas as pd\n\nimport mlflow\n```\n\n# RAGシステムの作成\n\nMLflowのドキュメントをベースとして質疑応答を行うRAGシステムを作成するためにLangchainとChromaを使います。\n\n```py\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.llms import OpenAI\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n```\n\n```py\nloader = WebBaseLoader(\"https://mlflow.org/docs/latest/index.html\")\n\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\n\nembeddings = OpenAIEmbeddings()\ndocsearch = Chroma.from_documents(texts, embeddings)\n\nqa = RetrievalQA.from_chain_type(\n    llm=OpenAI(temperature=0),\n    chain_type=\"stuff\",\n    retriever=docsearch.as_retriever(),\n    return_source_documents=True,\n)\n```\n\n# `mlflow.evaluate()`を用いたRAGシステムの評価\n\nRAGチェーンを通じてそれぞれの入力を処理するシンプルな関数を作成します。\n\n```py\ndef model(input_df):\n    answer = []\n    for index, row in input_df.iterrows():\n        answer.append(qa(row[\"questions\"]))\n\n    return answer\n```\n\n評価データセットを作成します。\n\n```py\neval_df = pd.DataFrame(\n    {\n        \"questions\": [\n            \"What is MLflow?\",\n            \"How to run mlflow.evaluate()?\",\n            \"How to log_table()?\",\n            \"How to load_table()?\",\n        ],\n    }\n)\n```\n\n`faithfulness`メトリックを作成します。インプット`input`に対するアウトプット`output`の例、その際の評点`score`と理由づけ`justification`を定義します。\n\n```py\nfrom mlflow.metrics.genai import faithfulness, EvaluationExample\n\n# この問題の文脈における faithfulness の良い例と悪い例を作成します\nfaithfulness_examples = [\n    EvaluationExample(\n        input=\"How do I disable MLflow autologging?\",\n        output=\"mlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default. \",\n        score=2,\n        justification=\"The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\",\n        grading_context={\n            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n        },\n    ),\n    EvaluationExample(\n        input=\"How do I disable MLflow autologging?\",\n        output=\"mlflow.autolog(disable=True) will disable autologging for all functions.\",\n        score=5,\n        justification=\"The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\",\n        grading_context={\n            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n        },\n    ),\n]\n\nfaithfulness_metric = faithfulness(model=\"openai:/gpt-4\", examples=faithfulness_examples)\nprint(faithfulness_metric)\n```\n\nLLMがJudge(審判)として動作するようにタスクが定義されます。\n```\nEvaluationMetric(name=faithfulness, greater_is_better=True, long_name=faithfulness, version=v1, metric_details=\nTask:\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called faithfulness based on the input and output.\nA definition of faithfulness and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must also justify your score.\n\nExamples could be included below for reference. Make sure to use them as references and to\nunderstand them before completing the task.\n\nInput:\n{input}\n\nOutput:\n{output}\n\n{grading_context_columns}\n```\n\n`relevance`メトリックを作成します。メトリックの `metric_details` にアクセスする、あるいはメトリックを print することで完全な評点のプロンプトを確認することができます。\n\n```py\nfrom mlflow.metrics.genai import relevance, EvaluationExample\n\n\nrelevance_metric = relevance(model=\"openai:/gpt-4\")\nprint(relevance_metric)\n```\n\n```\nEvaluationMetric(name=relevance, greater_is_better=True, long_name=relevance, version=v1, metric_details=\nTask:\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called relevance based on the input and output.\nA definition of relevance and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must also justify your score.\n\nExamples could be included below for reference. Make sure to use them as references and to\nunderstand them before completing the task.\n\nInput:\n{input}\n\nOutput:\n{output}\n\n{grading_context_columns}\n\nMetric definition:\nRelevance encompasses the appropriateness, significance, and applicability of the output with respect to both the input and context. Scores should reflect the extent to which the output directly addresses the question provided in the input, given the provided context.\n\nGrading rubric:\nRelevance: Below are the details for different scores:- Score 1: The output doesn't mention anything about the question or is completely irrelevant to the provided context.\n- Score 2: The output provides some relevance to the question and is somehow related to the provided context.\n- Score 3: The output mostly answers the question and is largely consistent with the provided context.\n- Score 4: The output answers the question and is consistent with the provided context.\n- Score 5: The output answers the question comprehensively using the provided context.\n\nExamples:\n\nInput:\nHow is MLflow related to Databricks?\n\nOutput:\nDatabricks is a data engineering and analytics platform designed to help organizations process and analyze large amounts of data. Databricks is a company specializing in big data and machine learning solutions.\n\nAdditional information used by the model:\nkey: context\nvalue:\nMLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n\nscore: 2\njustification: The output provides relevant information about Databricks, mentioning it as a company specializing in big data and machine learning solutions. However, it doesn't directly address how MLflow is related to Databricks, which is the specific question asked in the input. Therefore, the output is only somewhat related to the provided context.\n        \n\nInput:\nHow is MLflow related to Databricks?\n\nOutput:\nMLflow is a product created by Databricks to enhance the efficiency of machine learning processes.\n\nAdditional information used by the model:\nkey: context\nvalue:\nMLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n\nscore: 4\njustification: The output provides a relevant and accurate statement about the relationship between MLflow and Databricks. While it doesn't provide extensive detail, it still offers a substantial and meaningful response. To achieve a score of 5, the response could be further improved by providing additional context or details about how MLflow specifically functions within the Databricks ecosystem.\n        \n\nYou must return the following fields in your response one below the other:\nscore: Your numerical score for the model's relevance based on the rubric\njustification: Your step-by-step reasoning about the model's relevance score\n    )\n```\n\n```py\nresults = mlflow.evaluate(\n    model,\n    eval_df,\n    model_type=\"question-answering\",\n    evaluators=\"default\",\n    predictions=\"result\",\n    extra_metrics=[faithfulness_metric, relevance_metric, mlflow.metrics.latency()],\n    evaluator_config={\n        \"col_mapping\": {\n            \"inputs\": \"questions\",\n            \"context\": \"source_documents\",\n        }\n    },\n)\nprint(results.metrics)\n```\n\n```\n{'toxicity/v1/mean': 0.00020886990023427643, 'toxicity/v1/variance': 4.508897658386363e-09, 'toxicity/v1/p90': 0.0002846180694177747, 'toxicity/v1/ratio': 0.0, 'faithfulness/v1/mean': 3.0, 'faithfulness/v1/variance': 4.0, 'faithfulness/v1/p90': 5.0, 'relevance/v1/mean': 3.5, 'relevance/v1/variance': 2.25, 'relevance/v1/p90': 4.7}\n```\n\nMLflowにも記録されます。\n![Screenshot 2023-12-14 at 16.33.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/810b61b4-26eb-c64c-b277-2670c4a31295.png)\n\n```py\ndisplay(results.tables[\"eval_results_table\"])\n```\n![Screenshot 2023-12-14 at 16.34.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/63061853-d1b2-06a6-8127-bfb9a7a0325f.png)\n\n評価の根拠も`justification`として表示されます。\n![Screenshot 2023-12-14 at 16.35.03.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/212ab414-257d-0da8-476e-0d95d98e2ac0.png)\n\nこれも評価ビューで比較します。\n![Screenshot 2023-12-14 at 16.36.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2aa4e826-ffa3-23b2-e8e9-2d1531069e46.png)\n\n今度日本語でもトライしてみます。\n↓\nトライしてみました。\n\nhttps://qiita.com/taka_yayoi/items/b7ef9ccd64d4dd662aad\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "MLflowによる大規模言語モデルの評価", "text": "以前こちらの記事を書きました。\n\nhttps://qiita.com/taka_yayoi/items/f06adbf5510703b0510b\n\nマニュアルにも掲載されています。気づいたらMLflowもバージョン2.9.1になってました。\n\nhttps://docs.databricks.com/ja/mlflow/llm-evaluate.html\n\nこちらのノートブックをウォークスルーします。\n\nhttps://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/question-answering-evaluation.html\n\n```py\nimport os\nos.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(\"demo-token-takaaki.yayoi\", \"openai\")\n```\n\n```py\nimport openai\nimport pandas as pd\n\nimport mlflow\n```\n\n```py\neval_df = pd.DataFrame(\n    {\n        \"inputs\": [\n            \"How does useEffect() work?\",\n            \"What does the static keyword in a function mean?\",\n            \"What does the 'finally' block in Python do?\",\n            \"What is the difference between multiprocessing and multithreading?\",\n        ],\n        \"ground_truth\": [\n            \"The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates.\",\n            \"Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.\",\n            \"'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.\",\n            \"Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.\",\n        ],\n    }\n)\n```\n\n```py\nwith mlflow.start_run() as run:\n    system_prompt = \"Answer the following question in two sentences\"\n    basic_qa_model = mlflow.openai.log_model(\n        model=\"gpt-3.5-turbo\",\n        task=openai.ChatCompletion,\n        artifact_path=\"model\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": \"{question}\"},\n        ],\n    )\n    results = mlflow.evaluate(\n        basic_qa_model.model_uri,\n        eval_df,\n        targets=\"ground_truth\",  # specify which column corresponds to the expected output\n        model_type=\"question-answering\",  # model type indicates which metrics are relevant for this task\n        evaluators=\"default\",\n    )\nresults.metrics\n```\n\n```\n{'toxicity/v1/mean': 0.00021154171190573834,\n 'toxicity/v1/variance': 2.705208305975482e-09,\n 'toxicity/v1/p90': 0.0002699144184589386,\n 'toxicity/v1/ratio': 0.0,\n 'exact_match/v1': 0.0}\n```\n\nメトリクスに関してはこちらに説明がありました。\n\nhttps://mlflow.org/docs/latest/llms/llm-evaluate/index.html#use-default-metrics-for-pre-defined-model-types\n\nhttps://huggingface.co/spaces/evaluate-measurement/toxicity\n\n```py\nresults.tables[\"eval_results_table\"]\n```\n![Screenshot 2023-12-14 at 11.47.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6f5549b2-59c1-046d-41e3-189eef63f5b3.png)\n\n評価ビューでも確認できました。\n![Screenshot 2023-12-14 at 11.47.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b189d269-7b0c-b157-0919-ba7439cc3dac.png)\n\nと思ったら、すでに包括的に評価されていらっしゃいました。脱帽です。\n\nhttps://qiita.com/isanakamishiro2/items/8b6214b8dde113cee389\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksマーケットプレイスからネットワンシステムズ様のセンサーデータを分析してみる", "text": "以前、ネットワンシステムズ様に弊社Databricksマーケットプレイスのローンチデータパートナーとして参画いただきました。\n\nhttps://www.netone.co.jp/news/release/20230914_01.html\n\nこちらで公開されています。\n\nhttps://marketplace.databricks.com/details/375b1b8e-6f49-4fcd-bbe6-9636988de574/Net-One-Systems-Co-Ltd_netone-valley-Sensor-Data\n\n> 本データセットは、netone valleyで収集されたセンサーデータを提供します。本データセットを利用する事で、理想的なデータやランダム生成されたデータではなく、実環境のデータ分析を行うことが可能となります。\n> 本パッケージ内のデータには、温度、湿度、人数、環境騒音、二酸化炭素などが含まれています。\n\n# データおよびノートブックの取得\n\n![Screenshot 2023-12-13 at 14.47.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6b6194ab-0667-1db6-d98c-9b1428b8284b.png)\n\n**即時アクセス権を取得**をクリックします。カタログ名は適宜変更します。\n![Screenshot 2023-12-13 at 14.47.35.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3e081da2-4482-9fe3-ada1-c4c93ece91a7.png)\n![Screenshot 2023-12-13 at 14.47.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5d6a0c15-403c-c283-5fbc-83b5bf8124cf.png)\n![Screenshot 2023-12-13 at 14.47.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b76dd69e-6f54-a301-3cce-2cf5fbab3a4e.png)\n\nカタログエクスプローラでテーブルにアクセスできるようになります。\n![Screenshot 2023-12-13 at 14.48.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/583b21f9-200e-108d-7d50-de80b0213f97.png)\n![Screenshot 2023-12-13 at 14.48.45.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0dc2d16e-4559-995d-e667-b2de4e04154e.png)\n\n添付されているノートブックもインポートします。\n\n# ノートブックのウォークスルー\n\n```py\n%pip install prophet\n```\n\n```py\nimport pandas as pd\nfrom prophet import Prophet\nfrom pyspark.sql.functions import *\n```\n\nカタログ名は適宜変更します。\n\n```py\ndf = spark.table(\"net_one_systems_co_ltd_netone_valley_sensor_data_taka.sensor2.counts\")\ndisplay(df)\n```\n![Screenshot 2023-12-13 at 15.19.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2270977e-3c9a-24bc-0712-1ef61b039998.png)\n\n```py\n# area1のデータを取得し、データ一覧の表示とデータの可視化させる\nmax_pop_area1_df = df.filter(df.areaid != \"area1\").select('ts','counts_max')\n\ndisplay(max_pop_area1_df)\n```\n![Screenshot 2023-12-13 at 15.20.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/757bf76c-31bf-e2ea-fa79-2ffa6e13a10b.png)\n![Screenshot 2023-12-13 at 15.20.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e6d1cee6-28ca-f1fd-b192-9b54a7313628.png)\n\n```py\n# 時系列データをふさわしいタイムゾーンに調整\nmax_pop_area1_df = max_pop_area1_df.withColumn(\"Timezone=Asia/Tokyo\", from_utc_timestamp(col('ts'), 'Asia/Tokyo')).sort(desc(\"Timezone=Asia/Tokyo\"))\n\ndisplay(max_pop_area1_df)\n```\n![Screenshot 2023-12-13 at 15.20.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0f9ba878-4c3b-144e-6120-bccb0508bdd4.png)\n\n```py\n# Pyspark DataframeからPandas化させて、Prophetモデルに適合させるためカラム名を変更\narea1_data_df = max_pop_area1_df.toPandas().drop(columns=[\"ts\"]).rename(columns={\"counts_max\":\"y\",\"Timezone=Asia/Tokyo\":\"ds\"})\n\narea1_data_df\n```\n![Screenshot 2023-12-13 at 15.21.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e27f5b26-9902-d46a-5fd3-a3da56bb1d5d.png)\n\n```py\n# データをProphetモデルにfit\nm = Prophet()\nm.fit(area1_data_df)\n```\n\n```py\n# 未来９０日の予測データ結果を生成させる\nfuture = m.make_future_dataframe(periods=90)\n\nforecast = m.predict(future)\n```\n\n```py\n# 人数予測結果を表示\nfig1 = m.plot(forecast)\n```\n![Screenshot 2023-12-13 at 15.22.00.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/19fae6ff-3d08-4e4a-f11c-25e854a15f9d.png)\n\n```py\nfig2 = m.plot_components(forecast)\n```\n![download.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6a578367-372a-2706-54cd-d9bceb5da1dd.png)\n\n[Databricksマーケットプレイス](https://www.databricks.com/jp/product/marketplace)では[プロバイダーを募集](https://www.databricks.com/company/partners/data-partner-program)しています！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksマーケットプレイスからWhisper V3 Modelを試してみる", "text": "久々にマーケットプレイスを覗いたらモデルが増えてました。Whisper V3 Modelとな。\n\nhttps://marketplace.databricks.com/details/1eceaa77-6b60-42f0-9809-ceccf1b237f5/Databricks_Whisper-V3-Model\n\n音声テキスト変換のモデルなんですね。\n\nhttps://huggingface.co/openai/whisper-large-v3\n\n# モデルとノートブックの取得\n\n![Screenshot 2023-12-12 at 11.41.16.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f2dd3b9f-28cd-26f0-41ca-1fdd356d36fe.png)\n\n**即時アクセス権を取得**をクリックして、**その他のオプション**を展開します。モデルは新規の共有カタログ配下に格納されるので、カタログ名を変更します。\n![Screenshot 2023-12-12 at 11.41.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9cb2d4cc-4d0e-5f42-0e65-b9934115d77d.png)\n![Screenshot 2023-12-12 at 11.45.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9d0bb657-aee3-d0b2-342a-be3cd12cf5fc.png)\n\nカタログエクスプローラでモデルにアクセスできます。\n![Screenshot 2023-12-12 at 11.46.15.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/02a82877-4bb0-1c77-0efd-ed62e75ff396.png)\n\nサンプルノートブックもインポートします。\n![Screenshot 2023-12-12 at 11.43.31.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/15b60ee9-5972-1283-b8cf-ce2c797f3831.png)\n\n# ノートブックのウォークスルー\n\nノートブックの実行自体はCPUクラスターで問題ありません。Databricks SDKをインストールします。\n\n```py\n# Upgrade to use the newest Databricks SDK\n%pip install --upgrade databricks-sdk\ndbutils.library.restartPython()\n```\n\n```py\n# Select the model from the dropdown list\nmodel_names = ['whisper_large_v3']\ndbutils.widgets.dropdown(\"model_name\", model_names[0], model_names)\n```\n\nカタログ名は上で設定したものに変更します。必要に応じてモデルサービングエンドポイント名を変更します。\n```py\n# Default catalog name when installing the model from Databricks Marketplace.\n# Replace with the name of the catalog containing this model\n# You can also specify a different model version to load for inference\ncatalog_name = \"databricks_whisper_v3_model_taka\"\nversion = \"1\"\nmodel_name = dbutils.widgets.get(\"model_name\")\nmodel_uc_path = f\"{catalog_name}.models.{model_name}\"\nendpoint_name = f'{model_name}_marketplace_taka'\n```\n\nモデルのサービングにはGPUが必要となります。\n\n```py\n# Choose the right workload types based on the model size\nworkload_type = \"GPU_MEDIUM\"\n```\n\nSDKでモデルサービングエンドポイントをデプロイします。30分くらいかかります。\n\n```py\nimport datetime\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.serving import EndpointCoreConfigInput\nw = WorkspaceClient()\n\nconfig = EndpointCoreConfigInput.from_dict({\n    \"served_models\": [\n        {\n            \"name\": endpoint_name,\n            \"model_name\": model_uc_path,\n            \"model_version\": version,\n            \"workload_type\": workload_type,\n            \"workload_size\": \"Small\",\n            \"scale_to_zero_enabled\": \"False\",\n        }\n    ]\n})\nmodel_details = w.serving_endpoints.create(name=endpoint_name, config=config)\nmodel_details.result(timeout=datetime.timedelta(minutes=30))\n```\n\n![Screenshot 2023-12-12 at 13.27.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6cb963a5-0360-5ac2-56ba-53a0ff80a27f.png)\n\n\n音声ファイル自体にはこちらからアクセスできます。\n\nhttps://huggingface.co/datasets/Nexdata/accented_english\n\n2つの音声ファイルがあります。聞いてみるとバイデン首相に関するニュース記事のようです。\n\nデータセットとしてロードしてモデルサービングエンドポイントにリクエストします。\n```py\nfrom datasets import load_dataset\nimport pandas as pd\nimport base64\nimport json\n\nfrom databricks.sdk import WorkspaceClient\n\ndataset = load_dataset(\"Nexdata/accented_english\", split=\"train\")\nsample_path = dataset[0][\"audio\"][\"path\"]\n\n# Change it to your own input file name\nwith open(sample_path, 'rb') as audio_file:\n    audio_bytes = audio_file.read()\n    audio_b64 = base64.b64encode(audio_bytes).decode('ascii')\n\ndataframe_records = [audio_b64]\n\nw = WorkspaceClient()\nresponse = w.serving_endpoints.query(\n    name=endpoint_name,\n    dataframe_records=dataframe_records,\n)\nprint(response.predictions)\n```\n\n```py\n[\"The news forced the state to move immediately from planning the site's operational processes to building software, Baden said.\"]\n```\n\n(私の英語力では)合っています。\n\nSparkのUDF(ユーザー定義関数)でモデルをラッピングすることで、バッチ推論で活用する小tゴアできます。\n\n```py\nimport mlflow\nmlflow.set_registry_uri(\"databricks-uc\")\n\ncatalog_name = \"databricks_whisper_v3_models_taka\"\ntranscribe = mlflow.pyfunc.spark_udf(spark, f\"models:/{model_uc_path}/{version}\", \"string\")\n```\n\n```py\nimport pandas as pd\nfrom datasets import load_dataset\nimport base64\nimport json\n\ndataset = load_dataset(\"Nexdata/accented_english\", split=\"train\")\nsample_path = dataset[0][\"audio\"][\"path\"]\n\nwith open(sample_path, 'rb') as audio_file:\n    audio_bytes = audio_file.read()\n    dataset = pd.DataFrame(pd.Series([audio_bytes]))\n\ndf = spark.createDataFrame(dataset)\n\n# You can use the UDF directly on a text column\ntranscribed_df = df.select(transcribe(df[\"0\"]).alias('transcription'))\n\ndisplay(transcribed_df)\n```\n\nデータフレームに格納されている音声データからテキストに起こしてくれます。\n![Screenshot 2023-12-12 at 13.24.39.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6920f80c-55f4-fe2e-2eff-193262d21b82.png)\n\n複数行であっても同じように処理してくれます。\n\n```py\ndataset = load_dataset(\"Nexdata/accented_english\", split=\"train\")\n\nsample_path_1 = dataset[0][\"audio\"][\"path\"]\nsample_path_2 = dataset[1][\"audio\"][\"path\"]\n\nwith open(sample_path_1, 'rb') as audio_file_1:\n    audio_bytes_1 = audio_file_1.read()\n\nwith open(sample_path_2, 'rb') as audio_file_2:\n    audio_bytes_2 = audio_file_2.read()\n\ndataset = pd.DataFrame(pd.Series([audio_bytes_1, audio_bytes_2]))\n\ndf = spark.createDataFrame(dataset)\n\n# You can use the UDF directly on a text column\ntranscribed_df = df.select(transcribe(df[\"0\"]).alias('transcription'))\n\ndisplay(transcribed_df)\n```\n\nこれはこれで夢が広がりますね。\n![Screenshot 2023-12-12 at 13.25.44.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/18a6346c-1c10-8585-da18-2fe096d1761d.png)\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksノートブックにおけるinputの活用", "text": "Databricksを使い続けて3年経って、Databricksノートブックで普通に`input`を使えることを初めて知りました。\n\nhttps://www.python.jp/train/string/input.html\n\n[Databricks SDK for Python](https://docs.databricks.com/ja/dev-tools/sdk-python.html)を使います。\n\n```py\n%pip install databricks-sdk --upgrade\ndbutils.library.restartPython()\n```\n\n以下のように入力を受け付けたい場所で`input`を使います。ジョブを作成する際にパラメータを指定します。\n\n```py\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.jobs import Task, NotebookTask, Source\n\nw = WorkspaceClient()\n\njob_name            = input(\"ジョブの名称 (例 my-job): \")\ndescription         = input(\"ジョブの説明文 (例 My job): \")\nexisting_cluster_id = input(\"ジョブを実行するワークスペースの既存クラスターID (例 1234-567890-ab123cd4): \")\nnotebook_path       = input(\"実行するノートブックのワークスペースパス (例 /Users/someone@example.com/my-notebook): \")\ntask_key            = input(\"ジョブのタスクに適用するキー(タスク名) (for example, my-key): \")\n\nprint(\"ジョブの作成を試みます。お待ちください...\\n\")\n\nj = w.jobs.create(\n  name = job_name,\n  tasks = [\n    Task(\n      description = description,\n      existing_cluster_id = existing_cluster_id,\n      notebook_task = NotebookTask(\n        base_parameters = {\"n_estimators\": 50},\n        notebook_path = notebook_path,\n        source = Source(\"WORKSPACE\")\n      ),\n      task_key = task_key\n    )\n  ]\n)\n\nprint(f\"View the job at {w.config.host}/#job/{j.job_id}\\n\")\n```\n\n上のセルを実行すると入力ボックスが表示されます。値を入力してEnterを押します。\n![Screenshot 2023-12-12 at 9.46.38.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b33e110d-0c93-1498-4bb1-b2d933849e26.png)\n\n順次入力していきます。\n![Screenshot 2023-12-12 at 9.47.41.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/64d5ebd7-f5c8-9eca-2377-171a87c99d33.png)\n\n全て入力するとジョブが作成されます。\n![Screenshot 2023-12-12 at 9.48.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/665c29be-cedf-03b4-0535-0c8632bdbe71.png)\n\n指定した内容でジョブが作成されました！\n![Screenshot 2023-12-12 at 9.49.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/edd2c229-01ce-46e7-8ecf-f6072d05a8bb.png)\n\nパラメータをクイックに変更して試行錯誤したい場合には有効ですね。って、もっと早く気づけという話ですが。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "リアルタイム特徴量サービングにおけるオンラインテーブルの活用", "text": "[Use online tables for real\\-time feature serving \\| Databricks on AWS](https://docs.databricks.com/en/machine-learning/feature-store/online-tables.html) [2023/12/8時点]の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n:::note info\n**プレビュー**\nオンラインテーブルは機能制限のあるパブリックプレビューです。限定プレビューの間は、オンラインテーブルはSQLサーバレスDBUを消費します。オンラインテーブルの最終的な価格は今後明らかになります。プレビューに参加するには[こちら](https://forms.gle/9jLZkpXnJF9ZcxtQA)からサインアップしてください。\n\nオンラインテーブルのプレビューは以下のリージョンで利用できます: us-east-1, us-west-2, eu-west-1, ap-southeast-2\n:::\n\nオンラインテーブルとは、オンラインアクセスに最適化された行指向フォーマットで格納されるDeltaテーブルの読み取り専用のコピーです。オンラインテーブルは、リクエストの負荷に応じてスループットの能力をオートスケールする完全にサーバレスなテーブルであり、いかなる規模のデータに対して低いレーテンシーと高いスループットを提供します。オンラインテーブルは、高速なデータ検索のために、Databricks Model Serving、Feature & Function Serving、retrieval-augmented generation (RAG)アプリケーションと動作するように設計されています。\n\nオンラインテーブルの作成は1ステップのプロセスです。カタログエクスプローラでDeltaテーブルを選択し、**Create online table**を選択するだけです。\n\n# 要件\n\n- ワークスペースでUnity Catalogが有効化されていること。Unity Catalogメタストアの作成、ワークスペースでの有効化、カタログの作成方法については[ドキュメント](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html)をご覧ください。\n- Databricks管理者は、アカウントコンソールでサーバレスの利用状況に同意する必要があります。\n\n# オンラインテーブルの操作\n\nこのセクションではオンラインテーブルの作成、削除の方法、ステータスのチェック方法、オンラインテーブルのアップデートの起動方法を説明します。\n\n## 作成\n\nカタログエクスプローラからオンラインテーブルを作成します。必要な権限については、[ユーザーのアクセス権](#ユーザーのアクセス権)をご覧ください。\n\n1. カタログエクスプローラで、オンラインテーブルに同期したいソーステーブルに移動します。![](https://docs.databricks.com/en/_images/kebab-menu.png)ケバブメニューから**Create online table**を選択します。\n![](https://docs.databricks.com/en/_images/create-online-table.png)\n1. オンラインテーブルを設定するためにダイアログのセレクターを使用します。\n![](https://docs.databricks.com/en/_images/create-online-table-dlg.png)\n    **Name:** Unity Catalogのオンラインテーブルの名称。\n    **Primary Key:** オンラインテーブルの主キーとして使用するソーステーブルのカラム。\n    **Timeseries Key:** (オプション)時系列のキーとして使用するソーステーブルのカラム。指定された場合、オンラインテーブルにはそれぞれの主キーごとに最新の時系列キーの値を持つ行のみが含まれます。\n    **Sync mode:** 同期パイプラインがオンラインテーブルをどのようにアップデートするのかを指定します。**Snapshot**、**Triggered**、**Continuous**のいずれかを選択します。\n\n    | ポリシー | 説明 |\n    |:--|:--|\n    |Snapshot   | ソーステーブルのスナップショットを取得し、オンラインテーブルにコピーするためにパイプラインは一度実行されます。新たにソーステーブルのスナップショットを取得し、新規コピーを作成することで、ソーステーブルに対する以降の変更はオンラインテーブルに反映されます。オンラインテーブルのコンテンツは原子的に更新されます。  |\n    | Triggered  | オンラインテーブルでソーステーブルの初期コピーを作成するためにパイプラインが一度実行されます。スナップショットの同期モードとは異なり、オンラインテーブルがリフレッシュされた際、前回のパイプライン実行以来の変更のみが取得され、オンラインテーブルに適用されます。インクリメンタルなリフレッシュは手動、あるいはスケジュールに基づいて自動でトリガーすることができます。  |\n    | Continuous  | パイプラインは継続的に稼働します。ソーステーブルに対する以降の変更はリアルタイムのストリーミングモードでオンラインテーブルに適用されます。手動のリフレッシュは不要です。  |    \n\n    :::note\n    **注意**\n    **Triggered**、**Continuous**同期モードをサポートするには、ソーステーブルでは[Change data feed](https://docs.databricks.com/en/delta/delta-change-data-feed.html)が有効化されている必要があります。\n   :::\n\n1. 設定が完了したら、**Confirm**をクリックします。オンラインテーブルのページが表示されます。\n1. ダイアログで指定したカタログ、スキーマに指定された名前で、新たなオンラインテーブルが作成されます。カタログエクスプローラでは、オンラインテーブルは![](https://docs.databricks.com/en/_images/online-table-icon.png)として表示されます。\n    \n## ステータスのチェックとアップデートのトリガー\n\nオンラインテーブルのステータスをチェックするには、カタログでテーブル名をクリックします。オンラインテーブルのページが**Overview**タブにオープンされます。**Data Ingest**セクションには最新のアップデートのステータスが表示されます。アップデートをトリガーするには、**Sync now**をクリックします。また、**Data Ingest**セクションにはテーブルのアップデートを行うDelta Live Tablesパイプラインへのリンクが含まれています。\n![](https://docs.databricks.com/en/_images/online-table-in-catalog.png)\n\n## 削除\n\nオンラインテーブルページで、![](https://docs.databricks.com/en/_images/kebab-menu.png)ケバブメニューから**Delete**を選択します。\n\n# 特徴量サービングエンドポイントを用いたオンラインテーブルのサービング\n\nDatabricks外でホスティングされているモデルやアプリケーションに対して、オンラインテーブルから特徴量をサービングするための特徴量サービングエンドポイントを作成することができます。エンドポイントによって、REST APIを用いた低レーテンシーの特徴量利用が可能となります。\n\n1. `feature spec`の作成。\n    `feature spec`を作成する際、使用するソーステーブルの名前を指定します。すると、`feature spec`はソーステーブル名を用いてUnity Catalogに新規feature spec関数を作成します。これによって、オンラインとオフラインシナリオの両方で`feature spec`を活用することができます。オンラインの検索では、`feature spec`を用いるサービングエンドポイントは低レーテンシーの特徴量検索を実行するために、オンラインバージョンのテーブルを自動で活用します。\n\n    カタログエクスプローラの**Funtion**タブで`feature spec`を参照することができます。\n\n    ```py:Python\n    fe = FeatureEngineeringClient()\n    fe.create_feature_spec(\n      name=\"catalog.default.user_preferences_spec\",\n      features=[\n        FeatureLookup(\n          table_name=\"user_preferences\",\n          lookup_key=\"user_id\"\n        )\n      ]\n    )\n    ```\n\n1. 特徴量サービングエンドポイントの作成。\n    このステップでは、Deltaテーブル`user_preferences`からのデータを同期するオンラインテーブル`user_preferences_online_table`を作成しているものとします。特徴量サービングエンドポイントを作成するには`feature spec`を使用します。このエンドポイントによって、関連づけられたオンラインテーブルを用いてREST API経由でデータを利用できるようになります。\n\n    :::note\n    **注意**\n    このオペレーションを行うユーザーはオフラインテーブルとオンラインテーブル両方のオーナーである必要があります。\n   :::\n\n    ```py:Python\n    fe.create_feature_serving_endpoint(\n    name = \"user-preferences\",\n    config=EndpointCoreConfig(\n      served_entities=ServedEntity(\n        feature_spec_name=\"catalog.default.user_preferences_spec\",\n        workload_size=\"Small\",\n        scale_to_zero_enabled=True)\n      )\n    )\n    ```\n\n1. 特徴量サービングエンドピントからのデータの取得。\n    APIエンドポイントにアクセスするには、最初にOAuth認証を用いたリクエスtの認証を行い、HTTP GETリクエストを行います。\n\n    ```py:Python\n    url = \"https://{workspace_url}/serving-endpoints/user-preferences/invocations\"\n    databricks_token = xxxxxxx\n\n    headers = {'Authorization': f'Bearer {databricks_token}', 'Content-Type': 'application/json'}\n\n    data = {\n      \"dataframe_records\": [{\"user_id\": user_id}]\n    }\n    data_json = json.dumps(data, allow_nan=True)\n\n    response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n    if response.status_code != 200:\n      raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n\n    print(response.json()['outputs'][0]['hotel_preference'])\n    ```\n\n# RAGアプリケーションによるオンラインテーブルの活用\n\nRAGアプリケーションはオンラインテーブルの一般的なユースケースとなります。RAGアプリケーションが必要とする構造化データのオンラインテーブルを作成し、特徴量サービングエンドポイントにホスティングします。RAGアプリケーションはオンラインテーブルから適切なデータを検索するために、特徴量サービングエンドポイントを使用します。\n\n典型的なステップは以下のようになります:\n\n1. 特徴量サービングエンドポイントの作成。\n1. 適切なデータを検索するためにエンドポイントを使用するLangChainToolの作成。\n1. 適切なデータを取得するためのLangChainエージェントで上記ツールを使用。\n1. LangChainアプリケーションをホストするためのモデルサービングエンドポイントを作成。\n\nステップバイステップの手順については以下のサンプルノートブックをご覧ください。\n\n# サンプルノートブック: オンラインテーブル\n\n以下のノートブックでは、retrieval augmented generation (RAG)アプリケーションでのDatabricksオンラインテーブルや特徴量サービングエンドポイントの使い方を説明しています。\n\n## RAGアプリケーションを用いたオンラインテーブルのデモノートブック\n\nhttps://docs.databricks.com/_extras/notebooks/source/machine-learning/structured-data-for-rag.html\n\n# Databricksモデルサービングによるオンラインテーブルの活用\n\nDatabricks Model Servingの特徴量検索でオンラインテーブルを活用することができます。オンラインテーブルに特徴量テーブルを同期すると、当該特徴量テーブルの特徴量を用いてトレーニングされたモデルは、推論の際に自動でオンラインテーブルから特徴量の値を検索します。追加の設定は不要です。\n\n1. モデルのトレーニングで`FeatureLookup`を使う。\n    モデルのトレーニングでは、以下の例のようにモデルのトレーニングセットでオフラインの特徴量テーブルの特徴量を使います:\n\n    ```py:Python\n    training_set = fe.create_training_set(\n      df=id_rt_feature_labels,\n      label='quality',\n      feature_lookups=[\n          FeatureLookup(\n              table_name=\"user_preferences\",\n              lookup_key=\"user_id\"\n          )\n      ],\n      exclude_columns=['user_id'],\n    )\n    ```\n\n1. Databricks Model Servingでモデルをサービングする。モデルは自動でオンラインテーブルから特徴量を検索します。詳細は[Automatic feature lookup with MLflow models on Databricks](https://docs.databricks.com/en/machine-learning/feature-store/automatic-feature-lookup.html)をご覧ください。\n\n# ユーザーのアクセス権\n\nオンラインテーブルを作成するには以下の権限が必要です:\n\n- ソーステーブルに対する`SELECT`権限。\n- 目的のカタログに対する`USE_CATALOG`権限。\n- 目的のスキーマに対する`USE_SCHEMA`と`CREATE_TABLE`権限。\n\nオンラインテーブルに対するデータ同期パイプラインを管理するには、オンラインテーブルのオーナーであるか、オンラインテーブルに対する`REFLESH`権限が必要です。カタログに対する`USE_CATALOG`と`USE_SCHEMA`権限を持たないユーザーはカタログエクスプローラでオンラインテーブルを参照することはできません。\n\nUnity Catalogメタストアは[Privilege Model Version 1\\.0](https://docs.databricks.com/en/data-governance/unity-catalog/manage-privileges/upgrade-privilege-model.html)である必要があります。\n\n# エンドポイントの権限モデル\n\nデータをクエリーし、関数を実行するために必要な限定的な権限を持つ特徴量サービングやモデルサービングのエンドポイント向けには自動でユニークなサービスプリンシパルが作成されます。このサービスプリンシパルによって、リソースを作成したユーザーとは独立したデータと関数のリソースにエンドポイントがアクセスでき、作成者がワークスペースを去った後でもエンドポイントが動作し続けることを保証します。\n\nこのシステムサービスプリンシパルのライフタイムは、エンドポイントのライフタイムと同一です。監査ログには、Unity Catalogのカタログのオーナーがこのシステムサービスプリンシパルに必要な権限を付与したことを示すシステム生成レコードが表示されることがあります。\n\nプロダクション環境の管理の一般的な情報については、[Recommendations for production serving endpoints](https://docs.databricks.com/en/security/auth-authz/access-control/serving-endpoint-acl.html#recommendations-for-production-serving-endpoints)をご覧ください。\n\n# 制限\n\n- ソーステーブルごとに1つのオンラインテーブルのみがサポートされます。\n- ARRAY、MAP、STRUCTのデータタイプのカラムをオンラインテーブルの主キーとして使用することはできません。\n- オンラインテーブルの主キーとしてあるカラムが使用されている場合、ソーステーブルのそのカラムでnull値を含むすべての行は無視されます。\n- ソーステーブルとして、外部テーブル、システムテーブル、内部テーブルはサポートされません。\n- Deltaのチェンジデータフィードが有効化されていないソーステーブルでは、**Snapshot**同期モードしかサポートされません。\n- ソーステーブルとオンラインテーブルのカタログ、スキーマ、テーブル名には英数字とアンダースコアのみが許可されており、数字でスタートしてはいけません。\n- Stringタイプのカラムは64KB長に制限されます。\n- 行の最大サイズは2MBです。\n- ゲーテッドパブリックプレビューにおけるオンラインテーブルの最大サイズは200GBの非圧縮ユーザーデータとなります。\n- ゲーテッドパブリックプレビューにおけるUnity Catalogメタストアにおけるすべてのオンラインテーブルの合計サイズは1TBとなります。\n\n# トラブルシューティング\n\n## カタログエクスプローラで「Create online table」が表示されない\n\n通常この原因は、同期しようとしているテーブルがサポートされていないタイプによるものです。(カタログエクスプローラの**Details**タブに表示される)テーブルのセキュリティ保護種別が以下のいずれかであることを確認してください:\n\n- TABLE_EXTERNAL\n- TABLE_DELTA\n- TABLE_DELTA_EXTERNAL\n- TABLE_DELTASHARING\n- TABLE_DELTASHARING_MUTABLE\n- TABLE_STREAMING_LIVE_TABLE\n- TABLE_STANDARD\n- TABLE_FEATURE_STORE\n- TABLE_FEATURE_STORE_EXTERNAL\n- TABLE_VIEW\n- TABLE_VIEW_DELTASHARING\n- TABLE_MATERIALIZED_VIEW\n\n## オンラインテーブル作成時に「Triggered」や「Continuous」同期モードを選択できない\n\nソーステーブルでDeltaチェンジデータフィードが有効化されていない場合、あるいはビューやマテリアライズドビューの場合に発生します。**Incremental**同期モードを使うには、ソーステーブルでチェンジデータフィードを有効化するか、ビューではないテーブルを使います。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks Feature & Function Servingとは？", "text": "[What is Databricks Feature & Function Serving? \\| Databricks on AWS](https://docs.databricks.com/en/machine-learning/feature-store/feature-function-serving.html) [2023/12/8時点]の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n:::note info\n**プレビュー**\n本機能は[パブリックプレビュー](https://docs.databricks.com/release-notes/release-types.html)です。\n:::\n\nDatabricks Feature & Function Servingによって、Databricks外にデプロイされたモデルやアプリケーションでDatabricksプラットフォームのデータを利用できるようになります。Feature & Function Servingエンドポイントは、リアルタイムのトラフィックに合わせて自動でスケールし、高可用性かつ低レーテンシーの特徴量サービングを提供します。このページでは、Feature & Function Servingのセットアップ方法を説明します。\n\nDatabricksの特徴量を用いて構築されたモデルをサービングするためにDatabricksモデルサービングを使う際、モデルは推論リクエストに対して自動で特徴量を検索、変換します。Databricks Feature & Function Servingを用いることで、retrieval augmented generation (RAG)アプリケーションのための構造化データや、Unity Catalogにあるデータをベースとした特徴量を必要とするDatabricks外のモデルや、その他のアプリケーションでサービングされるモデルが必要とする特徴量をサービングすることができます。\n![](https://docs.databricks.com/en/_images/when-to-use-feature-serving.png)\n\n# なぜ、Feature & Function Servingを使うのか？\n\nDatabricks Feature & Function Servingは、事前にマテリアライズされた特徴量やオンデマンドの特徴量をサービングする単一のインタフェースを提供します。また、以下のようなメリットを提供します:\n\n- シンプルさ。Databricksがインフラストラクチャの面倒を見ます。単一のAPIコールによって、プロダクションレベルのサービングエンドポイントが作成されます。\n- 高可用性と高いスケーラビリティ。Feature & Function Servingエンドポイントは、サービングのリクエストのボリュームに応じて自動でスケールアップ、スケールダウンします。\n- セキュリティ。エンドポイントはセキュアなネットワーク領域にデプロイされ、エンドポイントが削除されるか、ゼロにスケールする際には停止される専用計算資源を使用します。\n\n# 要件\n\n- Databricks Runtime 14.2 ML以降\n- Feature & Function Servingでは、`databricks-feature-store`バージョン0.16.2以降が必要です。Databricks Runtime 14.2 MLにはバージョン0.16.1が含まれています。必要なバージョンを手動でインストールするには、`%pip install databricks-feature-store>=0.16.2`を実行します。Databricksノートブックを使っている場合には、新規セルで`dbutils.library.restartPython()`を実行してPythonカーネルを再起動する必要があります。\n\n# サンプルノートブック\n\n以下のノートブックでは、Feature & Function Servingエンドポイントの作成方法を説明しています。\n\n## Feature & Function Servingサンプルノートブック\n\nhttps://docs.databricks.com/_extras/notebooks/source/machine-learning/feature-function-serving-aws.html\n\n# `FeatureSpec`の作成\n\n`FeatureSpec`は特徴量と関数に対するユーザー定義セットとなります。`FeatureSpec`で特徴量と関数を結合することができます。`FeatureSpec`はUnity Catalogで管理され、カタログエクスプローラに表示されます。\n\n`FeatureSpec`で指定されるテーブルは、オンラインストアに公開される必要があります。特徴量のオンラインストアへの公開方法に関しては、[Publish features to an online store](https://docs.databricks.com/en/machine-learning/feature-store/publish-features.html)をご覧ください。\n\n```py:Python\nfrom databricks.feature_engineering import (\n  FeatureFunction,\n  FeatureLookup,\n  FeatureEngineeringClient,\n)\n\nfe = FeatureEngineeringClient()\n\nfeatures = [\n  # Lookup column `average_yearly_spend` and `country` from a table in UC by the input `user_id`.\n  FeatureLookup(\n    table_name=\"main.default.customer_profile\",\n    lookup_key=\"user_id\",\n    features=[\"average_yearly_spend\", \"country\"]\n  ),\n  # Calculate a new feature called `spending_gap` - the difference between `ytd_spend` and `average_yearly_spend`.\n  FeatureFunction(\n    udf_name=\"main.default.difference\",\n    output_name=\"spending_gap\",\n    # Bind the function parameter with input from other features or from request.\n    # The function calculates a - b.\n    input_bindings={\"a\": \"ytd_spend\", \"b\": \"average_yearly_spend\"},\n  ),\n]\n\n# Create a `FeatureSpec` with the features defined above.\n# The `FeatureSpec` can be accessed in Unity Catalog as a function.\nfe.create_feature_spec(\n  name=\"main.default.customer_features\",\n  features=features,\n)\n```\n\n# エンドポイントの作成\n\n`FeatureSpec`でエンドポイントを定義します。詳細については、[Create and manage model serving endpoints](https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html)と[the API documentation](https://api-docs.databricks.com/python/feature-store/latest/feature_store.endpoint_core_config.html)をご覧ください。\n\n```py:Python\nfrom databricks.feature_engineering.entities.feature_serving_endpoint import (\n  ServedEntity,\n  EndpointCoreConfig,\n)\n\nfe.create_feature_serving_endpoint(\n  name=\"customer-features\",\n    config=EndpointCoreConfig(\n    served_entities=ServedEntity(\n      feature_spec_name=\"main.default.customer_features\",\n             workload_size=\"Small\",\n             scale_to_zero_enabled=True,\n             instance_profile_arn=None,\n    )\n  )\n)\n```\n\nエンドポイントを確認するには、Databricks UIの左のサイドバーで**Serving**をクリックします。状態が**Ready**になったら、エンドポイントにクエリーを行うことができます。Databricksモデルサービングの詳細については、[Databricks Model Serving](https://docs.databricks.com/en/machine-learning/model-serving/index.html)をご覧ください。\n\n# エンドポイントの取得\n\nエンドポイントのメタデータと状態を取得するには、API `get_feature_serving_endpoint`を使います。\n\n```py:Python\nendpoint = fe.get_feature_serving_endpoint(name=\"customer-features\")\n# print(endpoint)\n```\n\n# エンドポイントへのクエリー\n\nサービングエンドポイントを試す最も簡単な方法はサービングUIを使うというものです:\n\n1. Databricksワークスペースの左のナビゲーションバーで**Serving**をクリックします。\n1. クエリーを行いたいエンドポイントをクリックします。\n1. 画面右上の**Query endpoint**をクリックします。\n![](https://docs.databricks.com/en/_images/query-endpoint-button.png)\n1. **Request**ボックスにJSONフォーマットでリクエストボディを入力します。\n1. **Send request**をクリックします。\n\n    ```json:JSON\n    // Example of a request body.\n    {\n      \"dataframe_records\": [\n        {\"user_id\": 1, \"ytd_spend\": 598},\n        {\"user_id\": 2, \"ytd_spend\": 280}\n      ]\n    }\n    ```\n    **Query endpoint**ダイアログには、curl、Python、SQLでのサンプルコードが含まれています。サンプルコードを参照、コピーするにはタブをクリックします。\n![](https://docs.databricks.com/en/_images/query-endpoint-dialog.png)\n    コードをコピーするには、テキストボックスの右上のコピーアイコンをクリックします。\n![](https://docs.databricks.com/en/_images/query-endpoint-dialog-with-code.png)\n\n# エンドポイントの削除\n\n:::note alert\n**警告！**\nこの操作は取り消せません。\n:::\n\n```py:Python\nfe.delete_feature_serving_endpoint(name=\"customer-features\")\n```\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksアシスタントの新たなUIを試してみる", "text": "こちらのアップデートで、Databricksアシスタントがデフォルトで有効化されました。なお、管理者画面からオプトアウト(無効化)することが可能です。\n\nhttps://docs.databricks.com/ja/release-notes/product/2023/november.html#ai-assistive-features-are-enabled-by-default\n\nそして、最近になってノートブックセルの変化に気づきました。右上がキラキラしてる。\n![Screenshot 2023-12-10 at 17.50.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/59ca707d-8ec7-45e6-053f-810a5c5b732d.png)\n\n**アシスタントを切り替え**とな。オンにしてみます。\n![Screenshot 2023-12-10 at 17.50.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/718b67f4-d5d1-0965-a87c-a1175500c6fd.png)\n\n何か出ました。アシスタントに尋ねるかコマンドのために`/`を入力だそうで。\n![Screenshot 2023-12-10 at 17.51.31.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/15c59e76-c4e4-6845-eb0e-f48f3e59de7d.png)\n\n`/`を入力します。`/doc`はコードにコメントを追加、`/explain`はチャットウィンドウでコードを説明、`/fix`はコードのエラーを修正。\n![Screenshot 2023-12-10 at 17.53.13.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8e514327-6c6e-d3a8-63cb-9f96e3df9ba1.png)\n![Screenshot 2023-12-10 at 17.53.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/25e76708-40cb-d802-7a36-9e45ddf1d049.png)\n![Screenshot 2023-12-10 at 17.53.31.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/724d133c-2664-107a-f2c5-955109842b06.png)\n\n説明と修正はこれまでも使えていましたが、`/doc`はどうなるのだろうって、なんか出た。コードにコメントを挿入してくれるんですね。嬉しいですが英語対応してほしい…。\n![Screenshot 2023-12-10 at 17.55.00.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/39c39b19-0fe7-4f45-61c8-2d8c8e870240.png)\n\n承認するとコメントが挿入されます。\n![Screenshot 2023-12-10 at 17.55.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8953421e-826b-3bee-9deb-ea64f96a01be.png)\n\n説明は上で言われていた通りコードの説明が表示されます。\n![Screenshot 2023-12-10 at 17.56.28.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/993c7d25-56d4-e1a9-800f-ec4f2089f3bc.png)\n\nエラーが起きた際には`/fix`を入力することで、修正案を提示してくれます。\n![Screenshot 2023-12-10 at 18.03.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5b44a9ab-9e64-1448-963d-085fb692eb17.png)\n\n\nそして、普通にアシスタントに問い合わせをするとコードを生成してくれます。SDKでのクラスターの作り方を聞いてみました。\n![Screenshot 2023-12-10 at 17.58.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/37fbe361-4da3-84f2-8391-437e195c8c5c.png)\n\nますますコーディングが捗ってしまいます。\n\nと、これを書いてからすでに検証されている方がいることに気づきました。不覚(ありがとうございます)\n\nhttps://qiita.com/isanakamishiro2/items/c7a9530205e503d4985e#_reference-e7e39f74f8cf06fb19e7\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks SDK for Pythonによるジョブ作成の効率化", "text": "以前こちらの会社ブログを執筆しました。\n\nhttps://www.databricks.com/jp/blog/2023/08/07/databricks-workflow-new-features\n\n> 実際にDatabricksを利用されているJapan Digital Design株式会社においても、こちらの並列処理の機能を活用して、さまざまな機械学習モデルのトレーニングの処理を並列化することで、使用計算資源の最適化、全体的な処理に要する時間の削減を実現されています。\n\nこちらの記事を書いておいてなんですが、「具体的にこれを実装するにはどうしたらいいのか」と思い立ちました。\n\nマルチタスクジョブで並列処理をGUIから作成することはできますが、このタスクが数十になると手動で作成するのが手間です。ここでは、[Databricks SDK for Python](https://docs.databricks.com/en/dev-tools/sdk-python.html)を活用します。\n\n# Databricks SDK for Pythonとは\n\n名前の通り、Pythonから直接DatabricksのリソースをコントロールできるSDKです。プロダクション用途でご利用いただけますがベータ版です。\n\nhttps://pypi.org/project/databricks-sdk/\n\nドキュメントはこちら。SDKを使う際にはこちらのリファレンスが欠かせません。\n\nhttps://databricks-sdk-py.readthedocs.io/en/latest/\n\n# やりたいこと\n\nある機械学習モデルをトレーニングするノートブックで、あるパラメータを変えながらトレーニングをしたいのですが、この処理を並列化したいものとします。[こちら](https://qiita.com/taka_yayoi/items/22f31dbc6e48a92d8192)のノートブックの簡素版です。そして、並列処理の前処理、後処理のためのノートブックも実行できるようにします。\n\n```py:ml-quickstart-training-jpn\nimport mlflow\nimport numpy as np\nimport pandas as pd\nimport sklearn.datasets\nimport sklearn.metrics\nimport sklearn.model_selection\nimport sklearn.ensemble\n\nfrom hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\nfrom hyperopt.pyll import scope\n\n# データのロードと前処理\nwhite_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-white.csv\", sep=';')\nred_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-red.csv\", sep=';')\nwhite_wine['is_red'] = 0.0\nred_wine['is_red'] = 1.0\ndata_df = pd.concat([white_wine, red_wine], axis=0)\n\n# ワイン品質に基づいた分類ラベルの定義\ndata_labels = data_df['quality'] >= 7\ndata_df = data_df.drop(['quality'], axis=1)\n\n# 80/20でトレーニング/テストデータセットを分割\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n  data_df,\n  data_labels,\n  test_size=0.2,\n  random_state=1\n)\n\n# このノートブックでのMLflow autologgingを有効化\nmlflow.autolog()\n```\n\nジョブでパラメータを受け取れるように、[dbutils.widgets.get](https://docs.databricks.com/ja/workflows/jobs/create-run-jobs.html#pass-parameters-to-a-databricks-job-task)を使ってエスティメータの数を受け取ります\n\n```py::ml-quickstart-training-jpn\n# ジョブパラメータとしてn_estimatorsを受け取ります\nn_estimators_para = dbutils.widgets.get(\"n_estimators\")\n\n# 新たなランをスタートし、後でわかるようにrun_nameを割り当てます\nwith mlflow.start_run(run_name=f'gradient_boost_{n_estimators_para}') as run:\n  model = sklearn.ensemble.GradientBoostingClassifier(\n    random_state=0, \n    \n    # n_estimatorsで新たなパラメータ設定をトライします\n    n_estimators = int(n_estimators_para),\n  )\n  model.fit(X_train, y_train)\n\n  predicted_probs = model.predict_proba(X_test)\n  roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])\n  mlflow.log_metric(\"test_auc\", roc_auc)\n  print(\"Test AUC of: {}\".format(roc_auc))\n```\n\n# SDKによるジョブの作成\n\n新規にノートブックを作成してSDKをインストールします。\n\n```py\n%pip install databricks-sdk --upgrade\ndbutils.library.restartPython()\n```\n\nライブラリのインポートやジョブで使用するノートブックを設定します。今回は前処理、後処理のノートブックでは`print`で文字列を表示しているだけですが、必要に応じて実装することも可能です。\n\n```py\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.jobs import Task, NotebookTask, Source, TaskDependency\nfrom databricks.sdk.service.compute import ClusterSpec, RuntimeEngine\n\nw = WorkspaceClient()\n\njob_name = \"ML parallel tasks job\"\nstart_task_description = \"開始タスク\"\nexecution_task_description = \"並列で処理を行うタスク\"\nfinish_task_description = \"終了タスク\"\n# 既存クラスターを使用する場合にはクラスターIDを指定\nexisting_cluster_id = \"1118-004519-nu0u899x\"\n\n# 開始ノートブック: 前処理などを記述\nstart_notebook_path = \"/Users/takaaki.yayoi@databricks.com/20231210_workflow/start\"\n# 並列実行するノートブック\nexecution_notebook_path = \"/Users/takaaki.yayoi@databricks.com/20231210_workflow/ml-quickstart-training-jpn\"\n# 終了ノートブック: 後処理などを記述\nfinish_notebook_path = \"/Users/takaaki.yayoi@databricks.com/20231210_workflow/finish\"\n```\n\nジョブの作成時には既存のクラスターも指定できますが、そうすると並列化の意味がありません。ここでは、新規のジョブクラスターを作成するようにします。新規クラスターの指定に必要な情報を取得しておきます。\n\n```py\n# ノードタイプ一覧\nnodes = w.clusters.list_node_types()\nnodes\n```\n```\nListNodeTypesResponse(node_types=[NodeType(node_type_id='r3.xlarge', memory_mb=31232, num_cores=4.0, description='r3.xlarge (deprecated)', instance_type_id='r3.xlarge', category='Memory Optimized', display_order=1, is_deprecated=True, is_encrypted_in_transit=False, is_graviton=False, is_hidden=True, is_io_cache_enabled=False, node_info=None, node_instance_type=NodeInstanceType(instance_type_id='r3.xlarge'...\n```\n\n[`select_spark_version`](https://databricks-sdk-py.readthedocs.io/en/latest/autogen/compute.html#databricks.sdk.mixins.compute.ClustersExt.select_spark_version)を使うと色々な条件でDatabricks Runtimeバージョンを表現する文字列を取得できます。クラスターのスペックの指定に必要になります。\n\n```py\n# 最新MLランタイム\nlatest = w.clusters.select_spark_version(ml=True)\nlatest\n```\n```\n'14.2.x-cpu-ml-scala2.12'\n```\n\nここがキモです。`Task`オブジェクトを組み立ててマルチタスクジョブを構成します。ジョブを作成する際に`Task`オブジェクトのリストを渡すので、先頭の処理、並列の処理、最終の処理のタスクを結合していきます。注意しなくてはならないのは`depends_on`のパラメータです。前段のどのタスクに依存するのかを指定するので、並列処理は先頭の処理、最終の処理は並列処理に依存するように指定する必要があります。タスク名を指定してもエラーになり、[`TaskDependency`](https://databricks-sdk-py.readthedocs.io/en/latest/autogen/jobs.html#databricks.sdk.service.jobs.TaskDependency)オブジェクトとしてリストに追加する必要があります。\n\n```py\ntasks = []\n\n# 先頭タスクの追加\nstart_task = Task(\n      description = start_task_description,\n      existing_cluster_id = existing_cluster_id,\n      notebook_task = NotebookTask(\n        base_parameters = {},\n        notebook_path = start_notebook_path,\n        source = Source(\"WORKSPACE\")\n      ),\n      task_key = \"start_task\",\n      depends_on = []\n    )\ntasks.append(start_task)\n\n# 並列処理タスクの追加\nparallel_tasks_list = []\nfor n_estimators in range(10, 60, 10):\n  print(n_estimators)\n\n  task_item = Task(\n      description = execution_task_description,\n      #existing_cluster_id = existing_cluster_id,\n      # 新規ジョブクラスター\n      # クラスターの設定方法は以下を参照\n      # https://databricks-sdk-py.readthedocs.io/en/latest/autogen/compute.html#databricks.sdk.service.compute.ClusterSpec\n      new_cluster = ClusterSpec(num_workers=1, spark_version=latest, node_type_id=\"i3.xlarge\"),\n      notebook_task = NotebookTask(\n        base_parameters = {\"n_estimators\": n_estimators},\n        notebook_path = execution_notebook_path,\n        source = Source(\"WORKSPACE\")\n      ),\n      task_key = f\"execution_task_{n_estimators}\", # タスク名が重複しないようにパラメータを埋め込み\n      depends_on = [TaskDependency(\"start_task\")]\n    )\n  \n  tasks.append(task_item)\n  parallel_tasks_list.append(task_item)\n\n# 依存関係を指定するためにtask_keyをTaskDependencyに変換してリストに格納\nparallel_task_keys = []\nfor task in parallel_tasks_list:\n  parallel_task_keys.append(TaskDependency(task.task_key))\n\n# 終了タスクの追加\nfinish_task = Task(\n      description = finish_task_description,\n      existing_cluster_id = existing_cluster_id,\n      notebook_task = NotebookTask(\n        base_parameters = {},\n        notebook_path = finish_notebook_path,\n        source = Source(\"WORKSPACE\")\n      ),\n      task_key = \"finish_job\",\n      depends_on = parallel_task_keys\n    )\ntasks.append(finish_task)\n\n# ジョブの作成\nj = w.jobs.create(\n  name = job_name,\n  tasks = tasks\n)\n\nprint(f\"View the job at {w.config.host}/#job/{j.job_id}\\n\")\n```\n\n上のセルを実行すると、期待した通りにジョブが作成されます。おおー、すごい。\n![Screenshot 2023-12-10 at 14.52.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5b60c227-f54b-5666-91f1-db525e571cad.png)\n\n# ジョブの実行\n\n並列タスクのクラスターは分離されているのでリソースが競合することもありません。早速ジョブを実行します。\n![Screenshot 2023-12-10 at 14.53.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/fee653dc-4a13-c4eb-1d4c-5a56f523bcf1.png)\n![Screenshot 2023-12-10 at 14.53.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d0d73e1d-8644-3a63-ff32-c145780e9818.png)\n![Screenshot 2023-12-10 at 14.59.16.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5a2a8e80-ab7e-84e6-90de-9cfeddf23245.png)\n\nすべての並列タスクが完了してから最終タスクが実行されます。\n![Screenshot 2023-12-10 at 15.00.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5073e070-130c-ae65-ef13-0a6cef5bdf90.png)\n\nそして、並列実行にしようしたノートブックにアクセスします。上でトレーニングした機械学習モデルはMLflowによってトラッキングされているので、記録の手間を気にする必要がありません。\n![Screenshot 2023-12-10 at 15.01.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/81e237b9-adef-092e-e9a7-9b6180e44435.png)\n\nグラフを用いてモデルを簡単に比較することもできます。\n![Screenshot 2023-12-10 at 15.03.04.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f37942a1-7332-6247-f5b5-5e8658ea4cb8.png)\n\nこのジョブはすべてをプログラムから作成しているので、ループの部分を変更すれば任意の並列度を達成することができます。\n\n```py\nfor n_estimators in range(10, 110, 10):\n```\n\n![Screenshot 2023-12-10 at 15.04.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1574ac9f-adbd-f5cd-deb3-a43dbdf65850.png)\n\nジョブの作成においては、GUIとSDKを適材適所で活用いただけると良いかと思います。是非ご活用ください！\n\n:::note\n**注意**\n[こちら](https://docs.databricks.com/ja/workflows/jobs/create-run-jobs.html)にあるように、ワークスペースにおける同時タスク実行数は1000に制限されていますのでご注意ください。\n:::\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "リアルタイムの構造化データによるRAGアプリケーションのレスポンス品質の改善", "text": "[Improve your RAG application response quality with real\\-time structured data \\| Databricks Blog](https://www.databricks.com/blog/improve-your-rag-application-response-quality-real-time-structured-data)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n[Retrieval Augmented Generation \\(RAG\\)](https://www.databricks.com/glossary/retrieval-augmented-generation-rag)は、生成AIアプリケーションにおいてコンテキストとして適切なデータを提供する効率的なメカニズムです。多くのRAGアプリケーションは、ドキュメント、Wiki、サポートチケットのような非構造化データから適切なコンテキストを検索するためにベクトルインデックスを活用します。昨日、我々はまさにこの助けとなるDatabricks Vector Searchのパブリックプレビューを発表しました。しかし、適切かつパーソナライズされた構造化データを用いて、これらのテキストベースのコンテキストを拡張することで、生成AIのレスポンス品質を強化することができます。顧客が「私の最近の注文はどこですか？」と問い合わせる小売ウェブサイトの生成AIツールを創造してください。このAIは、レスポンスの生成にLLMを使う前に、この問い合わせは特定の注文に関するものであることを理解し、ラインアイテムに対する最新の出荷情報を収集する必要があります。これらのスケーラブルなアプリケーションの開発には膨大な作業や、生成AIの能力と構造化データ、非構造化データの両方を取り扱うためのテクノロジーとのインテグレーションが必要となります。\n\nDatabricksデータインテリジェンスプラットフォームから構造化データをサービングするために設計された、低レーテンシーのリアルタイムサービスである**Databricks Feature & Function Serving**のパブリックプレビューを発表できることを嬉しく思っています。計算済みのML特徴量に即座にアクセスでき、Unity Catalogから任意のPython関数をサービングすることでリアルタイムのデータ変換処理を実行することができます。そして、取得されたデータは、リアルタイムのルールエンジン、従来のML、生成AIアプリケーションで活用することができます。\n\n構造化データのためのFeature & Function Serving([AWS](https://docs.databricks.com/ja/machine-learning/feature-store/feature-function-serving.html))([Azure](https://learn.microsoft.com/ja-jp/azure/databricks/machine-learning/feature-store/feature-function-serving))と非構造化データのためのDatabricks Vector Search([AWS](https://docs.databricks.com/ja/generative-ai/vector-search.html))([Azure](https://learn.microsoft.com/ja-jp/azure/databricks/generative-ai/vector-search))を苦r見合わせることで、生成AIアプリケーションにおけるプロダクション化を劇的にシンプルにすることができます。ユーザーは、Databricksで直接これらのアプリケーションを構築、デプロイすることができ、既存のデータパイプライン、ガバナンス、その他のエンタープライズ機能を活用することができます。\n\nさまざまな業界のDatabricksのお客様は、以下の表に示すようにパワフルな生成AIアプリケーションを構築するためにオープンソースのフレームワークとこれらのテクノロジーを活用しています。\n\n| 業界 | ユースケース |\n|:--|:--|\n|小売   | <ul><li>ユーザーの嗜好、検索履歴、位置などに基づく製品提案 / 検索ランキング<li>製品検索をベースとした画像やメタデータ<li>売り上げデータ、季節トレンド、市場/競合分析を用いた在庫管理や予測</ul>  |\n|教育   | <ul><li>過去の間違い、歴史的なトレンド、グループに基づいてパーソナライズされた教育計画<li>自動化された評点、フィードバック、フォローアップ、進捗レポート<li>対象デバイスに対するコンテンツフィルタリング</ul>  |\n|金融サービス   | <ul><li>アナリストや投資家が収支報告、市場インテリジェンスレポート、過去のトレンドの相関を見るための自然言語アプリ<li>不正およびリスク分析<li>パーソナライズされた資産管理、リタイアメント計画、what-if分析、ネクストベストアクション</ul>  |\n| 旅行とホスピタリティ  | <ul><li>パーソナライズされた顧客インタラクションとテイラーメードの旅行提案のためのチャットボット<li>気候、ライブの交通パターン、過去のデータを用いた動的ルート計画<li>競合分析、需要ベースのプライシングを用いた動的価格最適化</ul>  |\n| ヘルスケアとライフサイエンス  | <ul><li>患者/メンバーのエンゲージメント、健康状態の要約<li>パーソナライズされたケア、診療の意思決定、ケアの調整のためのサポートアプリ<li>R&Dレポートの要約、臨床試験分析、薬品の目的の再定義</ul>  |\n| 保険  | <ul><li>資産や近隣に関するテキストや構造化データを用いた住宅ローン引受のリスク評価<li>ポリシー、リスク、what-if分析に関する質問のためのチャットボット<li>クレーム処理の自動化</ul>  |\n| テクノロジーと製造  | <ul><li>予兆保全やガイドされた指示を用いた装置診断<li>履歴の統計情報に対するライブのデータストリームによる異常検知<li>日々の生産 / シフト分析や将来的な計画のための自動化された分析</ul>  |\n| メディアエンターテイメント  | <ul><li>アプリ内のコンテンツ検索やレコメンデーション、パーソナライズされたメールやデジタルマーケティング<li>コンテンツのローカライズ<li>パーソナライズされたゲーム体験やゲームレビュー</ul>  |\n\n# RAGアプリケーションへの構造化データのサービング\n\n生成AIアプリケーションの品質改善に構造化データがどのように役立つのかをデモンストレーションするために、以下の旅行計画チャットbotのサンプルを使用します。この例では、マッチするホテルを検索するために、ユーザーの嗜好(オーシャンビュー、家族に優しいなど)がどのようにホテルに関する非構造化の情報と組み合わされるのかを示しています。通常、ホテルの価格は需要と季節性によって動的に変化します。生成AIに組み込まれた価格計算機は、レコメンデーションがユーザーの予算に収まるようにします。ボットを強化する生成AIアプリケーションは、LangChainのエージェントAPIを用いて、必要となるパーソナライズされたユーザー嗜好と予算、ホテルの情報を提供するためのビルディングブロックとして、Databricks Vector SearchとDatabricks Feature & Function Servingを活用します。\n![](https://cms.databricks.com/sites/default/files/inline-images/image3_12.png)\n*ユーザーの嗜好と予算を考慮する旅行計画ボット*\n\n上述したこのRAGチェーンアプリケーションの[完全なノートブック](https://docs.databricks.com/en/_extras/notebooks/source/machine-learning/structured-data-for-rag.html)にアクセスすることができます。このアプリケーションはノートブック内でローカルに稼働することもできますし、チャットbotのユーザーインタフェースでアクセスできるエンドポイントとしてデプロイすることもできます。\n\n# リアルタイムエンドポイントとしてあなたのデータと関数にアクセス\n\n[Unity Catalogの特徴量エンジニアリング](https://www.databricks.com/jp/blog/simplification-of-AI-data-feature-store-evolved)によって、トレーニングとサービングのための特徴量をサービングするための主キーと任意のテーブルを活用できるようになっています。サービングでは、特徴量をオンデマンドで計算するPython関数がサポートされています。Databricksモデルサービングと同じテクノロジーを用いて構築されており、計算済みの特徴量やオンデマンドでの計算のために、特徴量と関数のエンドポイントを活用することができます。シンプルな構文によって、RESTエンドポイントとして特徴量を計算し、サービングする有効非循環グラフをエンコードできる、Unity Catalogにおける**feature spec function**を定義することができます。\n\n```py\nfrom databricks.feature_engineering import (\n  FeatureFunction,\n  FeatureLookup,\n  FeatureEngineeringClient,\n)\n\nfeatures = [\n  # Lookup columns `latitude` and `longitude` from `restarants` table in UC using the input `restaurant_id` as key\n  FeatureLookup(\n    table_name=\"main.default.restaurants\",\n    lookup_key=\"restaurant_id\",\n    features=[\"latitude”, “longitude\"]\n  ),\n  # Calculate a new feature called `distance` using resturant and user's current location\n  FeatureFunction(\n    udf_name=\"main.default.distance\",\n    output_name=\"distance\",\n    # bind the function parameter with input from other features or from request.\n    input_bindings={\"user_latitude\": \"user_latitude\", \"user_longitude\": \"user_longitude\",\n                    \"restaurant_latitude\": \"latitude\", \"restaurant_longitude\": \"longitude\"},\n  ),\n]\n\nfe = FeatureEngineeringClient()\n\n# Create a feature spec with the features listed above.\n# The FeatureSpec can be accessed in UC as a Function.\nfe.create_feature_spec(\n  name=\"main.default.restaurant_features\",\n  features=features,\n)\n```\n\nこのfeature spec functionは、RESTエンドポイントとしてリアルタイムでサービングすることができます。特徴量、関数、カスタムのトレーニング済みモデル、基盤モデルを含むすべてのエンドポイントは左のナビゲーションタブにあるサービングからアクセスすることができます。以下のAPIを用いてエンドポイントをプロビジョニングします。\n\n```py\nfrom databricks.feature_engineering.entities.feature_serving_endpoint import (\n  ServedEntity,\n  EndpointCoreConfig,\n)\n\nfe.create_feature_serving_endpoint(\n  name=\"restaurant-features\",\n    config=EndpointCoreConfig(\n    served_entities=ServedEntity(\n      feature_spec_name=\"main.default.restaurant_features\",\n      workload_size=\"Small\",\n      scale_to_zero_enabled=True\n    )\n  )\n)\n```\n\nまた、以下のようにUIのワークフローでエンドポイントを作成することもできます。\n![](https://cms.databricks.com/sites/default/files/inline-images/image1_1.gif)\n\nこれで、エンドポイントをクエリーすることでリアルタイムで特徴量にアクセスすることができます。\n\n```sh\ncurl \\\n  -u token:$DATABRICKS_TOKEN \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"dataframe_records\": [{\"user_latitude\": 37.9711, \"user_longitude\": -122.3940, \"restaurant_id\": 5}]}' \\\n  https://<databricks-instance>/serving-endpoints/restaurant-features/invocations\n```\n\nリアルタイムAIアプリケーションに構造化データをサービングするには、オペレーショナルデータベースに計算済みのデータをデプロイする必要があります。ユーザーはすでに計算済みの特徴量のソースとして外部のオンラインストアを活用することができます。例えば、[DynamoDB](https://docs.databricks.com/ja/machine-learning/feature-store/online-feature-stores.html)や[Cosmos DB](https://learn.microsoft.com/ja-jp/azure/databricks/machine-learning/feature-store/online-feature-stores)はDatabricksモデルサービングで特徴量をサービングする際によく使われています。**Databricks Online Tables**([AWS](https://docs.databricks.com/ja/machine-learning/feature-store/online-tables.html))([Azure](https://learn.microsoft.com/ja-jp/azure/databricks/machine-learning/feature-store/online-tables))は、計算済み特徴量の低レーテンシーのデータ検索に最適化されたデータフォーマットへの同期をシンプルにする新機能です。オンラインテーブルとして主キーを持つ任意のテーブルを同期することができ、データの鮮度を確実にするためにシステムが自動パイプラインをセットアップします。\n![](https://cms.databricks.com/sites/default/files/inline-images/image4_12.png)\n\nDatabricks Online Tablesを用いることで、生成AIアプリケーションに特徴量をサービングするために、主キーを持つすべてのUnity Catalogのテーブルを使用することができます。\n\n# 次のステップ\n\nあなたのRAGアプリケーションをカスタマイズするために、上で説明した[サンプルノートブック](https://docs.databricks.com/en/_extras/notebooks/source/machine-learning/structured-data-for-rag.html)をご活用ください。\n\nオンデマンド視聴できる[Databricks Generative AI Webinar](https://www.databricks.com/resources/webinar/disrupt-your-industry-generative-ai)をご覧ください。\n\nFeature & Function Serving([AWS](https://docs.databricks.com/ja/machine-learning/feature-store/feature-function-serving.html))([Azure](https://learn.microsoft.com/ja-jp/azure/databricks/machine-learning/feature-store/feature-function-serving))はパブリックプレビューです。APIドキュメントやその他のサンプルをご覧ください。\n\nDatabricks Online Tables([AWS](https://docs.databricks.com/ja/machine-learning/feature-store/online-tables.html))([Azure](https://learn.microsoft.com/ja-jp/azure/databricks/machine-learning/feature-store/online-tables))はゲーテッドのパブリックプレビューです。有効化するにはこちらの[フォーム](https://forms.gle/9jLZkpXnJF9ZcxtQA)にサインアップください。\n\n今週前半の[発表](https://qiita.com/taka_yayoi/items/cc9df235215d4b79fb1d)(高品質なRAGアプリケーション)をご一読ください。\n\nDatabricksに共有したいユースケースがありますか？feature-serving@databricks.comにコンタクトください。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks Vector SearchとFoundation Model APIを用いたRAGアプリケーション構築のウォークスルー", "text": "こちらの最後で触れられているデモをウォークスルーします。\n\nhttps://qiita.com/taka_yayoi/items/cc9df235215d4b79fb1d\n\n今回はウォークスルーするだけなので、英語のままですがこの後日本語対応します。\n\n# デモノートブックのインポート\n\n新規ノートブックを作成し、以下を実行します。\n\n```py\n%pip install dbdemos\n```\n```py\nimport dbdemos\ndbdemos.install('llm-rag-chatbot')\n```\n\n必要なノートブックがインポートされ、クラスターも作成されます。\n![Screenshot 2023-12-08 at 16.23.21.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d28c8791-f971-fa18-488e-9d252a45c10b.png)\n![Screenshot 2023-12-08 at 16.23.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e501f0a9-2d0f-1135-0576-bc3dfe9a3e52.png)\n\n`00-RAG-chatbot-Introduction`は概要の説明なのでスキップします。\n\n# config\n\nデモで使うUnity Catalogのカタログやスキーマ、Vector Searchエンドポイントを自分用のものに変更しておきます。\n\n```py\nVECTOR_SEARCH_ENDPOINT_NAME=\"taka_dbdemos_vs_endpoint\"\n\nDATABRICKS_SITEMAP_URL = \"https://docs.databricks.com/en/doc-sitemap.xml\"\n\ncatalog = \"takaakiyayoi_catalog\"\n\n#email = spark.sql('select current_user() as user').collect()[0]['user']\n#username = email.split('@')[0].replace('.', '_')\n#dbName = db = f\"rag_chatbot_{username}\"\ndbName = db = \"rag_chatbot\"\n```\n\n# 01-Data-Preparation-and-Index\n\n```py\n%pip install \"git+https://github.com/mlflow/mlflow.git@gateway-migration\" lxml==4.9.3 transformers==4.30.2 langchain==0.0.344 databricks-vectorsearch==0.22\ndbutils.library.restartPython()\n```\n```py\n%run ./_resources/00-init $reset_all_data=false\n```\n\n## ドキュメントのクローリング \n\nこちらを実行すると、Databricksのマニュアルサイトをクローリングしてテーブル`raw_documentation`を作成します。\n```py\nif not spark.catalog.tableExists(\"raw_documentation\") or spark.table(\"raw_documentation\").isEmpty():\n    # Download Databricks documentation to a DataFrame (see _resources/00-init for more details)\n    doc_articles = download_databricks_documentation_articles()\n    #Save them as a raw_documentation table\n    doc_articles.write.mode('overwrite').saveAsTable(\"raw_documentation\")\n\ndisplay(spark.table(\"raw_documentation\").limit(2))\n```\n![Screenshot 2023-12-08 at 16.37.31.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3ebf3998-5bdc-bf00-620c-31ff4cc469c1.png)\n![Screenshot 2023-12-08 at 16.38.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/37a299d0-dcc7-823d-bf19-21205b678c0f.png)\n\n## チャンクの作成\n\n今回使用するドキュメントはサイズの大きいものがあるため、そのままではLLMのプロンプトに指定できません。そのために、適切なサイズに分割してチャンク(chunk)を作成します。\n\n```py\nfrom langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\nfrom transformers import AutoTokenizer\n\nmax_chunk_size = 500\n\ntokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\ntext_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)\nhtml_splitter = HTMLHeaderTextSplitter(headers_to_split_on=[(\"h2\", \"header2\")])\n\n# Split on H2, but merge small h2 chunks together to avoid too small. \ndef split_html_on_h2(html, min_chunk_size = 20, max_chunk_size=500):\n  h2_chunks = html_splitter.split_text(html)\n  chunks = []\n  previous_chunk = \"\"\n  # Merge chunks together to add text before h2 and avoid too small docs.\n  for c in h2_chunks:\n    # Concat the h2 (note: we could remove the previous chunk to avoid duplicate h2)\n    content = c.metadata.get('header2', \"\") + \"\\n\" + c.page_content\n    if len(tokenizer.encode(previous_chunk + content)) <= max_chunk_size/2:\n        previous_chunk += content + \"\\n\"\n    else:\n        chunks.extend(text_splitter.split_text(previous_chunk.strip()))\n        previous_chunk = content + \"\\n\"\n  if previous_chunk:\n      chunks.extend(text_splitter.split_text(previous_chunk.strip()))\n  # Discard too small chunks\n  return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]\n\n# Let's create a user-defined function (UDF) to chunk all our documents with spark\n@pandas_udf(\"array<string>\")\ndef parse_and_split(docs: pd.Series) -> pd.Series:\n    return docs.apply(split_html_on_h2)\n  \n# Let's try our chunking function\nhtml = spark.table(\"raw_documentation\").limit(1).collect()[0]['text']\nsplit_html_on_h2(html)\n```\n\n## エンべディングの作成\n\n[DatabricksのFoundation Model API](https://qiita.com/taka_yayoi/items/c225a4f3df6db05591a4)を使えば、簡単にエンべディングモデルを呼び出してエンべディングを作成することができます。\n\n```py\nimport mlflow.deployments\ndeploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n\nresponse = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [\"What is Apache Spark?\"]})\nembeddings = [e['embedding'] for e in response.data]\nprint(embeddings)\n# NOTE: if you change your embedding model here, make sure you change it in the query step too\n```\n```\n[[0.0186004638671875, -0.0141448974609375, -0.0574951171875, 0.0034027099609375, 0.008453369140625, -0.0216064453125, -0.02471923828125, -0.004688262939453125, 0.0136566162109375, 0.050384521484375, -0.0272064208984375, -0.01470184326171875, 0.054718017578125, -0.0538330078125, -0.01035308837890625, -0.0162200927734375, -0.0188140869140625, -0.017242431640625, -0.051300048828125, 0.0177764892578125, 0.00434112548828125, 0.0284423828125, -0.055633544921875, -0.037689208984375, -0.001373291015625, 0.0203704833984375, -0.046661376953125, 0.01580810546875, 0.0938720703125, 0.0195770263671875, -0.044647216796875, -0.0124359130859375, -0.0062255859375, -0.0293121337890625, 0.04327392578125, -0.02557373046875, 0.049468994140625, 0.03228759765625, -0\n```\n\nVector Searchインデックスの作成対象となるテーブルではチェンジデータフィードを有効化する必要があります。これで、元のテーブルにレコードが追加されると自動でインデックスが更新されます。\n\n```sql\n%sql\n--Note that we need to enable Change Data Feed on the table to create the index\nCREATE TABLE IF NOT EXISTS databricks_documentation (\n  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n  url STRING,\n  content STRING,\n  embedding ARRAY <FLOAT>\n) TBLPROPERTIES (delta.enableChangeDataFeed = true); \n```\n\nエンべディングを作成する処理をUDF(User Defined Function)にまとめます。\n\n```py\nimport mlflow.deployments\ndeploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n\n@pandas_udf(\"array<float>\")\ndef get_embedding(contents: pd.Series) -> pd.Series:\n    def get_embeddings(batch):\n        #Note: this will gracefully fail if an exception is thrown during embedding creation (add try/except if needed) \n        response = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": batch})\n        return [e['embedding'] for e in response.data]\n\n    # Splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.\n    max_batch_size = 150\n    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]\n\n    # Process each batch and collect the results\n    all_embeddings = []\n    for batch in batches:\n        all_embeddings += get_embeddings(batch.tolist())\n\n    return pd.Series(all_embeddings)\n```\n\n以下を実行して、エンべディングをテーブル`databricks_documentation`に書き込みます。\n\n```py\n(spark.table(\"raw_documentation\")\n      .withColumn('content', F.explode(parse_and_split('text')))\n      .withColumn('embedding', get_embedding('content'))\n      .drop(\"text\")\n      .write.mode('overwrite').saveAsTable(\"databricks_documentation\"))\n\ndisplay(spark.table(\"databricks_documentation\"))\n```\n![Screenshot 2023-12-08 at 16.45.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c41f5361-bc53-3f69-19cd-b9d6b8275d7f.png)\n\n## Vector Search Indexの作成\n\n以下を実行してVector Searchエンドポイントを作成します。十数分お待ちください。\n\n```py\nfrom databricks.vector_search.client import VectorSearchClient\nvsc = VectorSearchClient()\n\nif VECTOR_SEARCH_ENDPOINT_NAME not in [e['name'] for e in vsc.list_endpoints()['endpoints']]:\n    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n\nwait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\nprint(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")\n```\n\n**クラスター > Vector Search**にアクセスするとエンドポイントのプロビジョンが始まっていることを確認できます。\n![Screenshot 2023-12-08 at 16.46.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/19f400aa-3ae3-6bd7-791e-4060b4408534.png)\n\nエンドポイントがReadyになりました。\n![Screenshot 2023-12-08 at 16.59.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/050c836a-f61d-5d2e-e261-14e5141dffe7.png)\n![Screenshot 2023-12-08 at 16.59.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/519808c3-d33a-07f8-48fc-b7032fd8847a.png)\n\nVector Searchエンドポイント、インデックスの作成対象のテーブルとインデックスの格納場所を指定します。これによって、インデックスが作成され、Vector Searchエンドポイントに格納されます。\n\n```py\nfrom databricks.sdk import WorkspaceClient\nimport databricks.sdk.service.catalog as c\n\n#The table we'd like to index\nsource_table_fullname = f\"{catalog}.{db}.databricks_documentation\"\n# Where we want to store our index\nvs_index_fullname = f\"{catalog}.{db}.databricks_documentation_vs_index\"\n\nif not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n  vsc.create_delta_sync_index(\n    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n    index_name=vs_index_fullname,\n    source_table_name=source_table_fullname,\n    pipeline_type=\"CONTINUOUS\",\n    primary_key=\"id\",\n    embedding_dimension=1024, #Match your model embedding size (bge)\n    embedding_vector_column=\"embedding\"\n  )\n\n\n#Let's wait for the index to be ready and all our embeddings to be created and indexed\nwait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\nprint(f\"index {vs_index_fullname} on table {source_table_fullname} is ready\")\n```\n\nカタログエクスプローラでインデックスを確認できます。\n![Screenshot 2023-12-08 at 17.02.10.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/17f42989-bb2f-e972-93e3-c858129c9f9c.png)\n\n十数分でインデックス作成が完了しました。\n![Screenshot 2023-12-08 at 17.06.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2d4b6c6a-9320-d3b5-55db-aa13e0daef97.png)\n\nインデックス作成の進捗をカタログエクスプローラから確認することもできます。\n![Screenshot 2023-12-08 at 17.06.05.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/49364a3b-7ca2-9cd6-ffb7-13df7e45342e.png)\n![Screenshot 2023-12-08 at 17.06.15.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bdb4ab00-7e1d-5f54-eb4d-0a98818637fd.png)\n\n## Vector Searchによる類似検索\n\nこれでインデックスが作成されたので、問い合わせのテキストをFoundation Model API経由でエンべディングモデルに渡すことでエンべディングを作成し、Vector Searchエンドポイントに問い合わせを行い類似検索を実行できます。\n\n```py\nimport mlflow.deployments\ndeploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n\nquestion = \"How can I track billing usage on my workspaces?\"\nresponse = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [question]})\nembeddings = [e['embedding'] for e in response.data]\n\nresults = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n  query_vector=embeddings[0],\n  columns=[\"url\", \"content\"],\n  num_results=1)\ndocs = results.get('result', {}).get('data_array', [])\ndocs\n```\n\n```\n[['https://docs.databricks.com/en/libraries/workspace-libraries.html',\n  'View workspace library details\\nView workspace library details\\nNote  \\nWorkspace folders provide convenience for discovering workspace libraries and managing ACLs.  \\nGo to the workspace folder containing the library.  \\nClick the library name.  \\nThe library details page shows the running clusters and the install status of the library. If the library is installed, the page contains a link to the package host. If the library was uploaded, the page displays a link to the uploaded package file.\\n\\nMove a workspace library\\nMove a workspace library\\nNote  \\nWorkspace folders provide convenience for discovering workspace libraries and managing ACLs. Moving a workspace library does not move files, but can modify which users have access to the workspace library.  \\nGo to the workspace folder containing the library.  \\nRight-click the library name and select Move. A folder browser displays.  \\nClick the destination folder.  \\nClick Move.\\n\\nDelete a workspace library',\n  0.5847994]]\n```\n\n# 02-Deploy-RAG-Chatbot-Model\n\n## Retrieverの作成\n\n```py\n%pip install \"git+https://github.com/mlflow/mlflow.git@gateway-migration\" langchain==0.0.344 databricks-vectorsearch==0.22 databricks-sdk==0.12.0 mlflow[databricks]\ndbutils.library.restartPython()\n```\n```py\n%run ./_resources/00-init $reset_all_data=false\n```\n\nここまで構築してきているRAGアプリケーションを、モデルサービングエンドポイントにデプロイしてREST API経由で呼び出せるようにします。この際には、モデルサービングエンドポイントからVector Searchエンドポイントにアクセスするために、Vector Search Indexにアクセスできるサービスプリンシパル(ユーザー)のパーソナルアクセストークンが必要となります。\n\n```py\nindex_name=f\"{catalog}.{db}.databricks_documentation_vs_index\"\n\n# Make sure you replace sp_name with the SP owning the token in the secret. It has be the principal in the PAT token used in the model\nsp_name = spark.sql('select current_user() as user').collect()[0]['user'] #Set to current user if you are using your own personal PAT token for the demo\nspark.sql(f'GRANT USAGE ON CATALOG {catalog} TO `{sp_name}`');\nspark.sql(f'GRANT USAGE ON DATABASE {catalog}.{db} TO `{sp_name}`');\nfrom databricks.sdk import WorkspaceClient\nimport databricks.sdk.service.catalog as c\nWorkspaceClient().grants.update(c.SecurableType.TABLE, index_name, \n                                changes=[c.PermissionsChange(add=[c.Privilege[\"SELECT\"]], principal=sp_name)])\n```\n\n今回の例では、スコープ`demo-token-takaaki.yayoi`、`token`のシークレットにパーソナルアクセストークンを登録しています。\n\n```py\n# url used to send the request to your model from the serverless endpoint\nhost = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\nos.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(\"demo-token-takaaki.yayoi\", \"token\")\n```\n\nVector Searchに問い合わせを行うretrieverを作成します。\n\n```py\nfrom databricks.vector_search.client import VectorSearchClient\nfrom langchain.vectorstores import DatabricksVectorSearch\nfrom langchain.embeddings import DatabricksEmbeddings\n\n# Test embedding Langchain model\n#NOTE: your question embedding model must match the one used in the chunk in the previous model \nembedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\nprint(f\"Test embeddings: {embedding_model.embed_query('What is Apache Spark?')[:20]}...\")\n\ndef get_retriever(persist_dir: str = None):\n    os.environ[\"DATABRICKS_HOST\"] = host\n    #Get the vector search index\n    vsc = VectorSearchClient(workspace_url=host, personal_access_token=os.environ[\"DATABRICKS_TOKEN\"])\n    vs_index = vsc.get_index(\n        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n        index_name=index_name\n    )\n\n    # Create the retriever\n    vectorstore = DatabricksVectorSearch(\n        vs_index, text_column=\"content\", embedding=embedding_model\n    )\n    return vectorstore.as_retriever()\n\n\n# test our retriever\nvectorstore = get_retriever()\nsimilar_documents = vectorstore.get_relevant_documents(\"How do I track my Databricks Billing?\")\nprint(f\"Relevant documents: {similar_documents[0]}\")\n```\n```\nTest embeddings: [0.0186004638671875, -0.0141448974609375, -0.0574951171875, 0.0034027099609375, 0.008453369140625, -0.0216064453125, -0.02471923828125, -0.004688262939453125, 0.0136566162109375, 0.050384521484375, -0.0272064208984375, -0.01470184326171875, 0.054718017578125, -0.0538330078125, -0.01035308837890625, -0.0162200927734375, -0.0188140869140625, -0.017242431640625, -0.051300048828125, 0.0177764892578125]...\n[NOTICE] Using a Personal Authentication Token (PAT). Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\nRelevant documents: page_content='View request status\\nIf you have requested a data product that requires provider approval, you can view request status in the Marketplace UI:  \\nLog into your Databricks workspace.  \\nFor required permissions, see Before you begin.  \\nIn the sidebar, click Marketplace.  \\nOn the upper-right corner of the Marketplace page, click My requests.  \\nOn the Requests tab, view all requests and their current review status: Pending, Fulfilled, and Denied, along with the requested and reviewed dates.  \\nTransactions that are ongoing between you and a data provider occur outside of the Databricks Marketplace system. Details of those transactions are not captured here.\\n\\nView and access installed data products' metadata={'id': 1446.0}\n```\n\n## RAGの構築\n\nここでも、Foundation Model APIでLlama2を呼び出してRAGを構築します。\n\n```py\n# Test Databricks Foundation LLM model\nfrom langchain.chat_models import ChatDatabricks\nchat_model = ChatDatabricks(endpoint=\"databricks-llama-2-70b-chat\", max_tokens = 200)\nprint(f\"Test chat model: {chat_model.predict('What is Apache Spark')}\")\n```\n```\nTest chat model: \nApache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Python, Scala, and R, and an optimized engine that supports general execution graphs. It also provides high-level tools and libraries for data loading, transformation, and machine learning.\n\nSpark is designed to handle large-scale data processing tasks and can process data in real-time or batch mode. It is highly scalable and can handle data processing tasks that are too large for a single machine to handle. It is also highly fault-tolerant, meaning that it can continue to process data even if one or more machines fail.\n\nSpark is widely used in a variety of industries, including finance, healthcare, retail, and telecommunications. It is often used for data warehousing, machine learning, and stream processing.\n\nSome of the key features of Apache Spark include:\n```\n\nプロンプトを指定し、Retrieverを組み込み、Q&Aボットとして動作するようにします。\n\n```py\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chat_models import ChatDatabricks\n\nTEMPLATE = \"\"\"You are an assistant for Databricks users. You are answering python, coding, SQL, data engineering, spark, data science, DW and platform, API or infrastructure administration question related to Databricks. If the question is not related to one of these topics, kindly decline to answer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\nUse the following pieces of context to answer the question at the end:\n{context}\nQuestion: {question}\nAnswer:\n\"\"\"\nprompt = PromptTemplate(template=TEMPLATE, input_variables=[\"context\", \"question\"])\n\nchain = RetrievalQA.from_chain_type(\n    llm=chat_model,\n    chain_type=\"stuff\",\n    retriever=get_retriever(),\n    chain_type_kwargs={\"prompt\": prompt}\n)\n```\n\n```py\n# langchain.debug = True #uncomment to see the chain details and the full prompt being sent\nquestion = {\"query\": \"How can I track billing usage on my workspaces?\"}\nanswer = chain.run(question)\nprint(answer)\n```\n```\nQuestion: How can I track billing usage on my workspaces?\n\nAnswer: You can track billing usage on your workspaces by viewing the \"Workspace Usage\" page in the Databricks portal. To access this page, follow these steps:\n\n1. Log in to your Databricks account.\n2. Click on the \"Workspaces\" tab in the sidebar.\n3. Click on the name of the workspace you want to view usage for.\n4. Click on the \"Usage\" tab in the sidebar.\n\nOn the Workspace Usage page, you will see a graph displaying your usage over time, as well as a table listing your usage by hour. You can use this information to track your billing usage and estimate your costs. Additionally, you can set up alerts to notify you when your usage reaches a certain threshold.\n\nNote that workspace usage is calculated based on the number of\n```\n\n## Q&A botモデルの保存\n\nモデルを保存するExperimentを設定します。独自のExperimentに保存したい場合には、`_resources/00-init`を編集してください。\n\n```py\n#dbdemos__delete_this_cell\n#force the experiment to the field demos one. Required to launch as a batch\ninit_experiment_for_batch(\"chatbot-rag-llm\", \"simple\")\n```\n\n以下を実行することで、Unity Catalog配下にモデルが登録されます。\n\n```py\nfrom mlflow.models import infer_signature\nimport mlflow\n\nmlflow.set_registry_uri(\"databricks-uc\")\nmodel_name = f\"{catalog}.{db}.dbdemos_chatbot_model\"\n\nwith mlflow.start_run(run_name=\"dbdemos_chatbot_rag\") as run:\n    signature = infer_signature(question, answer)\n    model_info = mlflow.langchain.log_model(\n        chain,\n        loader_fn=get_retriever,  # Load the retriever with DATABRICKS_TOKEN env as secret (for authentication).\n        artifact_path=\"chain\",\n        registered_model_name=model_name,\n        pip_requirements=[\n            \"git+https://github.com/mlflow/mlflow.git@gateway-migration\",\n            \"langchain==\" + langchain.__version__,\n            \"databricks-vectorsearch\",\n        ],\n        input_example=question,\n        signature=signature\n    )\n    #------------------------\n    # TODO: temporary fix to add the wheel, we won't need this after we switch to using PyPI\n    import mlflow.models.utils\n    mlflow.models.utils.add_libraries_to_model(\n        f\"models:/{model_name}/{get_latest_model_version(model_name)}\"\n    )\n```\n![Screenshot 2023-12-08 at 17.20.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9c6a5b38-0d61-98ea-7738-91a701565c52.png)\n\n[モデルレジストリがUnity Catalogに統合](https://qiita.com/taka_yayoi/items/48104b20affdd7faa1f7)されて便利です。\n![Screenshot 2023-12-08 at 17.20.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6567e0f0-2ef7-b398-f192-dd04cb0ac0d7.png)\n\n## サーバレスモデルエンドポイントへのデプロイ\n\n以下を実行すると、上のモデルがデプロイされたモデルサービングエンドポイントが作成されます。\n\n```py\n# Create or update serving endpoint\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.serving import EndpointCoreConfigInput, ServedModelInput\n\nserving_endpoint_name = f\"dbdemos_endpoint_{catalog}_{db}\"[:63]\nlatest_model_version = get_latest_model_version(model_name)\n\nw = WorkspaceClient()\nendpoint_config = EndpointCoreConfigInput(\n    name=serving_endpoint_name,\n    served_models=[\n        ServedModelInput(\n            model_name=model_name,\n            model_version=latest_model_version,\n            workload_size=\"Small\",\n            scale_to_zero_enabled=True,\n            environment_vars={\n                \"DATABRICKS_TOKEN\": \"{{secrets/demo-token-takaaki.yayoi/token}}\",  # <scope>/<secret> that contains an access token\n            }\n        )\n    ]\n)\n\nexisting_endpoint = next(\n    (e for e in w.serving_endpoints.list() if e.name == serving_endpoint_name), None\n)\nif existing_endpoint == None:\n    print(f\"Creating the endpoint {serving_endpoint_name}, this will take a few minutes to package and deploy the endpoint...\")\n    w.serving_endpoints.create_and_wait(name=serving_endpoint_name, config=endpoint_config)\nelse:\n    print(f\"Updating the endpoint {serving_endpoint_name} to version {latest_model_version}, this will take a few minutes to package and deploy the endpoint...\")\n    w.serving_endpoints.update_config_and_wait(served_models=endpoint_config.served_models, name=serving_endpoint_name)\n```\n![Screenshot 2023-12-08 at 17.23.04.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/169b56eb-958c-1057-84d6-3d29cfda79d2.png)\n\nこれでREST API経由でQ&A botモデルを呼び出せるようになりました。\n\n```py\nquestion = \"How can I track billing usage on my workspaces?\"\n\nanswer = w.serving_endpoints.query(serving_endpoint_name, inputs=[{\"query\": question}])\nprint(answer.predictions[0])\n```\n```\nI'm sorry, but the provided context does not contain information about tracking billing usage on workspaces. However, I can provide you with general information on how to track billing usage on Databricks.\n\nYou can track billing usage on your Databricks workspaces by following these steps:\n\n1. Navigate to the Databricks workspace that you want to track billing usage for.\n2. Click on the \"Workspace\" menu in the top navigation bar and select \"Workspace Settings\" from the dropdown list.\n3. In the Workspace Settings page, click on the \"Billing\" tab.\n4. In the Billing tab, you will see a summary of your workspace's billing usage, including the amount of data processed, the number of compute hours used, and the total cost.\n5. You can also view detailed billing usage data by clicking on the \"View detailed b\n```\n\n# 03-streamlit-application\n\nこれは元のデモには無いものです。せっかくなので画面を作りたかったのです。今回はドライバープロキシーを使いますが、[Lakehouse Apps](https://qiita.com/taka_yayoi/items/582cec2e2b888e6e515e)が待ち遠しいです。\n\n以下のpyファイルを準備します。\n\n```py:streamlit.py\nimport streamlit as st \nimport numpy as np \nimport json\nimport requests\n\nfrom databricks.sdk.runtime import dbutils\n\nst.title('Databricks Q&A bot')\n#st.header('Databricks Q&A bot')\n\ndef generate_answer(question):\n  # Driver Proxyと異なるクラスター、ローカルからDriver Proxyにアクセスする際にはパーソナルアクセストークンを設定してください\n  token = dbutils.secrets.get(\"demo-token-takaaki.yayoi\", \"token\")\n  url = \"https://e2-demo-west.cloud.databricks.com/serving-endpoints/dbdemos_endpoint_takaakiyayoi_catalog_rag_chatbot/invocations\"\n\n  headers = {\n      \"Content-Type\": \"application/json\",\n      \"Authentication\": f\"Bearer {token}\"\n  }\n  data = {\"inputs\": [question]}\n\n  response = requests.post(url, headers=headers, json=data)\n  if response.status_code != 200:\n    raise Exception(\n       f\"Request failed with status {response.status_code}, {response.text}\"\n    )\n  \n  response_json = response.json()\n  return response_json\n\nquestion = st.text_input(\"**質問**\")\n\nif question != \"\":\n  response = generate_answer(question)\n\n  answer = response[\"predictions\"][0]\n\n  st.write(f\"**回答:** {answer}\")\n```\n\n別のノートブックで以下を実行していきます。\n\n```py\n%pip install streamlit watchdog\ndbutils.library.restartPython()\n```\n\n```py\nfrom dbruntime.databricks_repl_context import get_context\n\ndef front_url(port):\n    \"\"\"\n    フロントエンドを実行するための URL を返す\n\n    Returns\n    -------\n    proxy_url : str\n        フロントエンドのURL\n    \"\"\"\n    ctx = get_context()\n    proxy_url = f\"https://{ctx.browserHostName}/driver-proxy/o/{ctx.workspaceId}/{ctx.clusterId}/{port}/\"\n\n    return proxy_url\n\nPORT = 1501\n\n# Driver ProxyのURLを表示\nprint(front_url(PORT))\n\n# 利便性のためにリンクをHTML表示\ndisplayHTML(f\"<a href='{front_url(PORT)}' target='_blank' rel='noopener noreferrer'>別ウインドウで開く</a>\")\n```\n\n以下を実行すると、ドライバープロキシー経由でstreamlitの画面にアクセスできます。\n\n```py\nstreamlit_file = \"/Workspace/Users/takaaki.yayoi@databricks.com/20231207_rag_dbdemos/llm-rag-chatbot/streamlit.py\"\n\n!streamlit run {streamlit_file} --server.port {PORT}\n```\n\n動きました！\n![Screenshot 2023-12-08 at 17.30.21.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/59029ba5-e8cc-62ef-6058-98ed43013112.png)\n![Screenshot 2023-12-08 at 17.30.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/77251c5b-0cf9-e802-a3bd-486d8207fbec.png)\n![Screenshot 2023-12-08 at 17.31.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/67fbcbdf-7f63-1bfa-3a42-642a10972528.png)\n\n\nこの記事を書きながら画面を動かすところまで正味1時間でした。足回りが整備されていくと、色々トライしやすくなるので、手前味噌ですがこのような機能拡張、大変助かります。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks SQLのクエリエディタでテーブル名をパラメータにしたい", "text": "表題の件、以前は「これはできないのかな」と思っていたらできました。\n\nクエリーパラメータとは、SQLクエリーで`{{}}`を用いるとその部分をパラメータ化でき、ウィジェットから値を指定できるようになる機能です。\n\nhttps://docs.databricks.com/ja/sql/user/queries/query-parameters.html\n\nWHERE句で使う分には問題ないのですが、以下のようにテーブル名をパラメータにしようとすると、\n\n```sql\nSELECT\n  *\nFROM\n  {{ tablename }}\n```\n\n以下のようにテーブル名の前後にクォーテーションが入ってしまい、パースエラーになってしまいます。\n\n```\n[PARSE_SYNTAX_ERROR] Syntax error at or near ''takaakiyayoi_catalog.japan_covid_analysis.covid_casesa''.line 4, pos 2\n```\n\nそこで登場するのがこちら。`IDENTIFIER`句です。文字列を受け取り、テーブル・ビュー、カラムなどとして解釈します。\n\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-names-identifier-clause.html\n\nクエリーをこのように変更します。\n\n```sql\nSELECT\n  *\nFROM\n  IDENTIFIER({{ tablename }})\n```\n\n下にウィジェットが表示されるので、テーブル名を入力します。\n![Screenshot 2023-12-07 at 16.23.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c4a73630-b546-13dc-78f5-5128e8d88d21.png)\n\n![Screenshot 2023-12-07 at 16.24.03.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b9b0c0e8-3c9b-fad2-3c60-dfda1b4fb312.png)\n\n動きました！\n![Screenshot 2023-12-07 at 16.34.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1ba8a390-6477-3e71-12ab-43b3370c3aa5.png)\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksによる高品質RAGアプリケーションの作成", "text": "[Building High Quality RAG Applications with Databricks \\| Databricks Blog](https://www.databricks.com/blog/building-high-quality-rag-applications-databricks)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n# 生成AIアプリケーションをプロダクションに移行するための新たなツールスイート\n\n[Retrieval\\-Augmented\\-Generation \\(RAG\\)](https://www.databricks.com/glossary/retrieval-augmented-generation-rag#:~:text=Retrieval%20augmented%20generation%20or%20RAG%20is%20an%20architectural%20approach%20that,as%20context%20for%20the%20LLM.)は、プロプライエタリかつリアルタイムのデータを[大規模言語モデル](https://www.databricks.com/glossary/large-language-models-llm#:~:text=Large%20language%20models%20(LLMs)%20are,near%2Darbitrary%20instructions%2C%20translation%20as)アプリケーションに組み込むためのパワフルな手段として急速に活用が進みました。皆様の企業データを用いて高品質なプロダクションLLMアプリの構築を支援する、RAGツールの**スイートを本日ローンチ**できることを嬉しく思っています。\n\nLLMは、新たなアプリケーションのプロトタイピングを迅速に行う際に非常に大きなブレイクスルーをもたらします。しかし、RAGアプリケーションを構築している数千のお客様企業とやり取りをした後に、これらのアプリケーションを**プロダクション品質**に持っていくためには最大の課題があることに気づきました。顧客に提供するアプリケーションに求められる品質基準に適合するためには、AIのアウトプットは正確で最新であり、企業の文脈を認識し安全である必要があります。\n\nRAGアプリケーションで高い品質を達成するには、開発者は自分たちのデータの品質やモデルのアウトプットを理解するためのリッチなツールと、RAGプロセスにおける全ての側面を組み合わせ、最適化できる基盤となるプラットフォームを必要としています。RAGには、データ準備、取得モデル、(SaaSやオープンソース)言語モデル、ランキング、後処理パイプライン、プロンプトエンジニアリング、カスタムの企業データに対するトレーニングモデルのように数多くのコンポーネントが関係します。Databricksは常に皆様のデータと最先端のML技術を組み合わせることにフォーカスしています。本日のリリースでは、この哲学をお客様が自身のデータを活用して高品質なAIアプリケーションを作成できるように拡張します。\n\n本日のリリースでは以下のパブリックプレビューが含まれています:\n\n- お使いのレイクハウスにあるテーブルに対するセマンティック検索を支援する[Vector Search](https://docs.databricks.com/ja/generative-ai/vector-search.html)サービス。\n- RAGアプリケーションから構造化されたコンテキストを利用できるようにする、オンライン[特徴量、関数のサービング](https://docs.databricks.com/ja/machine-learning/feature-store/feature-function-serving.html)。\n- トークンあたり課金の基盤LLMを提供する完全[マネージドの基盤モデル](https://docs.databricks.com/ja/machine-learning/foundation-models/index.html)。\n- RAGアプリのプロダクションパフォーマンスを監視する柔軟な[品質モニタリング](https://docs.databricks.com/ja/lakehouse-monitoring/index.html)インタフェース。\n- 様々なLLMを比較、消化するためのLLM開発ツールセット。\n\nこれらの機能は、プロダクションのRAGアプリケーションを構築する際に確認した3つの主要な課題に対応することを目的として設計されています。\n\n# 課題 #1 - お使いのRAGアプリケーションへのリアルタイムデータの提供\n\nRAGアプリケーションでは、最高の品質と最もパーソナライズされたレスポンスを生成するために、あなたの最新の構造化データ、非構造化データを組み合わせます。しかし、オンラインデータサービングのインフラストラクチャの維持管理は非常に困難なものとなる場合があり、企業はこれまでの経緯から複数のシステムを繋ぎ合わせており、集中管理されているデータレイクから注文仕立てのサービングレイヤーにデータをロードするための複雑なデータパイプラインを維持しなくてはなりませんでした。また、様々なインフラストラクチャスタックにデータのコピーが散財することで、重要なデータセットの保護が非常に困難となっています。\n\nこのリリースによって、Databricksではオンライン検索のためのあなたのデータのサービングとインデックス作成をネイティブにサポートします。[**Vector Search**](https://docs.databricks.com/ja/generative-ai/vector-search.html)は、Deltaテーブルから自動でデータのインデックスの作成とサービングを行うので、RAGアプリケーションにおけるセマンティック類似検索を通じてアクセスすることができます。ベストなパフォーマンス、スループット、コストを提供するためにVector Searchは内部で、処理の失敗、リトライの対応、バッチサイズの最適化を行います。構造化データに対して、[**特徴量と関数のサービング**](https://docs.databricks.com/ja/machine-learning/feature-store/feature-function-serving.html)は、企業がユーザーの情報に基づいてプロンプトのカスタマイズを行うために取り込みたいと考えることが多い、ユーザーやアカウントデータのような文脈データのmsオーダーのクエリーを提供します。\n\nUnity Catalogはサービングされるデータセットのオフライン、オンラインコピーの間のリネージを自動で追跡するので、データ品質問題のデバッグが容易になります。また、オンランデータセットとオフラインデータセット間のアクセスコントロール設定を強制するので、企業は誰がセンシティブなプロプライエタリ情報にアクセスしているのかを容易に監査、コントロールできることを意味します。\n![](https://cms.databricks.com/sites/default/files/inline-images/untitled-design_0.gif)\n\n# 課題 #2 - 基盤モデルの比較、チューニング、サービング\n\nRAGアプリケーションにおいて品質を決定づける主な要因は、ベースとなるLLMモデルの選択です。モデルは理由づけの能力、幻覚の傾向、コンテキストのウィンドウサイズ、サービングコストのような様々な次元で異なるので、モデルの比較は困難になることがあります。また、いくつかのモデルは、パフォーマンスを改善しコストを削減できるように、特定のアプリケーションにファインチューニングすることができます。ほぼ毎週新たなモデルがリリースされているので、特定アプリケーションにとってベストな選択肢を見つけ出すためにベースモデルの組み合わせを比較することは非常な重荷となります。さらに物事を複雑にしているのは、多くの場合モデルの提供者は異なるAPIを提供しており、RAGアプリケーションの迅速な比較や将来性の確認を非常に困難なものとしています。\n\nこのリリースでは、**LLM開発と評価に対する統合環境**を提供しており、特定のクラウドに依存しないプラットフォームでモデルファミリーに対して一貫性のあるツールセットを提供します。Databricksのユーザーは、Azure OpenAIサービス、AWS BedrockやAnthropicによる最先端のモデル、Lamma 2やMPTのようなオープンソースモデル、お客様がファインチューニングした完全にカスタムなモデルにアクセスすることができます。新たなインタラクティブなAIプレイグラウンドによって、MLflowと連携されたツールチェインが毒性、レーテンシー、トークン数のようなキーメトリクスを追跡することでリッチな比較を可能にしつつも、これらのとの容易なチャット可能にします。プレイグラウンドやMLflowにおける隣り合わせのモデル比較によって、ユーザーはそれぞれのユースケースにおけるベストなモデルを特定できるようになり、さらにはretrieverコンポーネントの評価もサポートしています。\n![](https://cms.databricks.com/sites/default/files/inline-images/playground-blog-0.png)\n\nまた、Databricksでは人気のあるLlamaやMPTモデルファミリーを含むフルマネージドのLLMモデルセットである[**Foundation Model API**](https://docs.databricks.com/ja/machine-learning/foundation-models/index.html)をリリースします。Foundation Model APIはトークンあたりの課金で使用することができるので、劇的にコストを削減し、柔軟性を高めます。Foundation Model APIはDatabricksのインフラストラクチャ内で提供されているので、センシティブなデータがサードパーティのサービスを経由する必要がありません。\n\n実際のところ、高品質の実現とは多くの場合、それぞれのアプリケーション固有の要件に合わせてベースモデルの組み合わせとマッチングを意味します。Databricksの[**モデルサービング**](https://www.databricks.com/jp/product/model-serving)アーキテクチャでは、完全にカスタムなモデル、Databricksマネージドのモデル、サードパーティの基盤モデルに至るすべてのタイプのLLMのデプロイ、制御、クエリーのための統合インタフェースを提供します。この柔軟性によって、お客様は適切なジョブのための適切なモデルを選択することができ、利用できるモデルのさらなる進化に直面したとしても将来にわたって対応できることになります。\n![](https://cms.databricks.com/sites/default/files/inline-images/blog-post.png)\n\n# 課題 #3 - プロダクションにおける品質と安全性の保証\n\nLLMアプリケーションがデプロイされると、どれだけ適切に動作しているのかを確認することが困難になることがあります。従来のソフトウェアと異なり、言語ベースのアプリケーションは単一の正しい答えや明確な「エラー」条件がありません。これは、品質(これはどれだけうまく動いているのか？)や異常、危険、毒性のあるアウトプット(これは安全なのか？)はどのようなものであるのかを理解することが簡単ではないことを意味します。Databricksでは、多くのお客様が小規模な内部向けプロトタイプで観測した品質が、大規模なユーザーベース向けにおいてどのように変換されるのかがわからないために、RAGアプリケーションのロールアウトを躊躇するのを多く見てきています。\n\nこのリリースに含まれる[Lakehouse Monitoring](https://www.databricks.com/jp/product/machine-learning/lakehouse-monitoring)は、RAGアプリケーションに対するフルマネージドな品質モニタリングをソリューションを提供します。Lakehouse Monitoringでは、アプリケーションのアウトプットの毒性、幻覚、その他の危険なコンテンツを自動でスキャンします。このデータは以降のアクションのために、ダッシュボード、アラート、その他の後段のデータパイプラインに入力されます。モニタリングはデータセットやモデルのリネージと連携しているので、開発者は古くなっているデータパイプラインや期待しない挙動をするモデルのようなエラーをクイックに診断することができます。\n\nモニタリングは安全性に対してだけのものではなく、品質に対しても適用されます。Lakehouse Monitoringは「サムズアップ/サムズダウン」スタイルのユーザーフィードバックや「ユーザー許容レーティング」(エンドユーザーはどのくらいの頻度でAI生成の提案を許容しているのか)のような派生メトリクスのようなアプリケーションレベルのコンセプトと連携することができます。我々の経験では、エンドツーエンドのユーザーメトリクスを計測することで、企業は公開されているRAGアプリケーションが適切に動作しているという自信を大きく強めることを目撃しています。また、パイプラインの監視もDatabricksによって完全に管理されているので、開発者は観測用のインフラストラクチャの管理ではなく、自分たちのアプリケーションに時間を費やすことができます。\n![](https://cms.databricks.com/sites/default/files/inline-images/image2_12.png)\n\nこのリリースにおけるモニタリング機能はまだ始まったばかりです。続報を楽しみにしていてください！\n\n# 次のステップ\n\n今週を通じて我々は詳細なブログ記事を公開し、次には実装のベストプラクティスの詳細に触れていきます。ですので、毎日Databricksブログをチェックしてください。そして、新たな[RAGのデモ](https://www.databricks.com/resources/demos/tours/deploy-llm-chatbots-rag-and-databricks-ai-vector-search?itm_data=demo_center)を通じて製品をチェックし、[オンデマンドで生成AIウェビナー](https://www.databricks.com/resources/webinar/disrupt-your-industry-generative-ai)をご覧いただき、実際のRAGツールスイートのクイックのデモをご覧ください。\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/89CTQQzpe1U?si=jeZvWBOpzgukN3Ok\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "dataikuとDatabricksとの連携を試してみる", "text": "こちらを実際に動かしてみます。\n\nhttps://docs.databricks.com/en/partners/ml/dataiku.html\n\nこの連携によって、dataikuの計算資源とストレージをDatabricksが提供する形になります。\n\n# Databricks Partner Connectからの接続\n\n事前にDataikuで使うカタログとスキーマを作成しておきます。ここでは、カタログ`dataiku`、スキーマ`connect`を作成しておきます。\n![Screenshot 2023-12-06 at 11.39.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/793c59cb-451c-fd71-9f21-d9c50012c5a4.png)\n\nPartner Connectの**dataiku**のタイルをクリックします。\n![Screenshot 2023-12-06 at 11.37.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ad2942c9-397b-d9ff-3028-bd9c0bff2e6f.png)\n\nカタログとスキーマを選択します。\n![Screenshot 2023-12-06 at 11.48.00.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/523cd2bd-826d-188a-320f-a750c7aa6484.png)\n\ndataiku接続用のユーザー、パーソナルアクセストークン、計算資源が作成されます。\n![Screenshot 2023-12-06 at 11.48.14.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9f0314e8-8df4-8d32-52ea-7896e39365c2.png)\n\n並行してdataikuのトライアルにサインアップしておきます。私は先にサインアップしましたが、この後の画面遷移でも自動でdataikuに移動するので、その際にサインアップでもいけるはずです。\n![Screenshot 2023-12-06 at 11.50.10.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3fe33c90-ddc9-0201-14c0-c88a94c25d19.png)\n\nNodesに**Databricks Partner Connect**と表示されているはずです。\n![Screenshot 2023-12-06 at 12.04.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6e0f076b-0d6e-4bfe-d4ff-387c37c068d1.png)\n\n# dataikuプロジェクトの実行\n\nこちらの記事を参考にさせていただきます。\n\nhttps://qiita.com/hktonton/items/dee44549acb2ee469eb6\n\nhttps://dev.classmethod.jp/articles/dataiku-basic-tutorial/\n\nBlank projectを選択して、空のプロジェクトを作成します。\n![Screenshot 2023-12-06 at 12.04.58.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a543f52d-1445-f9b1-6b2c-d8a662b0e0c2.png)\n\n**+ IMPORT YOUR FIRST DATASET**をクリックします。\n![Screenshot 2023-12-06 at 13.58.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/67d538aa-b708-2135-d7d9-abfacefda0ce.png)\n\nUpload you filesをクリックします。\n![Screenshot 2023-12-06 at 13.59.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/aadee398-f2f4-2801-ed9b-e6acdd1dcefd.png)\n\nここでは、COVID-19の感染者数のCSVファイルをドラッグ&ドロップします。\n![Screenshot 2023-12-06 at 13.59.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/39da98ca-ab74-ba85-0a5d-72cea09bd7cf.png)\n![Screenshot 2023-12-06 at 13.59.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/082fc3a7-4b07-36e8-234f-f7ad6872b445.png)\n\nCREATEをクリックします。\n![Screenshot 2023-12-06 at 14.00.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3f5d4b6d-e972-3ab7-52d8-f0e019282d0d.png)\n\nデータセットが作成されました。\n![Screenshot 2023-12-06 at 14.00.51.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/66be71ce-57af-aa3b-cfa8-6246144ec267.png)\n\n簡単な処理を実行します。上のメニューから**Flow**を選択します。\n![Screenshot 2023-12-06 at 14.03.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/90aa3494-c1c7-4fa2-8e80-27e60cfe3400.png)\n\n右の **+** をクリックしてペインを展開します。\n![Screenshot 2023-12-06 at 14.04.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/79bc4b1e-2836-062d-1152-d0e0bbc04f42.png)\n\nSample / Filterを選択します。Output datasetで書き込み先のDatabricksのカタログとスキーマを指定します。**CREATE RECIPE**をクリックします。\n![Screenshot 2023-12-06 at 14.05.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d3b6b58c-1478-dc34-50ef-e08c026e3e91.png)\n\n100行にサンプリングすることにして、**Run**をクリックします。これによって、Databricksのクラスターで処理が行われ、Unity Catalog配下のテーブルとして結果が保存されます。\n\n:::note\n**注意**\nこの時点で権限不足のエラーが出る場合には、カタログエクスプローラで権限を確認して不足の権限を設定してください。\n:::\n\nテーブルが作成されました！\n![Screenshot 2023-12-06 at 14.07.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6083ab02-4128-1bba-48d9-ed3135b7b3fa.png)\n![Screenshot 2023-12-06 at 14.08.14.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e38fddc7-9be2-3651-480a-1acd2cbd60f1.png)\n\nちなみに、dataikuのフローはこのような形になります。\n![Screenshot 2023-12-06 at 14.09.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6d6b9dc9-1f17-778c-8688-1aaada8cc0fb.png)\n\n\ndataikuとDatabricksを組み合わせて、データからの価値導出にお役立てください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Delta Live Tablesのチュートリアルを通じた新機能のウォークスルー", "text": "こちらのチュートリアルを実行しながら、最近の新機能をウォークスルーします。\n\nhttps://docs.databricks.com/ja/delta-live-tables/tutorial-sql.html\n\n以下のように、ノートブックでデータパイプラインを実装します。なお、[新たなセルUI](https://qiita.com/taka_yayoi/items/d08702b8e782e778121a)をオンにしています。\n\n![Screenshot 2023-12-06 at 10.31.04.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/56fbeb70-32f1-1309-9ba2-f3813cd81ee3.png)\n\nDLTパイプラインを作成します。\n![Screenshot 2023-12-06 at 10.34.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8fbbd74c-fce0-1f24-53fc-8374c171dc0d.png)\n\nここでいきなりパイプラインを実行するのではなく、新機能の検証(Validate)機能を試します。\n![Screenshot 2023-12-06 at 10.34.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/67f6a78d-d383-ead0-d8b1-7a76085289fc.png)\n\nhttps://docs.databricks.com/ja/delta-live-tables/updates.html#check-a-pipeline-for-errors-without-waiting-for-tables-to-update\n\n検証を実行すると、ターゲットテーブルにデータを書き込まずにパイプラインのロジックの検証を行うことができます。\n![Screenshot 2023-12-06 at 10.35.22.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bb721219-4a6d-441a-89b1-e0882f188634.png)\n\n検証がスタートします。\n![Screenshot 2023-12-06 at 10.37.12.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/fa2ac80b-2ca5-5962-0e75-aeaea21b371c.png)\n\n少し待つとパイプラインのグラフ構造が表示されます。\n![Screenshot 2023-12-06 at 10.42.44.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/27c87f4a-3690-7945-252e-5af49d2cbf15.png)\n\n指定したターゲットスキーマにもデータは書き込まれません。実際に処理を実行する前にロジックを確認する開発フェーズでは有用ですね。\n![Screenshot 2023-12-06 at 10.45.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b0f82ef3-4c2b-ef6a-e57c-2432661f5a18.png)\n\nちなみに**リスト**タブも追加されています。気付いてませんでした。パイプラインの各ステップを一覧することができ、フィルタリングも可能です。\n![Screenshot 2023-12-06 at 10.45.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/08c1f044-25ef-8e7e-b36f-03a600b7ed91.png)\n\nパイプラインを実行すると、リストビューでもリアルタイムで進捗を確認できます。\n![Screenshot 2023-12-06 at 10.47.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/529e0755-4f25-febb-e006-c10cb2b07882.png)\n![Screenshot 2023-12-06 at 10.48.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/270ef7b1-620d-ce85-2557-934ee0cd4dd0.png)\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksのFoundation Model APIを試してみる", "text": "こちらのAPIを試してみます。\n\nhttps://docs.databricks.com/ja/machine-learning/foundation-models/query-foundation-model-apis.html\n\n:::note\n**注意**\n執筆時点では、AWS東京リージョンでは利用できません。\n:::\n\n# Databricks Foundation Model APIとは？\n\nDatabrickモデルサービングでは、専用サービングエンドポイントから最先端のオープンソースモデルへアクセス、クエリーできるFoundation Model APIをサポートするようになりました。Foundation Model APIによって、開発者はご自身の手でモデルデプロイメントを維持管理することなしに、高品質な生成AIモデルを活用したアプリケーションをクイックかつ容易に構築できるようになります。\n\nhttps://docs.databricks.com/ja/machine-learning/foundation-models/index.html\n\nFoundation Model APIを利用できるワークスペースでは、Foundation Model APIのエンドポイントを確認できます。\n![Screenshot 2023-12-05 at 15.14.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7cbb42a5-4d98-f24d-4fa4-c783ed71ab84.png)\n\n# Databricks Foundation Model APIの活用\n\nライブラリをインストールします。\n\n```py\n%pip install databricks-genai-inference\ndbutils.library.restartPython()\n```\n\n`llama-2-70b-chat`を呼び出します。\n\n```py\nfrom databricks_genai_inference import ChatCompletion\n\nresponse = ChatCompletion.create(model=\"llama-2-70b-chat\",\n                                 messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                                           {\"role\": \"user\",\"content\": \"Knock knock.\"}],\n                                 max_tokens=128)\nprint(f\"response.message:{response.message}\")\n```\n\n動きました！\n```\nresponse.message:\nWho's there? I'm here to help with any questions or tasks you may have.\n```\n\n```py\nfrom databricks_genai_inference import ChatSession\n\nchat = ChatSession(model=\"llama-2-70b-chat\", system_message=\"You are a helpful assistant.\", max_tokens=128)\nchat.reply(\"Knock, knock!\")\nchat.last # return \"Hello! Who's there?\"\nchat.reply(\"Guess who!\")\nchat.last # return \"Okay, I'll play along! Is it a person, a place, or a thing?\"\n\nchat.history\n```\n\nなんてお手軽な。\n```\n[{'role': 'system', 'content': 'You are a helpful assistant.'},\n {'role': 'user', 'content': 'Knock, knock!'},\n {'role': 'assistant', 'content': \"\\nWho's there?\"},\n {'role': 'user', 'content': 'Guess who!'},\n {'role': 'assistant',\n  'content': \"Okay, I'll play along! Is it a person, a place, or a thing?\"}]\n```\n\nエンべディングモデルを呼び出します。\n\n```py\nfrom databricks_genai_inference import Embedding\n\nresponse = Embedding.create(\n    model=\"bge-large-en\",\n    input=\"3D ActionSLAM: wearable person tracking in multi-floor environments\")\nprint(f'embeddings: {response.embeddings}')\n```\n\nエンべディングを取得できます。\n```\nembeddings: [[0.050384521484375, 0.0523681640625, -0.019805908203125, -0.0118560791015625, 0.00033473968505859375, 0.01215362548828125, -0.01873779296875, 0.051788330078125, 0.047943115234375, 0.016265869140625, -0.03143310546875, 0.053802490234375, -0.01314544677734375, -0.027374267578125, 0.0021533966064453125, 0.043914794921875, -0.002056121826171875, -0.00612640380859375, 0.0245361328125, -0.00933074951171875, 0.0211944580078125, 0.045074462890625, -0.08319091796875, -0.054443359375, 0.00498199462890625, 0.0257415771484375, 0.03857421875, -0.0284576416015625, 0.043731689453125, 0.06231689453125, 0.02996826171875, -0.021697998046875, 0.032257080078125, -0.058837890625, -0.0283355712890625, -0.0260162353515625,\n```\n\n`mpt-7b-instruct`を呼び出します。\n\n```py\nfrom databricks_genai_inference import Completion\n\nresponse = Completion.create(\n    model=\"mpt-7b-instruct\",\n    prompt=\"Write 3 reasons why you should train an AI model on domain specific data sets.\",\n    max_tokens=128)\nprint(f\"response.text:{response.text:}\")\n```\n```\nresponse.text:['AI models are known to learn better when given task and domain specific training data. The following are a few reasons why it would be beneficial for an organization to prepare their own training data sets and train an AI model on it. 1. Internal requirements of the organization can be best understood by an internally curated data set. 2. Creating and maintaining a privacy protected data set is only possible if the organization is in control of it. 3. Data curation and verification processes can be best executed by the organization themselves.']\n```\n\n一応、日本語でも動作します。\n\n```py\nfrom databricks_genai_inference import Completion\n\nresponse = Completion.create(\n    model=\"mpt-7b-instruct\",\n    prompt=\"ドメイン固有のデータセットでAIモデルをトレーニングすべき3つの理由を挙げてください。\",\n    max_tokens=128)\nprint(f\"response.text:{response.text:}\")\n```\n```\nresponse.text:['3つの理由には、分析データ、マルチバインデース、丁寧な請求書に対する支出セキュリティがあります。\\nあいで、データの整合性を保つことができるかもしれないパターンを发見するために、モデルをトレーニングさせています。']\n```\n\n```py\nfrom databricks_genai_inference import ChatCompletion\n\nresponse = ChatCompletion.create(model=\"llama-2-70b-chat\",\n                                 messages=[{\"role\": \"system\", \"content\": \"あなたは有能なアシスタントです。日本語で回答します。\"},\n                                           {\"role\": \"user\",\"content\": \"日本で一番高い山は\"}],\n                                 max_tokens=128)\nprint(f\"response.message:{response.message}\")\n```\n```\nresponse.message:日本で一番高い山は、富士山です。標高3,776メートルです。\n```\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksノートブックの新しいセルUIを試す", "text": ":::note\n本機能ではフィードバックを募集しています。\n:::\n\nユーザー設定画面の**開発者**を見ると新たな項目が。\n\n**新しいセルUI**をオンにします。\n![Screenshot 2023-12-05 at 12.25.22.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e7d1c516-153b-704f-21d4-aac9c7103bc7.png)\n\nあと、**Custom light theme**でテーマを選べるようになってます。Dracula Lightを選択します。\n\nノートブックを作成します。すると、このようなUIが。\n![Screenshot 2023-12-05 at 12.26.04.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d76f2a61-ec40-24d0-7d4d-4ed63bbb9d1a.png)\n\nコードを記述して実行します。\n![Screenshot 2023-12-05 at 12.26.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f35e5d28-b1bd-2087-d6bc-7ebd14846c6e.png)\n![Screenshot 2023-12-05 at 12.26.35.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9f8e316b-64aa-8210-4ac9-33b0a67695e4.png)\n\nセルを追加するには、既存のセルの下にカーソルを移動します。コードかテキストを選択します。\n![Screenshot 2023-12-05 at 12.26.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/faa2f30c-d59c-df1c-d7f3-be396f5ddb12.png)\n\nコードを選択すると上と同じようにコードを入力するセルが追加されます。\n![Screenshot 2023-12-05 at 12.26.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a88581c6-d4a8-dccd-1678-48bd723e1612.png)\n\nテキストを選択するとマークダウンを記述するセルが追加されます。\n![Screenshot 2023-12-05 at 12.27.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ae7085cb-21bc-a3b9-8aa0-2c171f9e87ac.png)\n![Screenshot 2023-12-05 at 12.27.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f1c52172-7e90-5f2e-519a-18fb76bac479.png)\n\nセルの右上の言語をクリックすることで、セルの言語を切り替えることができます。\n![Screenshot 2023-12-05 at 12.27.38.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e4ee27b0-6544-2aed-5360-8caad085cb1c.png)\n\nケバブアイコンからはコンテキストメニューにアクセスできます。\n![Screenshot 2023-12-05 at 12.28.13.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/cd8f9ca7-e563-a49a-9b82-11f6fd992508.png)\n\n![Screenshot 2023-12-05 at 12.39.31.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/596bfb9a-be66-c933-3bee-74182bfae8fa.png)アイコンをクリックするとFocus modeに移行します。一つのセルのみが表示されるようになります。結果は下部に表示されます。\n![Screenshot 2023-12-05 at 12.28.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/90a08867-911b-3b02-d81b-20b7249e3248.png)\n\nフィードバックをお待ちしています！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks REST APIによるユーザーとグループの作成", "text": "DatabricksではREST APIで色々操作できます。ここでは、ユーザーとグループを作成してみます。\n\n# ユーザーの作成\n\n:::note info\n**プレビュー**\n本機能は[パブリックプレビュー](https://docs.databricks.com/release-notes/release-types.html)です。\n:::\n\nhttps://docs.databricks.com/api/workspace/users/create\n\nなお、マニュアルに記載があるように、ワークスペースに追加されたユーザーはアカウントユーザーとしても追加されます。\n\n> Creates a new user in the Databricks workspace. This new user will also be added to the Databricks account.\n\nワークスペース管理者のパーソナルアクセストークンを取得しておきます。\n\nターミナルで以下を実行します。\n\n```sh\ncurl -v -X POST  \"https://tydatabricks-tydb-test-environment.cloud.databricks.com/api/2.0/preview/scim/v2/Users\" \\\n--header 'Authorization: Bearer [パーソナルアクセストークン]' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:User\"],\n    \"userName\": \"takaaki.yayoi+test4@databricks.com\",\n    \"name\": {\n        \"familyName\": \"Yayoi\",\n        \"givenName\": \"Takaaki\"\n    },\n    \"emails\": [\n        {\n            \"value\": \"takaaki.yayoi+test4@databricks.com\",\n            \"type\": \"work\"\n        }\n    ]\n}' | jq\n```\n\nワークスペースの管理画面でユーザーを確認できます。って、管理画面のレイアウトが変わってる…。\n![Screenshot 2023-12-05 at 9.22.31.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6b91b3f6-a3d9-9778-ab7c-a58798857096.png)\n\nアカウントユーザーになっているのでアカウントコンソールでも確認できます。\n![Screenshot 2023-12-05 at 9.23.39.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/cdaae3ef-70ea-86c0-7593-cb216e1f9d7f.png)\n\nまた、招待メールが当該アドレスに送信されます。\n![Screenshot 2023-12-05 at 9.19.40.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/be9512f7-d828-2cf2-3f51-cd6c5cc92aa7.png)\n\n# グループの作成\n\nこちらのAPIになります。\n\nhttps://docs.databricks.com/api/workspace/groups/create\n\n:::note info\n**プレビュー**\n本機能は[パブリックプレビュー](https://docs.databricks.com/release-notes/release-types.html)です。\n:::\n\nなお、ここで作成されるグループはワークスペース固有の[ワークスペースローカルグループ](https://docs.databricks.com/ja/administration-guide/users-groups/groups.html#difference-between-account-groups-and-workspace-local-groups)なので、アカウントコンソールには反映されません。\n\n以下のコマンドを実行します。\n\n```sh\ncurl -v -X POST  \"https://tydatabricks-tydb-test-environment.cloud.databricks.com/api/2.0/preview/scim/v2/Groups\" \\\n--header 'Authorization: Bearer [パーソナルアクセストークン]' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:Group\"],\n    \"displayName\": \"workspace_group\"}' | jq\n```\n\nワークスペースの管理画面でユーザーを確認できます。\n![Screenshot 2023-12-05 at 9.27.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c62f1512-ead8-0dc0-f1ef-0d79fba61d61.png)\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksのVector Searchを動かしてみる", "text": "こちらのウォークスルーです。まずは本当に一通り動かすところまでです。\n\nhttps://qiita.com/taka_yayoi/items/529b8ae9da0a7a5e2d9c\n\n# エンべディングモデルの登録とサービング\n\nhttps://docs.databricks.com/en/_extras/notebooks/source/generative-ai/embedding-with-oss-models.html\n\nここではOSSのエンべディングモデルを使います。\n\nこのノートブックでは、Vector Searchで使用できるモデルサービングエンドポイントで`e5-small-v2`のオープンソーステキストエンべディングモデルをセットアップします。\n\n* Hugging Face Hubからモデルをダウンロード\n* MLflowモデルレジストリへの登録\n* モデルがサーブされるモデルサービングエンドポイントを起動\n\nモデル`e5-small-v2`は右で公開されています: https://huggingface.co/intfloat/e5-small-v2\n* MITライセンス\n* 派生系:\n   * https://huggingface.co/intfloat/e5-large-v2\n   * https://huggingface.co/intfloat/e5-base-v2\n   * https://huggingface.co/intfloat/e5-small-v2\n\nこれはDBR 13.2 MLでテストされていますが、他のバージョンでも動くはずです。特定のライブラリバージョンについては、[DBR 13.2 ML release notes](https://docs.databricks.com/release-notes/runtime/13.2ml.html)をご覧ください。\n\n## Databricks python SDKのインストール\n\nサービングエンドポイントを操作するためにPythonクライアントを使用します。\n\n```py\n%pip install -U databricks-sdk\ndbutils.library.restartPython()\n```\n\n## モデルのダウンロード\n\n```py\n# sentence_transformersライブラリを用いたモデルのダウンロード\nfrom sentence_transformers import SentenceTransformer\n\nsource_model_name = 'intfloat/e5-small-v2'  # Hugging Face Hubにおけるモデル名\nmodel = SentenceTransformer(source_model_name)\n```\n\n```py\n# 動作確認のためにモデルをテスト\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n```\n[[-7.24058524e-02  4.72473949e-02  1.45743387e-02 -3.30961198e-02\n  -4.54308167e-02  2.82988641e-02  1.07215106e-01 -3.77488472e-02\n   9.86611657e-03  3.15312035e-02  3.53261307e-02 -3.10433619e-02\n  -4.09466177e-02  4.39202003e-02  1.54092228e-02 -5.30367047e-02\n  -1.00718113e-02  5.08562103e-02 -1.31060824e-01  1.89213548e-02\n   8.88702646e-02 -6.99376166e-02  5.36029600e-03 -5.79565428e-02\n  -2.11174395e-02  5.67687815e-03  6.33126823e-03  9.21319053e-02\n  -1.43286828e-02 -8.07622373e-02 -4.14064080e-02 -8.48542550e-05\n   5.25100119e-02 -3.95204611e-02  6.05509654e-02 -2.89479122e-02\n  -4.94688265e-02  1.98759101e-02  2.70234849e-02  2.00989563e-02\n```\n\n## MLflowにモデルを登録\n\n```py\n# MLflowのモデル名: モデルレジストリではこのモデル名を使用します\nregistered_model_name = 'taka-e5-small-v2'\n```\n\n```py\nimport mlflow\n\n# 入力/出力スキーマの計算\nsignature = mlflow.models.signature.infer_signature(sentences, embeddings)\nprint(signature)\n```\n\n```\ninputs: \n  [string]\noutputs: \n  [Tensor('float32', (-1, 384))]\nparams: \n  None\n```\n\n```py\nmodel_info = mlflow.sentence_transformers.log_model(\n  model,\n  artifact_path=\"model\",\n  signature=signature,\n  input_example=sentences,\n  registered_model_name=registered_model_name)\n``` \n\nモデルが記録されます。\n![Screenshot 2023-12-04 at 17.19.38.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a83cea1f-634c-507a-05c1-639d6cedd0b6.png)\n\n```py\n# これによって、プログラムから登録したモデルのバージョンを取得します。\n# 実際には、使用する前にモデルを\"Productions\"のステージにプロモーションすることでしょう。\nmlflow_client = mlflow.MlflowClient()\nmodels = mlflow_client.get_latest_versions(registered_model_name, stages=[\"None\"])\nmodel_version = models[0].version\nmodel_version\n```\n\nバージョン1のモデルが記録されています。\n```\n'1'\n```\n\n### モデルサービングエンドポイントの作成\n\n詳細については、[モデルサービングエンドポイントの作成および管理](https://docs.databricks.com/ja/machine-learning/model-serving/create-manage-serving-endpoints.html)をご覧ください。\n\n:::note warn\n**警告**: このサンプルでは0にまでスケールダウンする*small*なCPUエンドポイントを作成します。より現実的なユースケースでは以下を必要とするかもしれません:\n* 高速な計算のためのGPUエンドポイント\n* (モデルサービングエンドポイントのコールドスタートにはある程度の起動時のオーバーヘッドがあるため)頻繁なクエリーが予測される場合には0にスケールダウンしないように\n:::\n\n```py\nendpoint_name = \"taka-e5-small-v2\"  # 作成するエンドポイント名\n```\n\n#### Databricks SDKワークスペースクライアントの作成\n\n```py\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.serving import EndpointCoreConfigInput\n\nw = WorkspaceClient()\n```\n\n#### エンドポイントの作成 \n\n```py\nendpoint_config_dict = {\n    \"served_models\": [\n        {\n            \"name\": f'{registered_model_name.replace(\".\", \"_\")}_{1}',\n            \"model_name\": registered_model_name,\n            \"model_version\": model_version,\n            \"workload_type\": \"CPU\",\n            \"workload_size\": \"Small\",\n            \"scale_to_zero_enabled\": True,\n        }\n    ]\n}\n\nendpoint_config = EndpointCoreConfigInput.from_dict(endpoint_config_dict)\n\n# エンドポイントの準備ができるまでには数分を要します\nw.serving_endpoints.create_and_wait(name=endpoint_name, config=endpoint_config)\n```\n```\nServingEndpointDetailed(config=EndpointCoreConfigOutput(config_version=1, served_models=[ServedModelOutput(creation_timestamp=1701671794000, creator='takaaki.yayoi@databricks.com', environment_vars=None, instance_profile_arn=None, model_name='taka-e5-small-v2', model_version='1', name='taka-e5-small-v2_1', scale_to_zero_enabled=True, state=ServedModelState(deployment=<ServedModelStateDeployment.DEPLOYMENT_READY: 'DEPLOYMENT_READY'>, deployment_state_message=''), workload_size='Small', workload_type='CPU')], traffic_config=TrafficConfig(routes=[Route(served_model_name='taka-e5-small-v2_1', traffic_percentage=100)])), creation_timestamp=1701671794000, creator='takaaki.yayoi@databricks.com', id='4d471313a0fa41d6bd9da6a586768b01', last_updated_timestamp=1701671794000, name='taka-e5-small-v2', pending_config=None, permission_level=<ServingEndpointDetailedPermissionLevel.CAN_MANAGE: 'CAN_MANAGE'>, state=EndpointState(config_update=<EndpointStateConfigUpdate.NOT_UPDATING: 'NOT_UPDATING'>, ready=<EndpointStateReady.READY: 'READY'>), tags=None)\n```\n\nエンべディングモデルをサービングするエンドポイントが作成されます。\n![Screenshot 2023-12-04 at 17.22.53.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/defc030a-7120-e182-c332-54db25ceb4b7.png)\n\n\n### (準備ができたら)エンドポイントへのクエリー\n\n上の`create_and_wait`コマンドは、エンドポイントが準備できるまで待つはずです。サービングエンドポイントの準備状態に関しては、DatabricksのUIでチェックすることもできます。\n\n詳細は[サービングエンドポイントへのスコアリングリクエストの送信](https://docs.databricks.com/ja/machine-learning/model-serving/score-model-serving-endpoints.html)をご覧ください。\n\n```py\nimport time\n\nstart = time.time()\n\n# (準備ができていないエンドポイントから)タイムアウトエラーを受け取る際には、待ってから以下を再実行します\nendpoint_response = w.serving_endpoints.query(name=endpoint_name, dataframe_records=['Hello world', 'Good morning'])\n\nend = time.time()\n\nprint(endpoint_response)\nprint(f'Time taken for querying endpoint in seconds: {end-start}')\n```\n\n```\nQueryEndpointResponse(predictions=[[-0.009296439588069916, 0.06918294727802277, 0.036366499960422516, 0.0034117570612579584, -0.015888312831521034, -0.015289321541786194, 0.06358581781387329, -0.08108192682266235, -0.0022949744015932083, 0.04259680584073067, 0.0557326041162014, 0.01342878770083189, -0.03011913038790226, 0.04097330570220947, 0.0466892383992672, -0.047855935990810394, 0.011086142621934414, 0.010089868679642677, -0.14671120047569275, 0.02561090514063835, 0.05807391181588173, -0.04596301168203354, -0.010017135180532932, -0.045596107840538025, -0.03825042396783829, -0.01948373019695282, 0.026301827281713486, 0.04099428281188011, -0.05741281434893608, -0.12793973088264465, -0.046298056840896606, -0.0011654832633212209, 0.05256413668394089, -0.029035048559308052, 0.045775555074214935, -0.042124878615140915, 0.022786177694797516, 0.02557487040758133, 0.01801937073469162,\n:\n:\nTime taken for querying endpoint in seconds: 0.16251897811889648\n```\n\nこれでエンべディングを返却するモデルサービングエンドポイントが起動しました。\n\n# Vector Search Python SDK活用のサンプル\n\nhttps://docs.databricks.com/en/_extras/notebooks/source/generative-ai/vector-search-python-sdk-example.html\n\nこのノートブックでは、Vector Searchを操作する際の主要なAPIとしての`VectorSearchClient`を提供するVector Search Python SDKの使用方法をデモンストレーションします。\n\nあるいは、直接REST APIを呼び出すことができます。\n\n**前提条件**: このノートブックではエンべディングモデルのモデルサービングエンドポイントがすでに作成されていることを前提としています。以下の`embedding_model_endpoint`とエンドポイント作成の関連ノートブックをご覧ください。\n\n```py\n%pip install --upgrade --force-reinstall databricks-vectorsearch\ndbutils.library.restartPython()\n```\n\n```py\nfrom databricks.vector_search.client import VectorSearchClient\n\nvsc = VectorSearchClient()\n```\n\n```py\nhelp(VectorSearchClient)\n```\n```\nHelp on class VectorSearchClient in module databricks.vector_search.client:\n\nclass VectorSearchClient(builtins.object)\n |  VectorSearchClient(workspace_url=None, personal_access_token=None, service_principal_client_id=None, service_principal_client_secret=None, azure_tenant_id=None, azure_login_id=None, disable_notice=False)\n |  \n |  Methods defined here:\n |  \n |  __init__(self, workspace_url=None, personal_access_token=None, service_principal_client_id=None, service_principal_client_secret=None, azure_tenant_id=None, azure_login_id=None, disable_notice=False)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  create_delta_sync_index(self, endpoint_name, index_name, primary_key, source_table_name, pipeline_type, embedding_dimension=None, embedding_vector_column=None, embedding_source_column=None, embedding_model_endpoint_name=None)\n |  \n |  create_direct_access_index(self, endpoint_name, index_name, primary_key, embedding_dimension, embedding_vector_column, schema)\n |  \n |  create_endpoint(self, name, endpoint_type='STANDARD')\n |  \n |  delete_endpoint(self, name)\n```\n\n## トイデータセットをソースDeltaテーブルにロード\n\n```py\n# 以下のソースDeltaテーブルを作成します\nsource_catalog = \"quickstart_catalog_taka\"\nsource_schema = \"vector_search\"\nsource_table = \"en_wiki\"\nsource_table_fullname = f\"{source_catalog}.{source_schema}.{source_table}\"\n```\n```py\nsource_df = spark.read.parquet(\"dbfs:/databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet\").limit(10)\ndisplay(source_df)\n```\n![Screenshot 2023-12-04 at 17.55.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2844fbf1-22a6-ec3a-1379-16c54f3bfeaa.png)\n\n```py\nsource_df.write.format(\"delta\").option(\"delta.enableChangeDataFeed\", \"true\").saveAsTable(source_table_fullname)\n```\n```py\ndisplay(spark.sql(f\"SELECT * FROM {source_table_fullname}\"))\n```\n![Screenshot 2023-12-04 at 17.56.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6769ba4b-1e0e-bd50-b4e6-0fddc9bcd4a5.png)\n\nカタログエクスプローラにもテーブルが表示されます。\n![Screenshot 2023-12-04 at 17.56.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/49a46344-9d22-d550-2f50-a40416ff190e.png)\n\n## Vector Searchエンドポイントの作成\n\n```py\nvector_search_endpoint_name = \"taka-vector-search-demo-endpoint\"\n```\n```py\nvsc.create_endpoint(\n    name=vector_search_endpoint_name,\n    endpoint_type=\"STANDARD\"\n)\n```\n```json\n{'name': 'taka-vector-search-demo-endpoint',\n 'creator': 'takaaki.yayoi@databricks.com',\n 'creation_timestamp': 1701673189267,\n 'last_updated_timestamp': 1701673189267,\n 'endpoint_type': 'STANDARD',\n 'last_updated_user': 'takaaki.yayoi@databricks.com',\n 'id': '0657e36c-652f-4036-8046-27ba3aabc3fa',\n 'endpoint_status': {'state': 'PROVISIONING'}}\n```\n\nしばらくすると、Vector Searchのエンドポイントが作成されます。\n![Screenshot 2023-12-04 at 17.59.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/20d750f0-7e7d-5d03-bebe-00cea662baca.png)\n\n# Vectorインデックスの作成\n\n```py\n# Vectorインデックス\nvs_index = \"en_wiki_index\"\nvs_index_fullname = f\"{source_catalog}.{source_schema}.{vs_index}\"\n\nembedding_model_endpoint = \"taka-e5-small-v2\"\n```\n\n```py\nindex = vsc.create_delta_sync_index(\n  endpoint_name=vector_search_endpoint_name,\n  source_table_name=source_table_fullname,\n  index_name=vs_index_fullname,\n  pipeline_type='TRIGGERED',\n  primary_key=\"id\",\n  embedding_source_column=\"text\",\n  embedding_model_endpoint_name=embedding_model_endpoint\n)\nindex.describe()\n```\n\n## Vectorインデックスの取得\n\n```py\n# Vectorインデックスの取得\n## Vectorインデックス名を用いてVector indexオブジェクトを取得するために get_index() メソッドを使用\nindex = vsc.get_index(endpoint_name=vector_search_endpoint_name, index_name=vs_index_fullname)\n## インデックスの設定情報のサマリーを参照するためにインデックスオブジェクトに対して describe() を使用\nindex.describe()\n```\n```json\n{'name': 'quickstart_catalog_taka.vector_search.en_wiki_index',\n 'endpoint_name': 'taka-vector-search-demo-endpoint',\n 'primary_key': 'id',\n 'index_type': 'DELTA_SYNC',\n 'delta_sync_index_spec': {'source_table': 'quickstart_catalog_taka.vector_search.en_wiki',\n  'embedding_source_columns': [{'name': 'text',\n    'embedding_model_endpoint_name': 'taka-e5-small-v2'}],\n  'pipeline_type': 'TRIGGERED',\n  'pipeline_id': 'ede5fde5-869c-41e7-96aa-4ea1132283a3'},\n 'status': {'detailed_state': 'ONLINE_NO_PENDING_UPDATE',\n  'message': 'Index creation succeeded using Delta Live Tables: https:xxxxx#joblist/pipelines/ede5fde5-869c-41e7-96aa-4ea1132283a3/updates/0c3fafd9-105c-4fe7-8674-e6d5f1d2f9db',\n  'indexed_row_count': 10,\n  'triggered_update_status': {'last_processed_commit_version': 0,\n   'last_processed_commit_timestamp': '2023-12-04T06:56:05Z'},\n  'ready': True,\n  'index_url': 'xxxx/api/2.0/vector-search/endpoints/taka-vector-search-demo-endpoint/indexes/quickstart_catalog_taka.vector_search.en_wiki_index'},\n 'creator': 'takaaki.yayoi@databricks.com'}\n```\n\nカタログエクスプローラにVector Search Indexが表示されます。しばらくすると、**Data Ingest**のUpdate statusが**完了**になります。\n![Screenshot 2023-12-04 at 18.03.43.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/08c77b28-9461-776b-83be-cb21b03e614f.png)\n\n## 類似検索\n\n類似ドキュメントを特定するためにVector Indexをクエリーします！\n\n```py\n# returns [col1, col2, ...]\n# これはカラムの任意のサブセットに設定することができます\nall_columns = spark.table(source_table_fullname).columns\n\nresults = index.similarity_search(\n  query_text=\"Greek myths\",\n  columns=all_columns,\n  num_results=2)\n\nresults\n```\n\n結果が返ってきました！\n\n```\n{'manifest': {'column_count': 8,\n  'columns': [{'name': 'title'},\n   {'name': 'id'},\n   {'name': 'revisionId'},\n   {'name': 'revisionTimestamp'},\n   {'name': 'revisionUsername'},\n   {'name': 'revisionUsernameId'},\n   {'name': 'text'},\n   {'name': 'score'}]},\n 'result': {'row_count': 2,\n  'data_array': [['Hercules',\n    13770.0,\n    681213961.0,\n    '2015-09-15T21:36:46',\n    'Djkeddie',\n    1884088.0,\n    '{{About|Hercules in classical mythology|the Greek divine hero  from which Hercules was adapted|Heracles|other uses|Hercules (disambiguation)}} {{pp-semi-indef|small=yes}}{{Infobox deity | type = Roman | name = Hercules | image = Pieter paul rubens, ercole e i leone nemeo, 02.JPG | image_size =  | alt =  | birth_place = | death_place = | caption = \\'\\'Hercules fighting the Nemean lion\\'\\'{{br}}by [[Peter Paul Rubens]] | god_of =  | abode = | symbol = | consort = [[Juventas]] | parents = [[Jupiter (mythology)|Jupiter]] and [[Alcmene]] | siblings = | children = | mount =  |\n```\n\n## 結果をLangChainドキュメントに変換\n\n取得された最初のカラムは page_content にロードされ、残りはメタデータにロードされます。\n\n```py\nfrom langchain.schema import Document\n\ndef convert_vector_search_to_documents(results) -> list[Document]:\n  column_names = []\n  for column in results[\"manifest\"][\"columns\"]:\n      column_names.append(column)\n\n  langchain_docs = []\n  for item in results[\"result\"][\"data_array\"]:\n      metadata = {}\n      score = item[-1]\n      # print(score)\n      i = 1\n      for field in item[1:-1]:\n          # print(field + \"--\")\n          metadata[column_names[i][\"name\"]] = field\n          i = i + 1\n      doc = Document(page_content=item[0], metadata=metadata)  # , 9)\n      langchain_docs.append(doc)\n  return langchain_docs\n\nlangchain_docs = convert_vector_search_to_documents(results)\n\nlangchain_docs\n```\n```\n[Document(page_content='History of physics', metadata={'id': 13758.0, 'revisionId': 679701588.0, 'revisionTimestamp': '2015-09-06T07:20:54', 'revisionUsername': 'Thony C.', 'revisionUsernameId': 3940951.0, 'text': '[[File:Newtons cradle animation book 2.gif|thumb|\"If I have seen further, it is only by standing on the shoulders of giants.\" &ndash;&nbsp;[[Isaac Newton]]&#8201;<ref>Letter to [[Robert Hooke]] (15 February 1676 by Gregorian reckonings with January 1 as New Year\\'s Day). equivalent to 5 February 1675 using the [[Julian calendar]] with March 25 as New Year\\'s Day</ref>]]  [[Physics]] (from the [[Ancient Greek]] φύσις \\'\\'[[physis]]\\'\\' meaning \"[[nature]]\") is the fundamental branch of [[science]] that developed out of the study of nature and [[philosophy]] known, until around the end of the 19th century, as \"[[natural philosophy]]\". Today, physics is ultimately defined as the study of [[matter]], [[energy]] and the relationships between them.{{citation needed|date=February 2015}} \n```\n\n## Vectorインデックスの削除\n\n```py\nvsc.delete_index(index_name=vs_index_fullname)\n```\n\nとりあえず動きました！次は日本語で動かしてみます。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks Vector Searchのインデックスの作成、クエリー方法", "text": "[How to create and query a Vector Search index \\| Databricks on AWS](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html) [2023/11/30時点]の翻訳です。\n\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n:::note info\n**プレビュー**\n本機能は[パブリックプレビュー](https://docs.databricks.com/release-notes/release-types.html)です。\n:::\n\n本書では[Databricks Vector Search](https://qiita.com/taka_yayoi/items/494b00f2c8fc60dcc466)を用いたVector Searchインデックスの作成方法とクエリー方法を説明します。\n\nUI、Python SDK、REST APIを用いてVector SearchエンドポイントやVector SearchインデックスのようなVector Searchのコンポーネントを作成、管理することができます。最もシンプルな方法としてはUIの使用をお勧めしますが、このガイドでは参照とクエリーにおけるSDKの利用方法もデモンストレーションします。\n\n# 要件\n\n- Unity Catalogが有効化されたワークスペース。\n- アカウントコンソールでサーバレスコンピュートを有効化。\n- インデックスを作成するカタログスキーマに対する`CREATE TABLE`権限。\n- [パーソナルアクセストークンが有効化されていること。](https://docs.databricks.com/ja/administration-guide/access-control/tokens.html)\n- SDKを使いたいのであれば、ノートブックにインストールすること。\n\n    ```py\n    %pip install databricks-vectorsearch\n    dbutils.library.restartPython()\n    ```\n\n# Vector Searchエンドポイントの作成\n\nUIを用いてVector Searchのエンドポイントを作成することができます。\n\n1. **Compute** UIを開くために左のサイドバーの**Compute**をクリックします。\n1. **Vector Search**タブをクリックし、**Create**をクリックします。\n![](https://docs.databricks.com/en/_images/create-endpoint.png)\n1. **Create endpoint form**が開きます。このエンドポイントの名前を入力します。\n1. Confirmをクリックします。\n\nSDKを使う場合には、Vector Searchエンドポイントを作成するために、以下を使うことができます。\n\n```py\nclient = VectorSearchClient()\n\nclient.create_endpoint(\n    name=\"vector_search_endpoint_name\",\n    endpoint_type=\"STANDARD\"\n)\n```\n\n# モデルサービングエンドポイントの設定\n\nこのセクションのガイドは、Databricksがあなたのためにエンべディングベクトルを自動で計算させるためのVector Searchインデックスを作成しようとしている場合にのみ適用されます。Deltaテーブルにご自身のエンべディングをお持ちの場合や、ご自身でエンべディングベクトルを計算する予定がある場合には、[Vector Searchインデックスの作成](#vector-searchインデックスの作成)までスキップしてください。\n\nVector Searchにえんべディングを計算させるためには、エンべディング生成のための[モデルサービングエンドポイント](https://docs.databricks.com/ja/machine-learning/model-serving/index.html)を設定する必要があります。お好きなエンべディングモデルのサービングのためのモデルサービングエンドポイントを設定するサンプルについては、[エンべディングモデル呼び出しのサンプルノートブック](#エンべディングモデル呼び出しのサンプルノートブック)のノートブックをご覧ください。\n\nVector Searchのためのモデルサービングエンドポイントをゼロにスケールダウンしないように設定することをお勧めします。サービングエンドポイントはウォームアップに数分を必要とするので、スケールダウンしているエンドポイントのインデックスに対する最初のクエリーは、エンドポイントのウォームアップ中にタイムアウトになるためです。\n\n# Vector Searchインデックスの作成\n\nUIを用いてVector Searchインデックスを作成することができます。以下のステップでは、ソースのDeltaテーブルの変更があった際に自動で更新される[Delta Sync Index](https://qiita.com/taka_yayoi/items/494b00f2c8fc60dcc466#%E3%83%9E%E3%83%8D%E3%83%BC%E3%82%B8%E3%83%89%E3%82%A8%E3%83%B3%E3%83%99%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E3%82%92%E7%94%A8%E3%81%84%E3%81%9Fdelta-sync-index)の作成方法を説明しています。[Vector Searchインデックス](https://qiita.com/taka_yayoi/items/494b00f2c8fc60dcc466#vector-search%E3%82%A4%E3%83%B3%E3%83%87%E3%83%83%E3%82%AF%E3%82%B9)をご覧ください。\n\n1. **Catalog Explorer** UIを開くために、左のサイドバーの**Catalog**をクリックします。\n1. 使用したいDeltaテーブルを選択します。\n1. 右上のケバブメニューから**Create vector search index**を選択します。\n![](https://docs.databricks.com/en/_images/create-index-button.png)\n1. **Create vector index**フォームに入力します。\n    1. インデックス名を指定します。\n    1. 主キーとして使用するカラムを選択します。\n    1. 使用したいエンべディングモデルをサービングしているモデルサービングエンドポイントを選択します。\n    1. Databricksがあなたのためにエンべディングを計算できるように**Compute embeddings**を選択します。\n        1. すでに計算されたエンべディングを持つDeltaテーブルがある場合には、**Use existing embedding column**を選択します。\n    1. **Embedding source column**を選択します。Databricksによって計算されるエンべディングでは、テキストのみがサポートされます。\n        1. **Use existing embedding column**を選択すると、任意のデータタイプをサポートするようにフィールドが**Embedding vector column**に変化します。\n    1. どの**Embedding model**を使用するのかを選択します。\n    1. 同期モードとして**Continuous**を選択します。`continuous`モードは数秒のレーテンシーでインデックスを同期し続けます。しかし、継続的な同期ストリーミングパイプラインを実行するために計算クラスターがプロビジョニングされるので、コストは高くなります。\n1. **Create**をクリックします。\n\n![](https://docs.databricks.com/en/_images/create-index-form.png)\n\nSDKを使う場合には、以下を使用することができます。このサンプルは、Databricksによって計算されるエンべディングを用いたDelta Sync Indexを作成します。\n\n```py\nindex = client.create_delta_sync_index(\n  endpoint_name=\"vector_search_demo_endpoint\",\n  source_table_name=\"vector_search_demo.vector_search.en_wiki\",\n  index_name=\"vector_search_demo.vector_search.en_wiki_index\",\n  pipeline_type='TRIGGERED',\n  primary_key=\"id\",\n  embedding_source_column=\"text\",\n  embedding_model_endpoint_name=\"e5-small-v2\"\n)\n```\n\nSDKを用いてのみ[Direct Vector Access Index](https://qiita.com/taka_yayoi/items/494b00f2c8fc60dcc466#direct-vector-access-index)を作成することができます。\n\n```py\nclient = VectorSearchClient()\nindex = client.create_direct_access_index(\n    endpoint_name=\"storage_endpoint\",\n    index_name=\"{catalog_name}.{schema_name}.{index_name}\",\n    primary_key=\"id\",\n    embedding_dimension=1024,\n    embedding_vector_column=\"text_vector\",\n    schema={\n     \"id\": \"int\",\n     \"field2\": \"str\",\n     \"field3\": \"float\",\n     \"text_vector\": \"array<float>\"}\n)\n```\n\n# Vector Searchエンドポイントのクエリー\n\nSDKを用いてのみVector Searchエンドポイントにクエリーを行うことができます。以下の例では以前のセクションで作成したDelta Sync Indexを使用しています。\n\n```py\nresults = index.similarity_search(\n  query_text=\"Greek myths\",\n  columns=[\"id\", \"text\"],\n  num_results=2\n)\n\nresults\n```\n\n# その他のリソース\n\n以下のノートブックでは、Vector Search Python SDKの使い方をデモンストレーションしています。\n\n## Python SDKを用いたVector Searchのノートブック\n\nhttps://docs.databricks.com/_extras/notebooks/source/generative-ai/vector-search-python-sdk-example.html\n\n## エンべディングモデル呼び出しのサンプルノートブック\n\n### Databricksモデルサービングを用いたAzure OpenAIエンべディング呼び出しのノートブック\n\nhttps://docs.databricks.com/_extras/notebooks/source/generative-ai/embedding-with-azure-openai.html\n\n### Databricksモデルサービングを用いたOpenAIエンべディング呼び出しのノートブック\n\nhttps://docs.databricks.com/_extras/notebooks/source/generative-ai/embedding-with-openai.html\n\n### OSSエンべディングモデルの登録およびサービングのノートブック\n\nhttps://docs.databricks.com/_extras/notebooks/source/generative-ai/embedding-with-oss-models.html\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksのVector Search", "text": "[Databricks Vector Search \\| Databricks on AWS](https://docs.databricks.com/en/generative-ai/vector-search.html) [2023/11/30時点]の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n:::note info\n**プレビュー**\n本機能は[パブリックプレビュー](https://docs.databricks.com/release-notes/release-types.html)です。\n:::\n\n本書では、DatabricksのベクトルデータベースソリューションであるDatabricks Vector Searchが何であるのかと動作原理の概要を説明します。\n\n# Databricks Vector Searchとは？\n\nDatabricks Vector Searchは、ベクトルデータベースでメタデータを含む皆様のデータのベクトル表現を格納できるサーバレスの類似検索エンジンです。Vector Searchによって、Unity Catalogで管理されるDeltaテーブルから自動で更新されるベクトル検索インデックスを作成しし、最も類似するベクトルを返却するためにシンプルなAPIでクエリーを行うことができます。\n\nVector Searchは以下のDatabricksの機能とインテグレーションされています:\n\n- [Deltaテーブル](https://docs.databricks.com/ja/introduction/delta-comparison.html)には、ベクトルデータベースの入力データが格納されます。Vector Searchによって、Deltaテーブルからベクトル検索インデックスを作成することができ、Deltaテーブルのコンテンツとそのインデックスが自動で同期されます。\n- [Unity Catalog](https://docs.databricks.com/ja/data-governance/unity-catalog/index.html)は、データガバナンスとアクセスコントロールリストを取り扱います。Unity Catalogによって、Vector Search APIと背後のデータベース両方へのアクセスを管理することができます。\n- [サーバレスコンピュート](https://docs.databricks.com/ja/serverless-compute/index.html)。Vector Searchはサーバレスコンピュートで稼働し、これはサーバーとインフラストラクチャの管理が、あなたのDatabricksアカウント内で行われることを意味します。\n- [モデルサービング](https://docs.databricks.com/ja/machine-learning/model-serving/index.html)は、エンべディング生成のためのモデルサービングへのクエリーを自動化します。\n\n以下のようなユースケースにVector Searchを適用することができます:\n\n- Retrieval augment generation (RAG)システム: Vector Searchによって、データの効率的な検索と拡張が可能となります。これによって、LLMアウトプットにおいてより正確なレスポンスに繋がり、幻覚のようなエラーを最小化します。\n- レコメンデーションシステム: eコマースやストリーミングプラットフォームにおいて、効率的な最近傍検索、ユーザー挙動と適切なアイテムのマッチングで活用されます。\n- 画像、動画認識: 画像や動画における類似特徴量のクイックな検索を促進します。\n- バイオインフォマティクス: DNAシーケンスのアライメントやプロテインの構造類似性検索のようなタスクに適用できます。\n- 異常検知: 不正検知やネットワークセキュリティのような文脈で外れ値を特定します。\n\n# Vector Searchの動作原理\n\nVector Searchは、あなたがエンべディングベクトルをどのように計算し、[Vector Searchインデックス](#vector-searchインデックス)をどのように管理したいのかによって挙動が異なります。\n\nハイレベルでは、Vector Searchを以下のように取り扱うことができます:\n\n1. Vector Searchエンドポイントを作成します。このエンドポイントは、Vector Searchインデックスの作成、アクセスに使用されます。REST APIあるいは**Compute** UIからVector Searchエンドポイントを作成することができます。それぞれのエンドポイントは、インデックスにあるベクトルの数と同時リクエスト数に応じて自動でスケールします。\n1. Vector Searchインデックスを作成します。このインデックスは、皆様のチャンク化されたデータを含むDeltaテーブルの特定のカラムをベースに作成されます。サポートされるインデックスのタイプの詳細については、[Vector Searchインデックス](#vector-searchインデックス)をご覧ください。\n1. Vector Searchは入力としてデータのチャンクを取り込み、それらをエンべディングベクトルとして知られる、数のシーケンスによって表現されるN次元の空間にマッピングします。\n1. Vector Searchは、類似するコンセプトが異なるコンセプトよりも近くにマッピングされるように、プロパティと共にこのエンべディングベクトルを活用します。例えば、りんごとオレンジは、りんごとボートよりも互いに近く配置されます。\n1. Vector Searchは、類似するベクトルを特定できるように高速でスケーラブルな近傍検索アルゴリズムを提供するベクトルデータベースにこれらのベクトルを格納します。\n1. お使いのVector Searchエンドポイントを用いてベクトルデータベースにクエリーを実行します。\n\n# 要件\n\n- Unity Catalogが有効化されたワークスペース。\n- サーバレスコンピュートの有効化。\n- ソーステーブルで`Change Data Feed`が有効化されていること。\n\n# Vector Searchインデックス\n\nVector Searchインデックスは、リアルタイムかつ効率的な近似最近傍検索クエリーをサポートする、あなたのデータのエンべディングベクトル表現となります。\n\nVector Searchでは、ソースのDeltaテーブルの特定のカラムからエンべディングのインデックスが作成されます。Vector Searchインデックスは、REST APIやSDKを用いてクエリー、更新に使用できるVector Searchエンドポイントによってサービングされます。Vector Searchエンドポイントには複数のインデックスを含めることができます。\n\nVector Searchインデックスは、Unity Catalog内のテーブルのようなオブジェクトとして表示されます。Vectorインデックスへのアクセスや、クエリー、更新、作成のようなインデックスレベルの権限は、Unity Catalogによって制御されます。\n\nVector Searchは2つのインデックスタイプをサポートしています:\n\n- **Delta Sync Index**はソースのDeltaテーブルと自動で同期し、Deltaテーブルの背後のデータが変更するとインクリメントタルに更新を行います。Databricksにエンべディングベクトルを管理させるか、ご自身で管理するのかを選択することができます。\n- **Direct Vector Access Index**は、REST APIやSDKを通じてエンベデディングベクトルとメタデータを直接読み書きすることができます。このインデックスでは、ご自身でエンべディングベクトルとインデックスの更新を管理します。\n\n以下のセクションでは、これらのインデックスそれぞれのシナリオを通じて、どのようにVector Searchが動作するのかをウォークスルーします。\n\n## マネージドエンベディングを用いたDelta Sync Index\n\nエンべディングの自動生成とDatabricksマネージドのエンべディングでは、Delta Sync Indexを作成します。\n\nこのシナリオでは、\n\n1. あなたのチャンク化されたデータを格納するDeltaテーブルを指定します。\n1. お好きなエンべディングモデルをサービングするモデルサービングエンドポイントを指定します。OpenAIやオープンソースモデル、ご自身でトレーニングしたカスタムモデルのように、あなたの要件に適した任意のエンべディングモデルを選択することができます。\n1. Deltaテーブルのデータからエンべディングを生成するために、Vector Searchはこのエンべディングモデルを活用します。\n1. Deltaテーブルのソースデータが更新されると、ソースのDeltaテーブルで`Change Data Feed`が有効化されている限り、Vector Searchはベクトルインデックスを更新します。\n1. 2つのアップデートモードがサポートされています: `triggered`と`continuous`です。`continuous`モードは数秒のレーテンシーでインデックスを同期し続けます。しかし、連続的な同期ストリーミングパイプラインを実行するために計算資源がプロビジョンされるので、コストが高くなります。\n1. Delta Sync APIを用いて、インデックスとベクトルデータベースにクエリーを行うことができます。\n\n## セルフマネージドのエンべディングを用いたDelta Sync Index\n\nご自身でDelta Sync Index を作成し、エンべディングを計算することもできます。この選択肢は、すでに事前生成したエンべディングを持っており、転送学習やエンべディングの再ランキングのように外部アクセスを必要とするお客様に適用されます。\n\nこのシナリオでは、\n\n1. Deltaテーブルに格納されているエンべディングベクトルを指定します。\n1. Vector SearchはソースのDeltaテーブルで指定したカラムをベースにインデックスを作成します。\n1. Deltaテーブルのソースデータが更新されると、ソースのDeltaテーブルで`Change Data Feed`が有効化されている限り、Vector Searchはベクトルインデックスを更新します。\n1. 2つのアップデートモードがサポートされています: `triggered`と`continuous`です。`continuous`モードは数秒のレーテンシーでインデックスを同期し続けます。しかし、連続的な同期ストリーミングパイプラインを実行するために計算資源がプロビジョンされるので、コストが高くなります。\n1. Delta Sync APIを用いて、インデックスとベクトルデータベースにクエリーを行うことができます。\n\n## Direct Vector Access Index\n\nDatabricks外に既存のパイプラインがある、あるいは、すでにベクトルデータベースを使用している場合には、Direct Vector Access Indexを作成することができます。\n\nこのシナリオでは、\n\n1. Deltaテーブルに格納されているエンべディングベクトルを指定します。\n1. Vector SearchはソースのDeltaテーブルで指定したカラムをベースにインデックスを作成します。\n1. インデックスは自動でスケールし、リアルタイムのパフォーマンスを提供しますが、ソースデータのインデックスに対する更新の管理はユーザーが行います。\n1.  Delta Sync APIを用いて、インデックスとベクトルデータベースにクエリーを行うことができます。\n\n# Vector Searchの活用\n\nVector SearchエンドポイントやVector SearchインデックスのようなVector Searchのコンポーネントは、UI、Python SDK、REST APIを用いて作成、管理することができます。最もシンプルに行うにはUIを用いることをお勧めします。[Vector Searchインデックスの作成とクエリー](https://docs.databricks.com/ja/generative-ai/create-query-vector-search.html)をご覧ください。\n\n以下の表では、サポートされているVector Searchインデックスの操作で利用できるAPIを列挙しています。\n\n|  | 取り込みパイプライン | エンべディングの計算 | 説明 |\n|:--|:--|:--|:--|\n|  [マネージドエンベディングを用いたDelta Sync Index](#マネージドエンベディングを用いたdelta-sync-index) | 自動 & マネージド  | モデルサービング経由のマネージド  | DatabricksはDeltaテーブルのデータのベクトルを生成するために、ユーザー提供のエンべディングモデルを使用します。テキストのみがサポートされています。  |\n| [セルフマネージドのエンべディングを用いたDelta Sync Index](#セルフマネージドのエンべディングを用いたdelta-sync-index)  |  自動 & マネージド | 自分で管理  | 入力としてユーザーがdoubleあるいはfload配列としてのエンべディングを持つDeltaテーブルのカラムを参照することで、エンべディングを提供します。  |\n| [Direct Vector Access Index](#direct-vector-access-index)  |  手動 & 自分で管理 | 自分で管理  |  ユーザーはベクトルインデックスのインサート、削除、アップデート操作を行います。すべてのデータがサポートされます。SDKを用いてのみ、これらのインデックスタイプを作成できます。 |\n\n# 制限\n\n- [PrivateLink](https://docs.databricks.com/ja/administration-guide/cloud-configurations/aws/privatelink.html)や[IPアクセスリスト](https://docs.databricks.com/ja/security/network/ip-access-list.html)を使用しているワークスペースではVector Searchはサポートされていません。\n- パブリッププレビュー中はCustomer Managed Keys (CMK)はサポートされません。\n- 制限のかかっているワークスペースはサポートされておらず、この機能はHIPAA準拠していません。\n- 行列レベルの権限管理はサポートされていません。しかし、多くのお客様ではfilter APIを用いてご自身のアプリケーションレベルのACLを実装しています。\n- この機能は以下のリージョンでのみ利用可能です:\n    - ap-southeast-2\n    - eu-west-1\n    - us-east-1\n    - us-east-2\n    - us-west-2\n\n# その他のリソース\n\n- [Vector Searchインデックスの作成とクエリー](https://docs.databricks.com/ja/generative-ai/create-query-vector-search.html)\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "[2023年12月版] Databricksとは何か？", "text": "Databricksの弥生です。Databricks Japanにjoinして3年が経ちました。\n\n2021年には以下の記事を訳したり、\n\nhttps://qiita.com/taka_yayoi/items/438f762126f57868aa35\n\n自分でもJupyter notebookとの比較記事を書いたりしました。\n\nhttps://qiita.com/taka_yayoi/items/d5ea3dede05a180091b2\n\nそして、他のソリューションアーキテクトの方によっても記事が書かれています。\n\nhttps://qiita.com/kohei-arai/items/83a8a54880defbf35fa0\n\nしかし、Databricksは日進月歩で進化しています。2週間周期で新機能がリリースされるので、これらの記事も古くなってしまっている部分が少なからずあります。\n\nと言うことで、2023年12月時点での「Databrikcsとは何か」を記事にします。\n\n# Databricksとは何か？\n\nまず、会社としてのDatabricksの説明をさせてください。同名のプラットフォームを提供している企業で、2013年に創業されました。今年でちょうど10年です。由来はユニークで、カルフォルニア大学バークレー校に在学していた大学院生と教授たちとで創業された会社です。そして、彼らは[Apache Spark](https://www.databricks.com/jp/spark/about)、[Delta Lake](https://www.databricks.com/jp/product/delta-lake-on-databricks)、[MLflow](https://www.databricks.com/jp/product/managed-mlflow)のオリジナルクリエーターです。このような背景から、企業全体としてオープンソースにコミットしています。\n![Screenshot 2023-12-02 at 8.42.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/70d57f67-7732-d690-1bfb-d87da7acc235.png)\n\n現在ではグローバルで従業員数は6000人を超えています。私が入社した3年前は1500人程度だったので成長著しいと言えます。未上場ですが先日[シリーズIの投資](https://xtech.nikkei.com/atcl/nxt/column/18/00001/08410/)を受けました。\n\n# レイクハウスとは何か？\n\n2020年まではプラットフォームとしてのDatabricksの別名は「Unified Analytics Platform」でしたが、この時点で「[(データ)レイクハウスプラットフォーム](https://www.databricks.com/jp/product/data-lakehouse)」という名前になりました。レイクハウスの由来は**データレイク + データウェアハウス**です。両者のいいところどりをしていると言う意味からこのような名前となりました。名前を作り出したのはDatabricksですが、今では[他社様](https://learn.microsoft.com/ja-jp/fabric/data-engineering/lakehouse-overview)も使うようになって認知が進んだと言えます。\n![](https://www.databricks.com/en-website-assets/static/b1cec6ed1d8bb95ac0386c146b61e1ae/11d44/unified.webp)\n\nデータウェアハウス、データレイク、データレイクハウスの説明、違いについてはこちらにまとめています。\n\nhttps://qiita.com/taka_yayoi/items/1ebbce9b76fb923d5e21\n\n非常にざっくり言うと、\n\n- **データウェハウス**: 構造化データの取り扱いが得意でガバナンスも効いており、処理が高速。しかし、高価で専用フォーマットが必要。機械学習には適していない。\n- **データレイク**: クラウドストレージとオープンフォーマットで安価に構造化データも非構造化データも格納でき、機械学習にも対応可能。しかし、ガバナンスが効かず読み書き性能が出ない。\n- **データレイクハウス** データの格納場所はデータレイクなので、安価かつオープンで任意のタイプのデータを取り扱える。かつ、ガバナンスも効いており処理も高速。もちろん、機械学習でも活用可能。\n\nまた、近年においてデータ&AIを取り組む際には、機械学習、データウェアハウス、データレイク、BI、ストリーミング処理、オーケストレーションなど様々なツールを組み合わせる必要があります。これによってシステムの複雑さが増加している中、今年からは**生成AI**という新たな技術、それによる新たなユースケースへの取り組みも増えてきています。\n![Screenshot 2023-12-02 at 9.03.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a5c10a7b-7e52-03ef-ca09-521ef2e8fa3b.png)\n\n結果として、以下のような課題がさらに深刻なものとなっています。\n![Screenshot 2023-12-02 at 9.03.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/75f4dc38-e71a-4ce9-f286-e25e87df1c67.png)\n\n- **サイロ化されたデータ、AI、ガバナンス**\n    - データの格納場所だけではなく、大規模言語モデルを含む各種機械学習モデルが散在することで、管理コストを引き上げ、イノベーションの加速を阻害します。\n- **AIがもたらすデータ、プライバシー&コントロールの課題**\n    - 特に生成AIは幻覚やバイアスなどの新たな課題を我々に突きつけるようになりました。我々は生成AIに対する新たなコントロールの手段を必要としています。\n- **高度に技術的なスタッフへの依存**\n    - 生成AI以前においても、複雑な処理を行うためにはSQLやPythonの習熟者の協力が必須となっており、クイックな課題解決が困難でした。\n\n2020年にリリースされたデータレイクハウスは主に最初の課題を解決するために提供されています。[Delta Lake](https://www.databricks.com/jp/product/delta-lake-on-databricks)と[Unity Catalog](https://qiita.com/taka_yayoi/items/530c922528e4269120bf)によってデータやAIのサイロを排除し、データとAIに対する包括的なガバナンスを実現することで、様々なデータ&AIワークロードを円滑に進めることができます。この結果、現在ではグローバル企業の74%がレイクハウスを導入しています。\n![Screenshot 2023-12-02 at 9.07.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9c4dbe91-3343-766b-6def-3d25e196d239.png)\n\nそれでは、その他の2つの課題についてはどうしたらいいのでしょうか？\n\n# データインテリジェンスプラットフォームとは何か？\n\nこのため、我々は生成AIを用いてデータレイクハウスを**データインテリジェンスプラットフォーム**に進化させます。\n![Screenshot 2023-12-02 at 9.10.05.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e0fe095b-faac-a311-3c6c-c89a862c99f0.png)\n\nデータインテリジェンスプラットフォームでは、レイクハウスに新たに**データインテリジェンスエンジン**を付け加えます。\n![Screenshot 2023-12-02 at 9.10.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5757674f-7278-36f6-7ca0-2cbea30a6584.png)\n\nこれによって、Databricksの各コンポーネントも以下のように進化します。\n![Screenshot 2023-12-02 at 9.13.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8e1ae931-adc9-afb9-78aa-eadf961dd94f.png)\n\n- Delta Lake: 利用パターンに基づき自動でデータレイアウトを最適化\n- Unity Catalog: [自然言語でセキュアに洞察を取得](https://qiita.com/taka_yayoi/items/e93ac9b0d8af9209defd)\n- Databricks AI(旧Lakehouse AI): カスタムLLMの作成チューニング、提供\n- Delta Live Tables: データ品質の自動化\n- Workflows: 過去の処理に基づくジョブコストの最適化\n- Databricks SQL: テキストからSQL\n\n特に、Databricks AIを活用することで、RAGなどの生成AIアプリケーションを容易に構築・運用できるようになります。\n![Screenshot 2023-12-02 at 9.15.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e25e34c3-dc81-4dce-54d7-751b31f7565d.png)\n\nまた、これ以外にも**Project Genie**というプロジェクトも進行中です。これによって、PythonやSQLを知らない方でも自然言語で容易にデータを操作できるようになります。\n![Screenshot 2023-12-02 at 9.17.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2876cae9-2d09-73d2-72aa-23e5199a3c5c.png)\n\nDatabricksのデータインテリジェンスプラットフォームによって、皆様に**シンプル**、**インテリジェント**、**プライベート**というメリットを提供することになります。\n\n- **シンプル**: 自然言語が使いやすさと効率性を全員に提供\n- **インテリジェント**: あなたのデータを理解するためにE2EでAIがインテグレーション\n- **プライベート**: プライベートデータを用いて容易にカスタムモデルを構築\n\n# Databricksの提供機能\n\n上述したデータインテリジェンスプラットフォームの機能の一部はプライベートプレビュー、パブリックプレビューですが、順次リリースされていきます。こちらですべての機能をカバーすることはできませんが、プレビュー中のものを含めていくつかのDatabricksの機能を説明します。\n\n## Databricksワークスペース\n\nブラウザからアクセスする作業場です。こちらでノートブックやクラスターを用いて様々な処理を実装します。ちなみに3年前は[日本語対応](https://qiita.com/taka_yayoi/items/ff4127e0d632f5e02603)してませんでした。今でも我々が日々翻訳しています！\n![Screenshot 2023-12-02 at 9.25.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/707321bb-625c-8eeb-33f6-f7b05eab5133.png)\n\nhttps://docs.databricks.com/ja/workspace/index.html\n\n## Databricksクラスター\n\nDatabricksにおける計算資源です。ちなみに、Databricksの課金はこのクラスターのスペックと稼働時間によって決まります。つまり、クラスターが停止していればDatabricksのお金は発生しません。なお、背後のクラウドサービスのお金は発生します。\n![Screenshot 2023-12-02 at 9.34.06.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/43a60dde-5e1d-a114-db4a-f8397c7f7fb0.png)\n\n以下のような機能を提供しています。背後ではApache Sparkが動作しているので、大量データでも水平分散させることで高速な処理を実現できます。\n\n- [Databricksランタイム](https://qiita.com/taka_yayoi/items/824b507019d3ade7eedc)(データ&AIの取り組みでよく使うソフトウェアを事前インストールするソフトウェアパッケージ)\n- 追加[ライブラリ](https://qiita.com/taka_yayoi/items/66cec8cf4c159666c4e0)のインストール\n- [initスクリプトの実行](https://qiita.com/taka_yayoi/items/88cb46be00a175085f87)\n- シェル実行のためのWebターミナル\n- [クラスターポリシー](https://qiita.com/taka_yayoi/items/93d7597d7fc739b48bb7)(ユーザーの設定範囲をコントロール)\n- GPUクラスターの構築\n- オートスケーリング\n- 自動停止\n\nhttps://docs.databricks.com/ja/clusters/index.html\n\nまた、[サーバレス](https://docs.databricks.com/ja/sql/admin/serverless.html)の機能も順次利用できるようになっています。起動がとても高速です！\n\n## Databricksノートブック\n\nDatabricksで最も利用することになるインタフェースです。ノートブック形式ですので、Juypterノートブックに慣れている方であればすぐに使い始められます。\n![Screenshot 2023-12-02 at 9.35.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/cc3602cd-dcd6-5b4f-f948-455dbd648d7a.png)\n\n以下のような便利機能がてんこ盛りです。\n\n- アクセスコントロール\n- 同時参照、同時編集\n- バージョン管理\n- 複数言語をサポート(Python、SQL、R、Scalaなど)、一つのノートブック内で言語を切り替えることも可能\n- 結果からクリック操作でグラフを作成\n- [Databricksアシスタント(生成AIアシスタント)](https://qiita.com/taka_yayoi/items/db3e9d976f9f7dbec86e)\n- MLflowとのインテグレーション(モデルをトレーニングすると自動で記録されます)\n\nhttps://docs.databricks.com/ja/notebooks/notebook-ui.html\n\n## Databricks AI\n\n従来の機械学習モデルだけではなく、生成AIモデルの開発、運用、管理を容易にするためのフレームワークです。\n\n- [Vector Search](https://docs.databricks.com/ja/generative-ai/vector-search.html)\n- [AI Functions](https://qiita.com/taka_yayoi/items/e0650e87dc33ded094e7)(SQLからLLMを呼び出せます)\n- キュレーテッドモデル\n- ファインチューニング\n- モデルサービング(GPUも使えます)\n- LLMのAutoML\n- 特徴量ストア\n\nAI Functionsを用いることで、SQLの関数にLLMを埋め込むことができます。\n![Screenshot 2023-12-02 at 10.24.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6b13dbc2-76e3-1090-77ff-69bedfb8111a.png)\n\nモデルサービングを用いることで、LLMを含む機械学習モデルに対してREST APIを受け付けるエンドポイントを構築できます。\n![Screenshot 2023-12-02 at 10.41.30.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e4511f9e-0c00-2e35-834b-59f8b038ed1d.png)\n\nGUIと組み合わせることで独自のRAGアプリケーションを構築できます。(こちらの画面は別マシンでホストしているstreamlitです)\n![Screenshot 2023-12-02 at 10.41.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5c2ed964-9279-6de7-b644-359c1d8c1da0.png)\n\nこちらの機能はプレビューのものがありますので、続報に期待していてください！\n\nhttps://docs.databricks.com/ja/generative-ai/generative-ai.html\n\n## Unity Catalog\n\nDatabricksにおける各種資産に対するガバナンス管理を一手に引き受けているのがUnity Catalogです。\n\n以下のような機能を提供しています。これも3年前はなかったので嬉しい限りです。\n\n- テーブル、データベース、ファイル、機械学習モデルなどに対するアクセスコントロール\n- 監査ログ\n- リネージ(依存関係)の自動捕捉\n- 洞察の自動抽出\n- 生成AIによる自動コメント作成\n\nカタログエクスプローラという機能で、データベース、テーブル、ファイル、機械学習モデルなどを管理できます。\n![Screenshot 2023-12-02 at 9.41.01.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a9c9bdbe-154b-6bcb-a632-0018163b203d.png)\n\n洞察タブにアクセスするとテーブルがどのように使われているのか、誰がよくアクセスしているのかという情報にアクセスできるので、データ活用が捗ります。\n![Screenshot 2023-12-02 at 9.41.15.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/506ca85d-804f-3139-d22c-97d16b53ab3f.png)\n\n強力なリネージ機能、テーブル間の依存関係だけではなく、カラム間の依存関係やファイル、ノートブック、ダッシュボードなどとの依存関係もすべて自動でキャプチャされます。\n![Screenshot 2023-12-02 at 9.41.28.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5f6fcd88-71ae-e02e-77ab-d7e5f00f991b.png)\n\nhttps://docs.databricks.com/ja/data-governance/unity-catalog/index.html\n\n今後の新機能はすべてUnity Catalogを前提としていると言っても過言ではありません。是非ご活用ください！\n\n## Databricksワークフロー\n\n開発した処理を夜間バッチなどで定期運用するというのは、典型的なユースケースといえます。DatabricksマネージドのオーケストレーションツールがDatabricksワークフローです。\n\nノートブックはもちろん、データパイプラインを実装したDelta Live Tablesパイプライン、Databricks SQLのダッシュボードなど、様々なオブジェクトを組み合わせたジョブを簡単に実装することができます。条件分岐、エラー時の通知、リトライ機能なども搭載しています。\n![Screenshot 2023-12-02 at 9.46.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6e33009c-1701-06d0-2bc8-0858c67f3bc9.png)\n![Screenshot 2023-12-02 at 9.46.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8a4665f2-b357-6a54-670e-7bcad70a82fe.png)\n\nhttps://docs.databricks.com/ja/workflows/index.html\n\nhttps://qiita.com/taka_yayoi/items/e881769270f0ec0b7d06\n\n## Databricks SQL\n\nDatabricksにビルトインされているBI・データウェアハウス機能がDatabricks SQLです。簡単なBIダッシュボードであれば、Databricks上で簡単に作成できます。\n![Screenshot 2023-12-02 at 9.50.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ef1b9d42-9206-e8d7-835d-ea2cc034cdeb.png)\n\n特に最近リリースされた[Lakeview](https://qiita.com/taka_yayoi/items/32afd1566b5bc3bedbec)を用いると、完全にノーコードでダッシュボードを作成することができます。\n![Screenshot 2023-12-02 at 9.51.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a5752532-8b74-cb5a-7165-3ec40169f7f3.png)\n\nhttps://docs.databricks.com/ja/sql/index.html\n\n# まとめ\n\nDatabricksは、これまでもレイクハウスプラットフォームとしてデータやAIのサイロ化、ガバナンスの不在などに関する課題を解決してきています。今回紹介した内容は、よりインテリジェントに処理を行えるようにし、企業における生成AIの活用を促進し、様々な方がデータ&AIを活用できるようにするために、生成AIのパワーを活用することでレイクハウスからデータインテリジェンスプラットフォームに進化しているのだと私自身は理解しています。これは、以前から掲げている理念である**データとAIの民主化**に即したものだと思います。Databricksの[データインテリジェンスプラットフォーム](https://www.databricks.com/jp/blog/what-is-a-data-intelligence-platform)の今後にご期待ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksのPandas UDFによるtransformerモデルの分散処理", "text": "こちらのセクションをウォークスルーします。\n\nhttps://docs.databricks.com/ja/machine-learning/train-model/huggingface/model-inference-nlp.html#use-pandas-udfs-to-distribute-model-computation-on-a-spark-cluster\n\n[Pandas UDF](https://qiita.com/taka_yayoi/items/b65197128ee698d87910)を用いることで、pandas.Seriesやpandas.DataFrameに対する処理を記述することができ、さらにこれをSparkで並列処理することができます。この仕組みはTransformerモデルにも適用できます。\n\n以下の例ではTransformersのpipelineを用いて、[英語からフランス語への翻訳を行うパイプライン](https://huggingface.co/tasks/translation)を作成しています。\n\n```py\nimport pandas as pd\nfrom transformers import pipeline\nimport torch\nfrom pyspark.sql.functions import pandas_udf\n\ndevice = 0 if torch.cuda.is_available() else -1\ntranslation_pipeline = pipeline(task=\"translation_en_to_fr\", model=\"t5-base\", device=device)\n\n@pandas_udf('string')\ndef translation_udf(texts: pd.Series) -> pd.Series:\n  translations = [result['translation_text'] for result in translation_pipeline(texts.to_list(), batch_size=1)]\n  return pd.Series(translations)\n```\n\n```py\ntexts = [\"Hugging Face is a French company based in New York City.\", \"Databricks is based in San Francisco.\"]\ndf = spark.createDataFrame(pd.DataFrame(texts, columns=[\"texts\"]))\ndisplay(df.select(df.texts, translation_udf(df.texts).alias('translation')))\n```\n\n翻訳されました。今回は2レコードだけでしたが、レコード数増えてもSparkの並列処理の恩恵を受けられると言うことですね。バッチで大量のドキュメントを翻訳すると言うユースケースで効果が出そうです。\n![Screenshot 2023-12-01 at 17.40.51.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/268ce40c-f746-73fa-dbc6-3c2271f5050c.png)\n\nさらには別のモデルとタスクを指定することで、NER(Named Entity Recognition: 固有表現抽出)を行うこともできます。\n\n```py\nfrom transformers import pipeline\nimport torch\ndevice = 0 if torch.cuda.is_available() else -1\nner_pipeline = pipeline(task=\"ner\", model=\"Davlan/bert-base-multilingual-cased-ner-hrl\", aggregation_strategy=\"simple\", device=device)\nner_pipeline(texts)\n```\n\n```\n[[{'entity_group': 'ORG',\n   'score': 0.99933606,\n   'word': 'Hugging Face',\n   'start': 0,\n   'end': 12},\n  {'entity_group': 'LOC',\n   'score': 0.99967843,\n   'word': 'New York City',\n   'start': 42,\n   'end': 55}],\n [{'entity_group': 'ORG',\n   'score': 0.9996372,\n   'word': 'Databricks',\n   'start': 0,\n   'end': 10},\n  {'entity_group': 'LOC',\n   'score': 0.999588,\n   'word': 'San Francisco',\n   'start': 23,\n   'end': 36}]]\n```\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "LangChainのSparkデータフレームエージェント", "text": "今度はこちらのノートブックをウォークスルーします。\n\nhttps://docs.databricks.com/ja/large-language-models/langchain.html#spark-dataframe-agent\n\n以下のコードでは、Sparkデータフレームエージェントのサンプルを説明します。\n\n## 要件\n\n- このノートブックを使うには、お使いのOpenAI APIトークンを指定してください。\n- Databricks Runtime 13.3 ML以降\n\n# ライブラリのインストール\n\n最新の`langchain`と`databricks-sql-connector`を使うことをお勧めします。\n\n:::note\n**注意**\nもとのノートブックのままではエラーとなるので`langchain_experimental`を追加します。\n:::\n\n```py\n%pip install --upgrade langchain databricks-sql-connector langchain_experimental\ndbutils.library.restartPython()\n```\n\n```py\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"<OpenAI APIキー>\"\n```\n\n# Sparkデータフレームエージェント\n\nこのSparkデータフレームエージェントによって、必要に応じてSparkデータフレームを操作できるようになります。シンプルに、LLMと対象のデータフレームを指定して`create_spark_dataframe_agent` をコールするだけです。\n\n```py\nfrom langchain.llms import OpenAI\n#from langchain.agents import create_spark_dataframe_agent\nfrom langchain_experimental.agents.agent_toolkits import create_spark_dataframe_agent\n\ndf = spark.read.csv(\"/databricks-datasets/COVID/coronavirusdataset/Region.csv\", header=True, inferSchema=True)\ndisplay(df)\n```\n![Screenshot 2023-11-30 at 17.15.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/af98a165-d202-24df-3a3a-b737d6f34269.png)\n\n```py\nagent = create_spark_dataframe_agent(llm=OpenAI(temperature=0), df=df, verbose=True)\n```\n\n問い合わせます。\n\n```py\nagent.run(\"How many rows are there?\")\n```\n```\n> Entering new AgentExecutor chain...\nThought: I need to find out how many rows are in the dataframe\nAction: python_repl_ast\nAction Input: df.count()\nObservation: 244\nThought: I now know the final answer\nFinal Answer: There are 244 rows in the dataframe.\n\n> Finished chain.\n'There are 244 rows in the dataframe.'\n```\n\nOpenAIなので日本語でもOKです。\n\n```py\nagent.run(\"何行ありますか？\")\n```\n```\n> Entering new AgentExecutor chain...\nThought: I need to know how many rows are in the dataframe\nAction: python_repl_ast\nAction Input: df.count()\nObservation: 244\nThought: I now know the final answer\nFinal Answer: データフレームには244行があります。\n\n> Finished chain.\n'データフレームには244行があります。'\n```\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "LangChainのPySparkデータフレームローダーとMLflowの活用", "text": "こちらの**PySpark DataFrame loader and MLFlow in Langchain ノートブック**をウォークスルーします。\n\nhttps://docs.databricks.com/ja/large-language-models/langchain.html#load-data-with-the-pyspark-dataframe-loader\n\nこのノートブックでは、PySparkとLangchainのインテグレーションを説明し、以下の方法を説明します:\n\n1. PySparkデータフレームからのLangchainドキュメントローダーの作成\n1. ドキュメントローダーを用いたLangchain RetrievalQAインスタンスの作成\n1. サンプルのRetrievalQAを保存するためにMLflowを活用\n\n### 要件\n\n- Databricks Runtime 13.3 ML以降\n- MLflow 2.5以降\n\n# インポート\n\n```py\n%pip install --upgrade langchain faiss-cpu mlflow\n\n# GPUクラスターでは以下を使います \n# %pip install --upgrade langchain faiss-gpu mlflow\n```\n\n```py\ndbutils.library.restartPython()\n```\n\n# PySparkベースのドキュメントロードによるRetrievalQAチェーンの作成\n\n`/databricks-datasets/`内のWikipediaデータセットを使いましょう。以下のセルにお使いのOpenAIのAPIキーを追加してください。\n\n```py\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"OpenAI APIキー\"\n```\n\n```py\nnumber_of_articles = 20\n\nwikipedia_dataframe = spark.read.parquet(\"databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet/*\").limit(number_of_articles)\ndisplay(wikipedia_dataframe)\n```\n![Screenshot 2023-11-30 at 15.29.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e8d5db67-ce3c-3d3a-0607-aa71e8850275.png)\n\n以下の行が、PySparkデータフレームからLangchainにデータをロードするために必要なことの全てです。\n\n```py\nfrom langchain.document_loaders import PySparkDataFrameLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = PySparkDataFrameLoader(spark, wikipedia_dataframe, page_content_column=\"text\")\ndocuments = loader.load()\n\n# 再帰的にテキストを分割\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\nprint(f\"Number of documents: {len(texts)}\")\n```\n```\nNumber of documents: 421\n```\n\n# HuggingfaceEmbeddingsを用いたFAISSベクトルストアの作成\n\nFAISSベクトルストアは、MLflowでモデルを記録できるようにするための中間ステップとなります。\n\n```py\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\n\nembeddings = OpenAIEmbeddings()\ndb = FAISS.from_documents(texts, embeddings)\n```\n\n# RetrievalQAチェーンの作成\n\n```py\nfrom langchain.chains import RetrievalQA\nfrom langchain import OpenAI\n\nretrieval_qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=db.as_retriever())\n```\n\n# RetrievalQAチェーンへのクエリー\n\n```py\nquery = \"Who is Harrison Schmitt\"\nresult = retrieval_qa({\"query\": query})\nprint(\"Result:\", result[\"result\"])\n```\n```\nResult:  Harrison Hagan \"Jack\" Schmitt is an American geologist, retired NASA astronaut, university professor and former U.S. Senator from New Mexico. In 1972, he was a member of the crew on board Apollo 17 and became the twelfth person to set foot on the Moon. He resigned from NASA in 1975 to run for election to the United States Senate.\n```\n\n# Mlflowによるチェーンのロギング\n\n```py\nimport mlflow\n\npersist_directory = \"langchain/faiss_index\"\ndb.save_local(persist_directory)\n\ndef load_retriever(persist_directory):\n  embeddings = OpenAIEmbeddings()\n  db = FAISS.load_local(persist_directory, embeddings)\n  return db.as_retriever()\n\n# RetrievalQAチェーンの記録\nwith mlflow.start_run() as mlflow_run:\n  logged_model = mlflow.langchain.log_model(\n    retrieval_qa,\n    \"retrieval_qa_chain\",\n    loader_fn=load_retriever,\n    persist_dir=persist_directory,\n  )\n```\n\n記録されました。\n![Screenshot 2023-11-30 at 15.32.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b5920b37-adbc-3cae-849b-cf9876046510.png)\n\n# MLFlowを用いたチェーンのロード\n\n他の機械学習モデルと同じようにロードできます。\n\n```py\nmodel_uri = f\"runs:/{ mlflow_run.info.run_id }/retrieval_qa_chain\"\n\nloaded_pyfunc_model = mlflow.pyfunc.load_model(model_uri)\nlangchain_input = {\"query\": \"Who is Harrison Schmitt\"}\nloaded_pyfunc_model.predict([langchain_input])\n```\n```\n[' Harrison Schmitt is an American geologist, retired NASA astronaut, university professor and former U.S. senator from New Mexico. He was the 12th person to set foot on the Moon and the second-to-last person to step off of the Moon.']\n```\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "データブリックスのUnity Catalogで実現する真のデータガバナンス", "text": "こちらのサマリーです。以下のリンクからウェビナーの録画を参照できます。\n\nhttps://events.databricks.com/20231027-unitycatalog-jp\n\n# ガバナンスとは？\n\n今ではガバナンスはデータのみに適用されるものではありません。\n\n- **データガバナンス**とは、企業のデータ資産をそのライフサイクルを通じて管理するための**原則、プラクティス、ツールから構成される包括的なアプローチ**のことです。\n- ビジネス戦略とデータに関連する要件をアラインすることで、データガバナンスは\n企業に対して**優れたデータ管理、品質、可視性、セキュリティ、コンプライアンス**の\n能力を提供します。\n- 効果的なデータガバナンス戦略によって、企業は許可されないアクセスから自身のデータを保護し、規制の要件によるコンプライアンスを保証しつつも、意思決定に\n容易にデータを活用できるようになります。\n\nデータとAIのガバナンスがビジネス価値を生み出します。\n![Screenshot 2023-11-29 at 21.22.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b8efee0b-57cd-8240-5e60-0a55accbc633.png)\n\nしかし、データとAIのガバナンスは複雑なものとなっています。\n![Screenshot 2023-11-29 at 21.23.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/60dccf4c-f405-3579-11e9-8407558694c2.png)\n![Screenshot 2023-11-29 at 21.24.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/53bc83dd-0513-9a7e-6cbe-045cecdcc078.png)\n\n企業はデータとAIに対する**統合的なアプローチ**を必要としています。\n![Screenshot 2023-11-29 at 21.24.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/00ede62a-078e-7711-e418-4b5489ec7cb1.png)\n\n# Databricksにおけるガバナンスの実現\n\nDatabricksレイクハウスはデータとAIのガバナンスを統合します。\n![Screenshot 2023-11-29 at 21.24.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b9a99529-94eb-f9d8-913d-1eb45201b348.png)\n\n[DatabricksのUnity Catalog](https://www.databricks.com/jp/product/unity-catalog)はデータ & AIに対する統合ガバナンスを提供します。\n\n- データとAIに対する統合ビュー\n- データとAIに対するシンプルな権限モデル\n- AIで強化された監視と観測可能性\n- オープンなデータ共有\n\n![Screenshot 2023-11-29 at 21.25.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6d330b63-3005-6c1c-0ff9-e9b398589627.png)\n\nUnity Catalogがお客様のデータの価値を解放します。\n![Screenshot 2023-11-29 at 21.26.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5dd5c2f3-55f4-8357-5c32-dead58ad3c49.png)\n\nUnity CatalogはデータとAIの統合ビューを提供します。\n\n- 構造化データ、非構造化データ、ファイル、ノートブック、MLモデル、ダッシュボードを一つの場所から**ディスカバリー・分類**\n- データを移動、コピーすることなしに、**単一のアクセスポイント**を用いて**他のデータベースやデータウェアハウス**からのデータを統合、クエリー\n- 自動化リネージ、タグ、自動生成されるデータ洞察を用いて、**ご自身のデータ領域をより深く理解**\n- 自然言語を用いたご自身のデータやAI資産の検索、理解、洞察の抽出による生産性の向上\n\n![Screenshot 2023-11-29 at 21.27.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0a3a34d3-ffbd-416f-e178-c5ab7ce0db0c.png)\n\nデータとAIに対するシンプルな権限モデルを提供します。\n\n- 一つの場所ですべてのデータとAI資産に対するアクセスポリシーを管理、監査するための**統合され、シンプルなインタフェース**を用いて皆様のデータ資産を保護\n- 強化されたセキュリティのために、行や列に対する**きめ細かいアクセス制御**を有効化\n- 一つの場所で一貫性を持って管理される権限と**オープンなインタフェース**を用いた他の計算プラットフォームからセキュアにデータにアクセス\n\n![Screenshot 2023-11-29 at 21.28.28.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/26f0bc4b-56a7-a478-904b-139a75961c58.png)\n\nAIで強化された監視と観測可能性を提供します。\n\n- データとMLモデルの品質問題に対する**プロアクティブな警告**を受信\n- 効率的な根本原因分析やインパクト評価のためにカラムレベルにまで追跡できる**リアルタイムのデータリネージ**へのアクセス\n- ステークホルダーとデータとML品質レポートを容易に共有するための**自動生成ダッシュボード**の活用\n- 課金、監査、リネージなどに対する運用インテリジェンスによる完全なデータとAIの**観測可能性**の実現\n\n![Screenshot 2023-11-29 at 21.29.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7dd6c49d-3d2c-6175-e25e-9ca171c86113.png)\n\nオープンなデータ共有を実現します。\n\n- クラウド、リージョン、プラットフォーム横断で複製不要のシームレスなデータ共有のために、オープンソースのDelta Sharingで**ベンダーロックインを回避**\n- **データ以上のもの**を共有 - ノートブック、MLモデル、ダッシュボード、アプリケーション\n- **オープンなマーケットプレース**でデータ製品の探索と収益化\n- **スケーラブルなデータクリーンルーム**で\nセンシティブなデータに対するセキュアな\nコラボレーション\n\n![Screenshot 2023-11-29 at 21.31.00.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/69f341b7-35ea-3b67-7d3f-ed39b0e61329.png)\n\n# ガバナンスの課題とUnity Catalogのアプローチ\n\n- 目の前にあるテーブルにどのようなクエリーを行えばいいのかわからない。\n- 目の前にあるテーブルが作成された経緯がわからず、容易に活用できない。\n- テーブルの説明文を記載したいが手が回らない。\n- 特定のテーブルに詳しい専門家が特定できず問い合わせられない。\n- 特定のテーブルがどこで利用されているのかわからず、更新・削除が行えない。\n- テーブルだけではなく、機械学習モデルやファイルに対するアクセス制御が煩雑。\n- データに対するアクセスコントロールの粒度が粗く、セキュリティを高めると分析できない。\n\n## 目の前にあるテーブルにどのようなクエリーを行えばいいのかわからない\n\n- カタログエクスプローラでテーブルにアクセスし、**洞察タブ**を表示します。\n- テーブルに対して実行されているクエリーにクイックにアクセスすることができます。\n- クエリーをクリックすると詳細を確認でき、![Screenshot 2023-11-29 at 21.33.45.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/886f603c-eaa4-d9b2-e64e-82a8b7aae9b8.png)をクリックすることでクエリエディタに移動できます。\n\n![Screenshot 2023-11-29 at 21.33.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/4b8e947d-cb17-d684-64ab-2b72e65cedf8.png)\n\n## 目の前にあるテーブルが作成された経緯がわからず、容易に活用できない\n\n- カタログエクスプローラでテーブルにアクセスし、**依存関係タブ**を表示します。依存関係がUpstreamになっているテーブルから対象のテーブルが作成されていることがわかります。\n- **リネージグラフを表示**をクリックすることで、より視覚的に依存関係を確認することができます。\n\n![Screenshot 2023-11-29 at 21.34.54.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0ca4535e-9c38-3332-8f1a-8e2894a7a049.png)\n![Screenshot 2023-11-29 at 21.35.04.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8acd63ec-154f-6432-c75b-8d6bc50d0a58.png)\n\n## テーブルの説明文を記載したいが手が回らない\n\n- カタログエクスプローラでテーブルにアクセスすると、生成AI(大規模言語モデル)によって、(英語ですが)自動で説明文が生成されます。\n\n![Screenshot 2023-11-29 at 21.35.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/15048a82-3db9-6165-6362-13457bbbe4fc.png)\n\n## 特定のテーブルに詳しい専門家が特定できず問い合わせられない\n\n- カタログエクスプローラでテーブルにアクセスし、**洞察タブ**を表示します。\n- よく使用するユーザーには、対象のテーブルに頻繁にクエリーを実行しているユーザーが表示されるので、詳細を聞いてみましょう。\n\n![Screenshot 2023-11-29 at 21.36.28.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/06dad6ee-f163-15b4-cc45-37c697cd3b38.png)\n\n## 特定のテーブルがどこで利用されているのかわからず、更新・削除が行えない\n\n- カタログエクスプローラでテーブルにアクセスし、**依存関係タブ**を表示します。\n- **ダウンストリームへの接続**でフィルタリングして、テーブルにアクセスしているダッシュボード、ノートブック、クエリーなどを特定します。\n\n![Screenshot 2023-11-29 at 21.37.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/767467d6-b58e-fdd8-7266-8acee6377ffd.png)\n\n## テーブルだけではなく、機械学習モデルやファイルに対するアクセス制御が煩雑\n\n- カタログエクスプローラでは、テーブルだけではなく、機械学習モデルやファイルに対するアクセス制御を同じGUIから行えます。\n\n![Screenshot 2023-11-29 at 21.38.14.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b0541fd1-826b-c8b6-b6bc-e1a6d2b56627.png)\n\n## データに対するアクセスコントロールの粒度が粗く、セキュリティを高めると分析できない\n\n- 行フィルター、列マスクを用いることで、行・列レベルのアクセスコントロールが可能です。\n\n![Screenshot 2023-11-29 at 21.38.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ef41efde-1d99-b92a-2178-c07bd6edbdf3.png)\n\n# まとめ\n\n- 今ではガバナンスはデータ(テーブル)のみに適用されるべきものではなく、ファイルや機械学習モデル、処理ロジック(ノートブック)、ダッシュボードなど、データやAIの活用に必要となるあらゆるコンポーネントに対して適用する必要があります。\n- しかし、これらの個別のオブジェクトに個別のソリューションを適用したとしても、すぐに運用が破綻することは想像に難くありません。\n- DatabricksのUnity Catalogを活用することで、これらの課題に容易に対応することができることを感じていただけたら幸いです。皆様のデータと(生成AIを含む)AIの取り組みを支援するために、継続的にUnity Catalogを改善してまいります。是非ご活用ください！\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "周回遅れでLLaVAをDatabricksで動かしてみる", "text": "マルチモーダルすごい。\n\n# LLaVA: Large Language and Vision Assistant\n\nhttps://github.com/haotian-liu/LLaVA/tree/main\n\nhttps://llava-vl.github.io/\n\nhttps://huggingface.co/liuhaotian/llava-v1.5-7b\n\n# Databricksでの動作確認\n\n先人達の肩に乗ります。\n\nhttps://qiita.com/isanakamishiro2/items/f5419f8a031cca0dde41\n\nhttps://qiita.com/fuyu_quant/items/2692198b65d9763b45a2\n\nhttps://note.com/npaka/n/n3656c32f71af\n\nhttps://note.com/owlet_notes/n/nfb8ba6bc0a06\n\n結構大きめのGPUクラスターを準備しました。そうでないと[GPUに収まらないエラー](https://note.com/owlet_notes/n/nfb8ba6bc0a06#cbe35bf5-ffca-450b-a939-2134b3448cb0)になりました。\n![Screenshot 2023-11-28 at 18.30.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/48e09416-94c4-3a0d-61fb-f18c63c5732d.png)\n\n\n`ERROR: Project xxxx has a 'pyproject.toml' and its build backend is missing the 'build_editable' hook. Since it does not have a 'setup.py' nor a 'setup.cfg', it cannot be installed in editable mode. Consider using a build backend that supports PEP 660.\n`を回避するために`touch setup.cfg`を実行しています。[こちら](https://stackoverflow.com/questions/69711606/how-to-install-a-package-using-pip-in-editable-mode-with-pyproject-toml)を参考にしました。\n\n```sh\n%sh\ngit clone https://github.com/haotian-liu/LLaVA.git\ncd LLaVA\ntouch setup.cfg\npip install -U -qq transformers accelerate\npip install -e .\n```\n\nPythonカーネルをリスタート。\n\n```py\ndbutils.library.restartPython()\n```\n\nモデルのダウンロード。\n\n```py\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import get_model_name_from_path\nfrom llava.eval.run_llava import eval_model\n\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\n\ntokenizer, model, image_processor, context_len = load_pretrained_model(\n    model_path=model_path,\n    model_base=None,\n    model_name=get_model_name_from_path(model_path)\n)\n```\n\n推論します。\n\n```py\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\nprompt = \"What are the things I should be cautious about when I visit here?\"\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n\nargs = type('Args', (), {\n    \"model_path\": model_path,\n    \"model_base\": None,\n    \"model_name\": get_model_name_from_path(model_path),\n    \"query\": prompt,\n    \"conv_mode\": None,\n    \"image_file\": image_file,\n    \"sep\": \",\",\n    \"temperature\": 0.2,\n    \"top_p\": 0.8,\n    \"num_beams\": 1,\n    \"min_new_tokens\": None,\n    \"max_new_tokens\": None\n})()\n\ndisplayHTML(f\"<img src=\\\"{image_file}\\\">\")\n\neval_model(args)\n```\n\n動きました！\n![Screenshot 2023-11-28 at 18.35.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1992b0e1-c088-7746-460b-2a9df9a9e59e.png)\n\n日本語ではどうでしょう。\n\n```py\nprompt = \"この場所を訪れた際に注意すべきことはなんですか？\"\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n\nargs = type('Args', (), {\n    \"model_path\": model_path,\n    \"model_base\": None,\n    \"model_name\": get_model_name_from_path(model_path),\n    \"query\": prompt,\n    \"conv_mode\": None,\n    \"image_file\": image_file,\n    \"sep\": \",\",\n    \"temperature\": 0.2,\n    \"top_p\": 0.8,\n    \"num_beams\": 1,\n    \"min_new_tokens\": None,\n    \"max_new_tokens\": None\n})()\n\ndisplayHTML(f\"<img src=\\\"{image_file}\\\">\")\n\neval_model(args)\n```\n\nす、すごい。\n![Screenshot 2023-11-28 at 18.36.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6115afdd-e43c-c137-3e93-0fd0e143cdab.png)\n\n関数化します。\n\n```py\ndef evaluate_image(image_file, prompt):\n\n  args = type('Args', (), {\n    \"model_path\": model_path,\n    \"model_base\": None,\n    \"model_name\": get_model_name_from_path(model_path),\n    \"query\": prompt,\n    \"conv_mode\": None,\n    \"image_file\": image_file,\n    \"sep\": \",\",\n    \"temperature\": 0.2,\n    \"top_p\": 0.8,\n    \"num_beams\": 1,\n    \"min_new_tokens\": None,\n    \"max_new_tokens\": None\n  })()\n\n  displayHTML(f\"<img src=\\\"{image_file}\\\">\")\n  eval_model(args)\n```\n\nこちらの画像を試します。\n\nhttps://www.irasutoya.com/2021/01/blog-post_906.html\n\n```py\nevaluate_image(\"https://1.bp.blogspot.com/-pzkUACogq0E/X5OcHr5ZnSI/AAAAAAABb5Q/xb-j2PQXgu03_vypUL1XNOYv4bhpWEFgQCNcBGAsYHQ/s400/bird_mameruriha_inko_blue.png\", \"何が描かれていますか？\")\n```\n\nうひー。マルチモーダルすごい。\n![Screenshot 2023-11-28 at 18.37.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/62399d2b-77cb-97c0-c139-4757e5c65bf3.png)\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksクラスターでGradioを動かしてみる", "text": "こちらのノリで行けると思ったらうまくいきませんでした。以下のサンプルにあるように[FastAPI](https://fastapi.tiangolo.com/)を使ってます。\n\nhttps://qiita.com/taka_yayoi/items/74a27964527929032efc\n\n# Gradioとは\n\nhttps://www.gradio.app/\n\n機械学習モデル向けGUIをクイックに作ることができるフレームワークだとのこと。\n\n# サンプル\n\nこちらに公開されていました。\n\nhttps://github.com/stikkireddy/dolly-chat-gradio\n\n# 実装\n\n上のサンプルは、[Databricksモデルサービングエンドポイント](https://docs.databricks.com/ja/machine-learning/model-serving/create-manage-serving-endpoints.html)を呼び出すようにカスタマイズされていますが、ここではシンプルなものを動作させるに留めます。\n\nこちらはヘルパーモジュールです。\n\n```py:databricks_magic/__init__.py\nimport json\nfrom dataclasses import dataclass\n\nimport uvicorn\nfrom fastapi import FastAPI\n\n\n@dataclass\nclass ProxySettings:\n    proxy_url: str\n    port: str\n    url_base_path: str\n\n\nclass DatabricksApp:\n\n    def __init__(self, port):\n        # self._app = data_app\n        self._port = port\n        import IPython\n        self._dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n        self._display_html = IPython.get_ipython().user_ns[\"displayHTML\"]\n        self._context = json.loads(self._dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())\n        # コンテキストを設定した後にこちらを実行する必要があります\n        self._cloud = self.get_cloud()\n        # クラウドの特定後にプロキシーの設定を作成します\n        self._ps = self.get_proxy_settings()\n        self._fastapi_app = self._make_fastapi_app(root_path=self._ps.url_base_path.rstrip(\"/\"))\n        self._streamlit_script = None\n        # すべてが設定されたらURLを表示します\n\n    def _make_fastapi_app(self, root_path) -> FastAPI:\n        \"\"\"\n        FastAPIを用いてWebアプリを構成します\n        \"\"\"\n        fast_api_app = FastAPI(root_path=root_path)\n\n        @fast_api_app.get(\"/\")\n        def read_main():\n            return {\n                \"routes\": [\n                    {\"method\": \"GET\", \"path\": \"/\", \"summary\": \"Landing\"},\n                    {\"method\": \"GET\", \"path\": \"/status\", \"summary\": \"App status\"},\n                    {\"method\": \"GET\", \"path\": \"/dash\", \"summary\": \"Sub-mounted Dash application\"},\n                ]\n            }\n\n        @fast_api_app.get(\"/status\")\n        def get_status():\n            return {\"status\": \"ok\"}\n\n        return fast_api_app\n\n    def get_proxy_settings(self) -> ProxySettings:\n        \"\"\"\n        Driver Proxyの設定を取得します\n        \"\"\"\n        if self._cloud.lower() not in [\"aws\", \"azure\"]:\n            raise Exception(\"only supported in aws or azure\")\n        prefix_url_settings = {\n            \"aws\": \"https://dbc-dp-\",\n            \"azure\": \"https://adb-dp-\",\n        }\n        suffix_url_settings = {\n            \"aws\": \"cloud.databricks.com\",\n            \"azure\": \"azuredatabricks.net\",\n        }\n        org_id = self._context[\"tags\"][\"orgId\"]\n        org_shard = \"\"\n        # URLを構築する際、org_shardにはdns名の\".\"の接尾辞は不要です\n        if self._cloud.lower() == \"azure\":\n            org_shard_id = int(org_id) % 20\n            org_shard = f\".{org_shard_id}\"\n        cluster_id = self._context[\"tags\"][\"clusterId\"]\n        url_base_path = f\"/driver-proxy/o/{org_id}/{cluster_id}/{self._port}/\"\n        return ProxySettings(\n            proxy_url=f\"{prefix_url_settings[self._cloud.lower()]}{org_id}{org_shard}.{suffix_url_settings[self._cloud.lower()]}{url_base_path}\",\n            port=self._port,\n            url_base_path=url_base_path\n        )\n\n    @property\n    def app_url_base_path(self):\n        return self._ps.url_base_path\n\n    def mount_gradio_app(self, gradio_app):\n        import gradio as gr\n        gr.mount_gradio_app(self._fastapi_app, gradio_app, f\"/gradio\")\n        # self._fastapi_app.mount(\"/gradio\", gradio_app)\n        self.display_url(self.get_gradio_url())\n\n    def get_cloud(self):\n        \"\"\"\n        クラウドプロバイダーを特定します\n        \"\"\"\n        if self._context[\"extraContext\"][\"api_url\"].endswith(\"azuredatabricks.net\"):\n            return \"azure\"\n        return \"aws\"\n\n    def get_gradio_url(self):\n        \"\"\"\n        gradioのURLへのリンクを返却します\n        \"\"\"\n        # リダイレクトしないようにするために \"/\" で終わる必要があります\n        return f'<a href=\"{self._ps.proxy_url}gradio/\">Click to go to Gradio App!</a>'\n\n    def display_url(self, url):\n        self._display_html(url)\n\n    def run(self):\n        \"\"\"\n        uvicornでアプリケーションを実行します\n        \"\"\"\n        uvicorn.run(self._fastapi_app, host=\"0.0.0.0\", port=self._port)\n```\n\n実行するノートブックはこちら。\n\n```py\n%pip install Jinja2==3.0.3 fastapi uvicorn nest_asyncio gradio==3.32.0\n```\n\n```py\nfrom databricks_magic import DatabricksApp\ndbx_app = DatabricksApp(8098)\n```\n\nこちらがGUIの実装です。\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Hello Gradio\")\n    gr.Markdown(\"## Gradioプロトタイプ\")\n    with gr.Accordion(\"設定: 設定するにはクリックしてください\", open=False):\n        with gr.Row():\n            endpoint = gr.Textbox(label=\"エンドポイント\", interactive=True)\n            token = gr.Textbox(label=\"パスワード\", interactive=True, type=\"password\")\n\n    chatbot = gr.Chatbot([], elem_id=\"chatbot\").style(height=500)\n    with gr.Row():\n        with gr.Column(scale=0.85):\n            txt = gr.Textbox(\n                show_label=False,\n                placeholder=\"テキストを指定してください\",\n            ).style(container=False)\n        with gr.Column(scale=0.15, min_width=0):\n            btn = gr.Button(\"クリア\")\n```\n\n上記アプリケーションをヘルパーモジュールにマウントします。\n\n```py\ndbx_app.mount_gradio_app(demo)\n```\n\nリンクが表示されます。\n![Screenshot 2023-11-27 at 21.26.15.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b9d7be43-1cc7-02c9-e081-616862711c57.png)\n\nWebサーバを起動します。\n\n```py\nimport nest_asyncio\nnest_asyncio.apply()\ndbx_app.run()\n```\n```\nINFO:     Started server process [1207]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8098 (Press CTRL+C to quit)\n```\n\n上で表示されたリンクをクリックするとGUIが表示されます。\n![Screenshot 2023-11-27 at 21.25.58.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/837748c7-8f15-80b5-ed21-c460dbf0ad02.png)\n\n次は、イベントを設定して実際にLLMを呼び出してチャットbotとして動作させるところまでやってみます。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "ETLのE(Extract)実践", "text": "こちらを書いてから、「やっぱりExtract(抽出)も重要だなー」と思った次第です。\n\nhttps://qiita.com/taka_yayoi/items/51bd9a1a895109ec1625\n\nオプションの指定が必要なので、設定がしやすいPySparkで処理します。以下のようなケースをカバーします。\n\n- データベースの抽出\n- 単一CSVファイルの抽出\n- 複数CSVファイルの抽出\n- Parquetファイルの抽出\n- JSONファイルの抽出\n- 画像ファイルの抽出\n\n抽出結果はすべてSparkデータフレームになるので、データフレームに変換処理(T)を適用したり、ターゲットデータベースにロード(L)することもできます。\n\n# 抽出処理の実践\n\n## データの準備\n\nちょうど、Databricksにはサンプルデータがあるのでこれらに対してExtract処理を実行してみます。\n\nhttps://qiita.com/taka_yayoi/items/3f8ccad13c6efd242be1\n\n## 抽出戦略の立案\n\n兎にも角にもソースデータの場所とフォーマットを確認することからです。\n\n1. データの所在の確認: ストレージ上のファイルかデータベースかメッセージか\n1. データフォーマットの確認: ファイルの場合は中身を確認します。それ以外の場合には仕様を確認しましょう\n1. (ファイルの場合)ファイル個数の確認: ファイルは一つか複数か\n\n## データベースの抽出\n\n[前回作成](https://qiita.com/taka_yayoi/items/51bd9a1a895109ec1625)したブロンズテーブルから抽出するものとします。`spark.table`を用いることで、データがSparkデータフレームとして読み込まれます。\n\n```py:Python\ndf = spark.table(\"takaakiyayoi_catalog.etl_in_action.bronze\")\ndisplay(df)\n```\n![Screenshot 2023-11-26 at 13.43.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8edc6ab9-10aa-8e16-d7db-307f5be03a6f.png)\n\n## 単一CSVファイルの抽出\n\nファイルを確認します。マジックコマンド`%fs`を用いることで、DBFSに対するコマンドを実行することができます。このケースでは`head`が適しています。\n\n```\n%fs\nhead /databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\n```\n\nヘッダーありのCSVであることがわかります。\n```\n[Truncated to first 65536 bytes]\n\"\",\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n\"1\",0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n\"2\",0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n\"3\",0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n\"4\",0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n\"5\",0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n\"6\",0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n```\n\n```py:Python\ndf = spark.read.option(\"header\", True).csv(\"dbfs:/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")\ndisplay(df)\n```\n![Screenshot 2023-11-26 at 13.45.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e6c0beab-0e8a-0483-13f6-9707ea1c0719.png)\n\n\n単一ファイルなのでpandasでも読み込めます。\n\n```py:Python\nimport pandas as pd\npdf = pd.read_csv(\"/dbfs/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")\ndisplay(pdf)\n```\n\n## 複数CSVファイルの抽出\n\nSparkの場合、同じディレクトリに格納されているスキーマが同一のファイルは、ディレクトリ指定で一括で読み込むことができます。なお、pandasの場合にはリストやループなどで実装しないといけないですし、そもそもメモリに乗り切らなくてエラーになる可能性が高まります。\n\n```py:Python\ndf = spark.read.option(\"header\", False).csv(\"dbfs:/databricks-datasets/airlines/\")\ndisplay(df)\n```\n![Screenshot 2023-11-26 at 13.48.15.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a5c334a3-ab54-99bc-0f05-ebe3c134379a.png)\n\nおや、ヘッダーが無いようです。もう少し調査してみます。\n\n```\n%fs\nls dbfs:/databricks-datasets/airlines/\n```\n![Screenshot 2023-11-26 at 13.53.01.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/04bd7d0e-bc9b-91a1-7497-c0eb6a71e147.png)\n\nREADME.mdを確認します。\n\n```\n%fs\nhead dbfs:/databricks-datasets/airlines/README.md\n```\n\n有用なことは書いてありません。\n```\n================================================\nAirline On-Time Statistics and Delay Causes\n================================================\n\n## Background\nThe U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time, delayed, canceled and diverted flights appears in DOT's monthly Air Travel Consumer Report, published about 30 days after the month's end, as well as in summary tables posted on this website. BTS began collecting details on the causes of flight delays in June 2003. Summary statistics and raw data are made available to the public at the time the Air Travel Consumer Report is released.\n\n\nFAQ Information is available at http://www.rita.dot.gov/bts/help_with_data/aviation/index.html\n\n\n## Data Source\nhttp://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp\n```\n\n最初のデータファイルを確認します。\n```\n%fs \nhead dbfs:/databricks-datasets/airlines/part-00000\n```\n```\n[Truncated to first 65536 bytes]\nYear,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay,IsArrDelayed,IsDepDelayed\n1987,10,14,3,741,730,912,849,PS,1451,NA,91,79,NA,23,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES\n1987,10,15,4,729,730,903,849,PS,1451,NA,94,79,NA,14,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,NO\n1987,10,17,6,741,730,918,849,PS,1451,NA,97,79,NA,29,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES\n1987,10,18,7,729,730,847,849,PS,1451,NA,78,79,NA,-2,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,NO,NO\n```\n\nこのファイルにはヘッダーがあります。つまり、最初のデータファイルのみにヘッダーがあるということです。やり方はいくつかありますが、最初のデータファイルはヘッダーありで読み込み、残りのデータファイルはヘッダーなしで読み込んで明示的にヘッダーを付与し、これらを結合することにします。\n\n### ヘッダーありファイルの抽出\n\n```py:Python\ndf_first = spark.read.option(\"header\", True).csv(\"dbfs:/databricks-datasets/airlines/part-00000\")\ndisplay(df_first)\n```\n\nヘッダー情報を取得しておきます。\n```py:Python\ndf_schema = df_first.schema\ndf_schema\n```\n```\nStructType([StructField('Year', StringType(), True), StructField('Month', StringType(), True), StructField('DayofMonth', StringType(), True), StructField('DayOfWeek', StringType(), True), StructField('DepTime', StringType(), True), StructField('CRSDepTime', StringType(), True), StructField('ArrTime', StringType(), True), StructField('CRSArrTime', StringType(), True), StructField('UniqueCarrier', StringType(), True), StructField('FlightNum', StringType(), True), StructField('TailNum', StringType(), True), StructField('ActualElapsedTime', StringType(), True), StructField('CRSElapsedTime', StringType(), True), StructField('AirTime', StringType(), True), StructField('ArrDelay', StringType(), True), StructField('DepDelay', StringType(), True), StructField('Origin', StringType(), True), StructField('Dest', StringType(), True), StructField('Distance', StringType(), True), StructField('TaxiIn', StringType(), True), StructField('TaxiOut', StringType(), True), StructField('Cancelled', StringType(), True), StructField('CancellationCode', StringType(), True), StructField('Diverted', StringType(), True), StructField('CarrierDelay', StringType(), True), StructField('WeatherDelay', StringType(), True), StructField('NASDelay', StringType(), True), StructField('SecurityDelay', StringType(), True), StructField('LateAircraftDelay', StringType(), True), StructField('IsArrDelayed', StringType(), True), StructField('IsDepDelayed', StringType(), True)])\n```\n\n### ヘッダーのないファイルの抽出\n\nヘッダーのないファイルのみを対象にしたいので、オプション`pathGlobFilter`を使用します。[こちら](https://stackoverflow.com/questions/65175959/how-to-read-multiple-csv-leaving-out-specific-ones-from-a-nested-directory-in)を参考にさせていただきました。ただ、正規表現に自信がなかったので確認のためのロジックを組み込んでいます。`input_file_name`は入力ファイルのパスを取得できるので、これを追加カラムとして組み込んでいます。自信がある場合には確認はスキップしてください(でも、確認することをお勧めします)。\n\n```py:Python\nfrom pyspark.sql.functions import input_file_name\n\ndf_rest = spark.read.option(\"header\", False).csv(\n    \"dbfs:/databricks-datasets/airlines/\",\n    schema=df_schema,\n    pathGlobFilter=\"part-[0-9][0-9][0-9][0-9][1-9]\",\n)\ndf_rest = df_rest.withColumn(\"filename\", input_file_name())\ndisplay(df_rest)\n```\n![Screenshot 2023-11-26 at 14.30.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/76a7a1b3-77cc-b8ff-0400-a43b411cb391.png)\n\nこのデータフレームをテーブルに見立ててSQLで確認したいので、一時ビューとして登録します。\n\n```py:Python\ndf_rest.createOrReplaceTempView(\"df_rest\")\n```\n\nこれでSQLからアクセスすることができます。\n\n```sql:SQL\n%sql\nSELECT DISTINCT filename FROM df_rest;\n```\n\n`part-00000`がスキップされていることが確認できました。\n![Screenshot 2023-11-26 at 14.32.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d753e82a-ddab-d57b-a6c6-a9b859867396.png)\n\n結合に不要な列があるので、もう一度データフレームを作り直してから`union`で結合します。\n\n```py:Python\ndf_rest = spark.read.option(\"header\", False).csv(\n    \"dbfs:/databricks-datasets/airlines/\",\n    schema=df_schema,\n    pathGlobFilter=\"part-[0-9][0-9][0-9][0-9][1-9]\",\n)\ndf_all = df_first.union(df_rest)\ndisplay(df_all)\n```\n\n完了です。\n![Screenshot 2023-11-26 at 14.35.13.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/806c78e9-4acd-1d13-e792-755fbc16c208.png)\n\n## Parquetファイルの抽出\n\nフォーマットで`Parquet`を指定するだけで大丈夫です。\n\n```py:Python\ndf = spark.read.format(\"parquet\").option(\"header\", True).load(\"dbfs:/databricks-datasets/amazon/data20K/\")\ndisplay(df)\n```\n![Screenshot 2023-11-26 at 14.47.06.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0ed2b724-549d-f085-4978-4d0e98e71135.png)\n\n## JSONファイルの抽出\n\nフォーマットで`json`を指定するだけで大丈夫です。\n\n```py:Python\ndf = spark.read.format(\"json\").load(\"dbfs:/databricks-datasets/definitive-guide/data/activity-data/\")\ndisplay(df)\n```\n![Screenshot 2023-11-26 at 14.48.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/833856b5-02ab-383e-fc52-898ff8c4598d.png)\n\n## 画像ファイルの抽出\n\nSparkでは[画像データソース](https://qiita.com/taka_yayoi/items/8d4b1b61699d68a34e58)もサポートしています。お手軽にディレクトリに格納されている画像ファイルをデータフレームに取り込むことができます。\n\n```\n%fs\nls dbfs:/databricks-datasets/cctvVideos/train_images/\n```\n\n`label=x`というディレクトリが格納されています。\n![Screenshot 2023-11-26 at 14.53.05.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bd5b14c4-4d88-6e11-8ba0-4b2582059651.png)\n\nそれぞれのディレクトリに画像(JPG)ファイルが格納されています。\n\n```\n%fs\nls dbfs:/databricks-datasets/cctvVideos/train_images/label=0/\n```\n![Screenshot 2023-11-26 at 14.53.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6ad7b734-80c9-6e3c-9397-fe187d0403af.png)\n\n`format`で`image`を指定します。\n\n```py:Python\ndf = spark.read.format(\"image\").load(\"dbfs:/databricks-datasets/cctvVideos/train_images/\")\ndisplay(df)\n```\n\n画像がデータフレームとして読み込まれ、親ディレクトリの`label=x`も列に追加されています。この機能を活用すると、画像分類タスクなどにおける教師データとして容易に活用できるようになります。\n![Screenshot 2023-11-26 at 14.54.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/400b1331-a838-557a-f286-00a8c4ae519a.png)\n\n# まとめ\n\n- まずはソースデータを確認しましょう。`head`コマンドが一番お手軽かと。\n- 時には複雑な前処理が必要になる場合がありますが、必要な関数などは揃ってます！\n- 大量データを処理する場合には、コア数の多いマシンで並列度を高めてSparkで高速に処理しましょう。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "ETLのT(Transformation)実践", "text": "最近、データエンジニアリングを改めて勉強中です。\n\nこちらの記事でデータエンジニアリングとは何か、なぜ重要なのかを学び、\n\nhttps://qiita.com/taka_yayoi/items/cba8021d5f368b3c77bb\n\nこちらの記事では実際にETL処理を実行してみました。\n\nhttps://qiita.com/taka_yayoi/items/22e0aa3194848d596198\n\n# ETLとは\n\nETLとは、\n\n- **E**xtract(抽出): ソースシステムからのデータの取り込み\n- **T**ransformation(変換): ターゲットシステムに投入する前に必要な変換処理を適用\n- **L**oad(ロード): ターゲットシステムへのロード\n\nの組み合わせなわけですが、E(抽出)はソースシステムとのインタフェースやフォーマット、取り込み形態(バッチやストリーミング、洗い替えなのか差分取り込みなのか)に注意すれば良いわけで、ロードも同様にどのようにターゲットシステムにロードするのかさえ特定すれば良いわけです(とは言っても、実際に実装する際には悩むことはあります)。\n\n![](https://www.databricks.com/wp-content/uploads/2021/05/ETL-Process.jpg)\n\nこちらの記事でも説明されています。\n\nhttps://www.databricks.com/jp/glossary/extract-transform-load\n\nそして、個人的に鬼門だと思うのがT(変換)だと思うのです。ターゲットシステムにどのような形態で書き込むのかに応じて要件は千差万別です。\n\n良い資料がないかと探していたら、こちらに辿り着きました。ハイレベルですが、変換処理が網羅的に説明されています。\n\nhttps://aws.amazon.com/jp/what-is/etl/?nc1=h_ls\n\nそこで、この記事ではそれらの変換処理を実際に実行してみます。もちろんDatabricksで。\n\n**追記** 思った以上に抽出処理も大変だったので記事にしました。\n\nhttps://qiita.com/taka_yayoi/items/08aeeac8f63a2f8158eb\n\n# 変換処理の実践\n\n## データの準備\n\n変換処理を実践しようとした際に意外に課題になるのが、**どのようにデータを準備するのか**かもしれません。実業務のデータでいきなりトライするのもちょっと怖いですし。そう言った際にまたまたたどり着いたのがこちらのサービス。ダミーデータを簡単に作ることができます。\n\nhttps://generatedata.com/\n\n生成されるSQLはそのままだとDatabricksではエラーになるので、以下のように修正しています。\n\n```sql:SQL\nCREATE TABLE IF NOT EXISTS takaakiyayoi_catalog.etl_in_action.bronze (\n  `name` varchar(255),\n  `email` varchar(255),\n  `numberrange` INTEGER,\n  `postalZip` varchar(10),\n  `region` varchar(50),\n  `address` varchar(255),\n  `phone` varchar(100),\n  `text` VARCHAR(255),\n  `country` varchar(100),\n  `list` varchar(255),\n  `alphanumeric` varchar(255),\n  `currency` varchar(100)\n);\nINSERT INTO\n  takaakiyayoi_catalog.etl_in_action.bronze (\n    `name`,\n    `email`,\n    `numberrange`,\n    `postalZip`,\n    `region`,\n    `address`,\n    `phone`,\n    `text`,\n    `country`,\n    `list`,\n    `alphanumeric`,\n    `currency`\n  )\nVALUES\n  (\n    \"Gareth Robbins\",\n    \"ultricies@yahoo.net\",\n    3,\n    \"M2 3PK\",\n    \"Hampshire\",\n    \"6901 Donec Avenue\",\n    \"(777) 977-8928\",\n    \"Duis risus odio, auctor vitae, aliquet nec, imperdiet nec, leo.\",\n    \"Netherlands\",\n    \"9\",\n    \"WFC54KYL9VU\",\n    \"$57.96\"\n  ),\n  (\n    \"Cole Weber\",\n    \"etiam.laoreet.libero@hotmail.com\",\n    6,\n    \"982197\",\n    \"Ryazan Oblast\",\n    \"Ap #728-7530 Diam Rd.\",\n    \"(835) 954-8753\",\n    \"rutrum urna, nec luctus felis purus ac tellus. Suspendisse sed\",\n    \"Brazil\",\n    \"1\",\n    \"HIA41GBL9RT\",\n    \"$63.15\"\n  ),\n  (\n    \"Aidan Burch\",\n    \"lacus.etiam.bibendum@outlook.net\",\n    5,\n    \"8052\",\n    \"Kiên Giang\",\n    \"1998 Nisl. St.\",\n    \"(383) 937-3521\",\n    \"pede. Nunc sed orci lobortis augue scelerisque mollis. Phasellus libero\",\n    \"Mexico\",\n    \"9\",\n    \"WWG84MSH3FO\",\n    \"$22.17\"\n  ),\n  (\n    \"Portia Solomon\",\n    \"amet@yahoo.edu\",\n    7,\n    \"50705\",\n    \"Chiapas\",\n    \"726-8739 Tellus, St.\",\n    \"(766) 881-5712\",\n    \"ipsum nunc id enim. Curabitur massa. Vestibulum accumsan neque et\",\n    \"Netherlands\",\n    \"5\",\n    \"INJ39LDD8GR\",\n    \"$24.38\"\n  ),\n  (\n    \"Erasmus Conner\",\n    \"nibh.sit.amet@yahoo.couk\",\n    0,\n    \"32281\",\n    \"Southwestern Tagalog Region\",\n    \"P.O. Box 342, 7732 Nulla. Ave\",\n    \"1-576-377-3268\",\n    \"mi, ac mattis velit justo nec ante. Maecenas mi felis,\",\n    \"Norway\",\n    \"5\",\n    \"CHX09SRV8TO\",\n    \"$0.78\"\n  );\n```\n\n:::note\n**注意**\nこのクエリーには`$`が含まれているため、ノートブックで実行すると期待した通りに動作しません。クエリーエディタで実行します。[こちら](https://community.databricks.com/t5/data-engineering/databricks-sql-unable-to-escape-dollar-sign-in-column-name/td-p/8608)にあるように対応される予定です。\n:::\n\n重複レコードを作りたいので、以下のクエリーも実行します。\n\n```sql:SQL\nINSERT INTO\n  takaakiyayoi_catalog.etl_in_action.bronze (\n    `name`,\n    `email`,\n    `numberrange`,\n    `postalZip`,\n    `region`,\n    `address`,\n    `phone`,\n    `text`,\n    `country`,\n    `list`,\n    `alphanumeric`,\n    `currency`\n  )\nVALUES\n  (\n    \"Gareth Robbins\",\n    \"ultricies@yahoo.net\",\n    3,\n    \"M2 3PK\",\n    \"Hampshire\",\n    \"6901 Donec Avenue\",\n    \"(777) 977-8928\",\n    \"Duis risus odio, auctor vitae, aliquet nec, imperdiet nec, leo.\",\n    \"Netherlands\",\n    \"9\",\n    \"WFC54KYL9VU\",\n    \"$57.96\"\n  );\n```\n以下のようにテーブルが作成されました。\n![Screenshot 2023-11-25 at 19.08.17.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b000fe04-3e45-6f94-5e9d-7c8fbd3b1239.png)\n\n## 変換戦略の立案\n\n手元のテーブルを見ながら、最終的にどのような形式のテーブルにしたいのかを考えます。\n\nここでは以下の変換をかけます。[こちらの記事](https://aws.amazon.com/jp/what-is/etl/?nc1=h_ls)のいくつかの変換処理を実行します。なお、すべてSQLで実行します。下に行くほど難しいかと。\n\n1. **データ重複排除:** 重複レコードの排除\n1. **データクレンジング:** 不要列の除外\n1. **分割:** 姓名を別の列に分離\n1. **データ形式の改訂:** 文字列になっている`currency`列を数値に変換\n1. **暗号化:** メールアドレスの`@`の前半を匿名化\n\n## 重複レコードの排除\n\n意図的にレコードを重複させておいてなんですが、まずはレコードの重複排除から行います。SELECTの`DISTINCT`を使用します。\n\n```sql:SQL\nSELECT\n  DISTINCT *\nFROM\n  takaakiyayoi_catalog.etl_in_action.bronze;\n```\n重複が排除されました。\n![Screenshot 2023-11-25 at 19.14.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/577ded0f-7eb5-75a5-11f4-604f4839d552.png)\n\n## 不要列の除外\n\nSELECTの後で必要な列のみを指定します。\n\n```sql:SQL\nSELECT\n  DISTINCT name,\n  email,\n  phone,\n  country,\n  region,\n  address,\n  postalZip,\n  currency\nFROM\n  takaakiyayoi_catalog.etl_in_action.bronze;\n```\n![Screenshot 2023-11-25 at 19.16.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d5de25b9-d6b3-dfdd-3411-d8164311ef6d.png)\n\n## 姓名を別の列に分離\n\n[split関数](https://docs.databricks.com/ja/sql/language-manual/functions/split.html)を使います。戻り値が配列になるので、[添字を指定](https://ganeshchandrasekaran.com/databricks-how-to-split-a-string-using-spark-sql-ada34ec2de82)して性と名それぞれの列に分割します。\n\n```sql:SQL\nSELECT\n  DISTINCT split(name, ' ') [0] AS first_name,\n  split(name, ' ') [1] AS sir_name,\n  email,\n  phone,\n  country,\n  region,\n  address,\n  postalZip,\n  currency\nFROM\n  takaakiyayoi_catalog.etl_in_action.bronze;\n```\n![Screenshot 2023-11-25 at 19.21.22.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6e621b8c-ca38-d68b-5a5e-47e8f1e0b474.png)\n\n## 文字列になっている`currency`列を数値に変換\n\n`$`を除外した上で数値型に変換します。先頭文字を除外するので[substring関数](https://docs.databricks.com/ja/sql/language-manual/functions/substring.html)を使います。戻り値は数値型に変換可能な形になっているので、型の[cast](https://docs.databricks.com/ja/sql/language-manual/functions/cast.html)を行います。\n\n```sql:SQL\nSELECT\n  DISTINCT split(name, ' ') [0] AS first_name,\n  split(name, ' ') [1] AS sir_name,\n  email,\n  phone,\n  country,\n  region,\n  address,\n  postalZip,\n  cast(substring(currency, 2) AS DOUBLE) AS currency_num\nFROM\n  takaakiyayoi_catalog.etl_in_action.bronze;\n```\n\n数値型に変換されました。\n![Screenshot 2023-11-25 at 19.29.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3649121c-fb70-d625-02c7-c1c7a5a91628.png)\n\n## メールアドレスの`@`の前半を匿名化\n\n最後の処理です。正規表現を使うので、[regexp_replace関数](https://docs.databricks.com/ja/sql/language-manual/functions/regexp_replace.html)を使います。\n\n```sql:SQL\nSELECT\n  DISTINCT split(name, ' ') [0] AS first_name,\n  split(name, ' ') [1] AS sir_name,\n  regexp_replace(email, '.*@', 'xxxxx@') AS anonymized_email,\n  phone,\n  country,\n  region,\n  address,\n  postalZip,\n  cast(substring(currency, 2) AS DOUBLE) AS currency_num\nFROM\n  takaakiyayoi_catalog.etl_in_action.bronze;\n```\n\n匿名化されました！\n![Screenshot 2023-11-25 at 19.34.58.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/123725de-7711-ee34-388e-816a64f77c8d.png)\n\n## シルバーテーブルへのロード\n\nこの結果は、[メダリオンアーキテクチャ](https://www.databricks.com/jp/glossary/medallion-architecture)におけるシルバーテーブルに該当するので、テーブルに書き込みます。CTAS(Create Table As Select)を使います。\n\n```sql:SQL\nCREATE TABLE takaakiyayoi_catalog.etl_in_action.silver AS\nSELECT\n  DISTINCT split(name, ' ') [0] AS first_name,\n  split(name, ' ') [1] AS sir_name,\n  regexp_replace(email, '.*@', 'xxxxx@') AS anonymized_email,\n  phone,\n  country,\n  region,\n  address,\n  postalZip,\n  cast(substring(currency, 2) AS DOUBLE) AS currency_num\nFROM\n  takaakiyayoi_catalog.etl_in_action.bronze;\n```\nシルバーテーブルの完成です！\n![Screenshot 2023-11-25 at 19.36.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0fb2eb54-bffa-9d52-b8c9-81ca886f7128.png)\n\n通常はこれよりも多いレコード数になりますので、これらを集計することでゴールドテーブルを作る形となります。\n\n# まとめ\n\n- 変換処理を実装する際には、生のデータ(ブロンズテーブル)からロードするデータ(シルバーテーブル)をイメージし、どのような処理が必要なのかを洗い出しましょう。\n- いきなりすべての処理を実装するのではなく、ステップバイステップで実装していき動作を確認しましょう。\n- すべての変換処理が実装できたら、シルバーテーブルにロードしましょう。\n- 必要な処理のほとんどはビルトインの関数で準備されていますので、[マニュアル](https://docs.databricks.com/ja/sql/language-manual/index.html)にも目を通しておきましょう。\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "データエンジニアリングのイントロダクション", "text": "[What Is Data Engineering? \\| Dremio](https://www.dremio.com/resources/guides/intro-data-engineering/)の翻訳です。\n\nビジネスでは大量のデータが生成されます。顧客フィードバックからセールスのパフォーマンス、株価に至る全ては、企業のオペレーションに影響を与えます。しかし、データが告げるストーリーを理解することがは常に簡単で直感的とは限りません。だからこそ、多くの企業がデータエンジニアリングに依存しているのです。\n\n# データエンジニアリングとは何か？\n\nデータエンジニアリングとは、様々なソースやフォーマットで提供される生のデータを収集、分析できるようにするシステムの設計、構築に関するプロセスを意味します。このようなシステムによって、ユーザーはビジネスの成長につながる実践的なデータアプリケーションを作り出すことが可能となります。\n![](https://www.dremio.com/wp-content/uploads/2022/03/data-engineering.png)\n\n# なぜ、データエンジニアリングが重要なのでしょうか？\n\n企業の規模に関係なく、すべての企業が重要なビジネス上の質問に回答するために調査する必要がある膨大な量の様々なデータを所有しています。データエンジニアリングは、アナリスト、データサイエンティスト、エグゼクティブのようなデータ利用者がすべてのデータを信頼しながら、クイックかつセキュアに調査できるようにするためのプロセスをサポートすることを目的としています。\n\nデータが様々テクノロジーによって管理され、多様な構造で格納されているため、データアナリストは課題に直面にしています。さらに、分析で使用するツールはデータが同じテクノロジーで管理され、同じ構造で格納されていることを前提としています。\n\n例えば、あるブランドが顧客に関して収集するすべてのデータを考えてみます:\n- あるシステムには請求と出荷が格納されています。\n- 別のシステムでは注文履歴を保持しています。\n- そして、他のシステムではカスタマーサポート、行動情報、サードパーティデータが格納されています。\n\n全体的にはこのデータは顧客の包括的なビューを提供します。しかし、これらの様々なデータセットは独立しており、「どのようなタイプの注文が最も高価なカスタマーサポートのコストにつながったのか」という特定の質問への回答が非常に困難になります。\n\nデータエンジニアリングでは、これらのデータセットを統合し、あなたの質問をクイックかつ効率的に回答できるようにします。\n\n# データエンジニアは何をするのでしょうか？\n\nデータエンジニアリングは、需要の高まっているスキルです。データエンジニアは、データを統合し、あなたたちがナビゲートする助けとなるシステムを設計する人たちです。データエンジニアは以下を含む様々なタスクを実行します:\n- 取得：ビジネスに関連するすべての様々なデータセットを特定します\n- クレンジング：データにおけるすべてのエラーを特定し、綺麗にします\n- 変換：すべてのデータに共通的なフォーマットを与えます\n- 曖昧性の除去：複数の方法で解釈し得るデータを解釈します\n- 重複排除：データの重複したコピーを排除します\n\nこれらが完了すると、[データレイク](https://www.dremio.com/resources/guides/intro-data-lakes/)や[データレイクハウス](https://www.dremio.com/data-lakehouse/)のような中央リポジトリにデータを格納することができます。また、データエンジニアはデータのサブセットを[データウェアハウス](https://www.dremio.com/resources/guides/intro-data-warehouses/)にコピー、移動することができます。\n\n# なぜ、データエンジニアリングで処理が必要なのでしょうか？\n\nデータエンジニアは、モダンなデータ分析を支援するために複雑化が進む環境の設計、オペレーション、サポートで重要な役割を担います。これまでは、データエンジニアは適切なパフォーマンスを保証するためにクエリーがクイックに処理されるように、データウェアハウスのスキーマ、テーブル構造、インデックスを注意深く作成していました。データレイクの出現によって、データエンジニアは後段にいるデータ利用者が分析に使用するより多くのデータを管理、提供することになりました。データレイクに格納されるデータは、構造化されておらず、フォーマットも適用されていないことがあります。ビジネスサイドがデータから価値を導出する前に、データエンジニアは注意を払う必要があります。\n\n幸運なことに、データエンジニアリングを通じてデータが完全に綺麗にフォーマットされることで、データの読み込み、理解が容易になります。ビジネスにおいては定常的にデータが生成されるので、これらのプロセスのいくつかを自動化するソフトウェアを見つけ出すことが重要となります。\n\n適切なソフトウェアスタックは、あなたのデータから膨大な量の情報と価値を抽出し、「[データパイプライン](https://www.dremio.com/resources/guides/intro-data-pipeline/)」として知られるデータに対するエンドツーエンドのジャーニーを作り出すことでしょう。パイプラインを通じて情報が伝達され、その過程で何度かの変換、補強、要約を行われることになります。\n\n# データエンジニアリングのツールとスキル\n\nデータエンジニアはデータを操作するために、数多くの様々なツールを活用します。彼らは、ソースシステムからターゲットの目的地にデータを移動するエンドツーエンドのデータパイプラインを作成する特殊なスキルセットを活用します。\n\nデータエンジニアは以下を含む様々なツールとテクノロジーを扱います:\n- ETLツール: [ETL(抽出、変換、ロード)ツール](https://www.dremio.com/resources/guides/intro-etl-tools/)はシステム間でデータを移動します。データにアクセスし、分析により適した形にするステップを通じてデータを「変換」するルールを適用します。\n- SQL: 構造化クエリー言語(SQL)はリレーショナルデータベースへのクエリーにおける標準言語です。\n- Python: Pythonは汎用プログラミング言語です。データエンジニアはETLタスクでPythonを使うことがあります。\n- クラウドデータストレージ: Amazon S3、Azure Data Lake Storage(ADLS)、Google Cloud Storageなど\n- クエリーエンジン: 回答を得るためにデータに対してクエリーを実行するエンジン。データエンジニアは、[Dremio Sonar](https://www.dremio.com/platform/sonar)、Spark、Flinkなどのエンジンを取り扱います。\n\n# データエンジニア vs. データサイエンス\n\nデータエンジニアリングとデータサイエンスはお互いに補完し合うスキルです。データエンジニアは、分析のためにデータを信頼でき、一貫性のあるものにする助けとなります。データサイエンティストは、大規模なデータセットを必要とする、データ探索、機械学習、その他の分析プロジェクトで信頼できるデータを必要とします。データサイエンティストは、自分たちの分析で必要とするデータの特定、準備でデータエンジニアを頼りにすることができます。\n\n# Dremioによるデータエンジニアリング\n\nDremioはデータエンジニアによるデータ管理、BIやアドホック分析における企業データに対する単一かつ統合されたアクセスポイントをシンプルにします。Dremioによる[データレイクハウス](https://www.dremio.com/data-lakehouse/)の詳細を学びましょう。\n\nより詳細を知りたいですか？[データエンジニアリング](https://www.dremio.com/resources/guides/data-engineering/)に関する技術文書をご覧ください。\n"}
{"title": "Databricksにおけるエンドツーエンドのデータパイプラインの構築", "text": "こちらの記事に触発されました。\n\nhttps://qiita.com/zumax/items/0bd6dbd659b15281287c\n\nDatabricksのドキュメントにはもう一つのサンプルがあります。こちらをウォークスルーします。\n\nhttps://docs.databricks.com/ja/getting-started/data-pipeline-get-started.html\n\n# ステップ1：クラスターを作成する\n\n今回はMLは行わないので、[Photon](https://www.databricks.com/jp/product/photon)有効化クラスターにします。\n![Screenshot 2023-11-24 at 15.48.22.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/790ff219-fef4-1f5b-03c7-fa7c1fee0410.png)\n\n# ステップ2：ソースデータを探索する\n\nこちらを参考に探索してみます。\n\nhttps://docs.databricks.com/ja/getting-started/data-pipeline-explore-data.html\n\n`%fs`は[DBFS](https://qiita.com/taka_yayoi/items/f15b50469b368907ec3b)にアクセスするためのマジックコマンドです。\n\n```\n%fs ls \"/databricks-datasets/songs/data-001\"\n```\nファイルが一覧されます。\n![Screenshot 2023-11-24 at 15.54.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/93cf4252-53e0-067b-9aed-bc9336fe8028.png)\n\n`README.md`を確認します。\n\n```\n%fs head --maxBytes=10000 \"/databricks-datasets/songs/README.md\"\n```\n```\nSample of Million Song Dataset\n===============================\n\n## Source\nThis data is a small subset of the [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/).\nThe original data was contributed by The Echo Nest.\nPrepared by T. Bertin-Mahieux <tb2332 '@' columbia.edu>\n\n## Attribute Information\n- artist_id:string\n- artist_latitude:double\n- artist_longitude:double\n- artist_location:string\n- artist_name:string\n- duration:double\n- end_of_fade_in:double\n- key:int\n```\n個々のファイルの中身を確認します。\n\n```\n%fs head --maxBytes=10000 \"/databricks-datasets/songs/data-001/part-00000\"\n```\n```\n[Truncated to first 10000 bytes]\nAR81V6H1187FB48872\tnan\tnan\t\tEarl Sixteen\t213.7073\t0.0\t11\t0.419\t-12.106\tSoldier of Jah Army\tnan\tSOVNZSZ12AB018A9B8\t208.289\t125.882\t1\t0.0\tRastaman\t2003\t--\nARVVZQP11E2835DBCB\tnan\tnan\t\tWavves\t133.25016\t0.0\t0\t0.282\t0.596\tWavvves\t0.471578247701\tSOJTQHQ12A8C143C5F\t128.116\t89.519\t1\t0.0\tI Want To See You (And Go To The Movies)\t2009\t--\nARFG9M11187FB3BBCB\tnan\tnan\tNashua USA\tC-Side\t247.32689\t0.0\t9\t0.612\t-4.896\tSanta Festival Compilation 2008 vol.1\tnan\tSOAJSQL12AB0180501\t242.196\t171.278\t5\t1.0\tLoose on the Dancefloor\t0\t225261\nARK4Z2O1187FB45FF0\tnan\tnan\t\tHarvest\t337.05751\t0.247\t4\t0.46\t-9.092\tUnderground Community\t0.0\tSOTDRVW12AB018BEB9\t327.436\t84.986\t4\t0.673\tNo Return\t0\t101619\n```\n\nタブ区切りのCSVなので、Sparkでロードします。\n\n```py:Python\ndf = spark.read.format('csv').option(\"sep\", \"\\t\").load('dbfs:/databricks-datasets/songs/data-001/part-00000')\ndf.display()\n```\n![Screenshot 2023-11-24 at 15.56.38.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e141f926-c3aa-e741-b371-e268200c04c3.png)\n\nこれは音楽に関する情報が含まれている[Million Songsデータセット](http://millionsongdataset.com/)です。\n\n# ステップ3：生データを取り込む\n\nこれはETLにおけるExtract(抽出)処理になります。生データを読み込み、[メダリオンアーキテクチャ](https://www.databricks.com/jp/glossary/medallion-architecture)におけるブロンズテーブル`bronze_table`として保持します。\n\n```py:Python\nfrom pyspark.sql.types import DoubleType, IntegerType, StringType, StructType, StructField\n\n# 以下のコードで使用される変数の定義\nfile_path = \"/databricks-datasets/songs/data-001/\"\ntable_name = \"takaakiyayoi_catalog.end_to_end_etl.bronze_table\" # 適宜変更してください\ncheckpoint_path = \"/tmp/pipeline_get_started/_checkpoint/song_data\"\n\nschema = StructType(\n  [\n    StructField(\"artist_id\", StringType(), True),\n    StructField(\"artist_lat\", DoubleType(), True),\n    StructField(\"artist_long\", DoubleType(), True),\n    StructField(\"artist_location\", StringType(), True),\n    StructField(\"artist_name\", StringType(), True),\n    StructField(\"duration\", DoubleType(), True),\n    StructField(\"end_of_fade_in\", DoubleType(), True),\n    StructField(\"key\", IntegerType(), True),\n    StructField(\"key_confidence\", DoubleType(), True),\n    StructField(\"loudness\", DoubleType(), True),\n    StructField(\"release\", StringType(), True),\n    StructField(\"song_hotnes\", DoubleType(), True),\n    StructField(\"song_id\", StringType(), True),\n    StructField(\"start_of_fade_out\", DoubleType(), True),\n    StructField(\"tempo\", DoubleType(), True),\n    StructField(\"time_signature\", DoubleType(), True),\n    StructField(\"time_signature_confidence\", DoubleType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"year\", IntegerType(), True),\n    StructField(\"partial_sequence\", IntegerType(), True)\n  ]\n)\n\n(spark.readStream\n  .format(\"cloudFiles\")\n  .schema(schema)\n  .option(\"cloudFiles.format\", \"csv\")\n  .option(\"sep\",\"\\t\")\n  .load(file_path)\n  .writeStream\n  .option(\"checkpointLocation\", checkpoint_path)\n  .trigger(availableNow=True)\n  .toTable(table_name)\n)\n```\n\n処理が完了すると、カタログエクスプローラでテーブルを確認することができます。\n![Screenshot 2023-11-24 at 16.06.01.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e1b3e4e0-ba5f-5b1b-070d-5313887c85e2.png)\n\n# ステップ4：生データを準備する\n\n最終的な利用形態を想定した場合、不要な列がある場合にはこれを除外して利便性を高めます。また、管理面からデータの処理時刻の列を追加します。これがETLにおけるTransform(変換)処理となります。\n\nすでにテーブルとしてデータが保持されているので、SQLを用いてシルバーテーブルを作成することができます。ここまではPythonを実行してきていますが、マジックコマンド`%sql`を用いることで、そのセルのみSQLを実行することができます。ここでは`silver_table`というシルバーテーブルを作成します。もちろん、要件に応じてより[複雑な変換処理](https://qiita.com/taka_yayoi/items/0638c81168d85512f8ad)を実装することも可能です。\n\n```sql:SQL\n%sql\nCREATE OR REPLACE TABLE\n  takaakiyayoi_catalog.end_to_end_etl.silver_table (\n    artist_id STRING,\n    artist_name STRING,\n    duration DOUBLE,\n    release STRING,\n    tempo DOUBLE,\n    time_signature DOUBLE,\n    title STRING,\n    year DOUBLE,\n    processed_time TIMESTAMP\n  );\n\nINSERT INTO\n  takaakiyayoi_catalog.end_to_end_etl.silver_table\nSELECT\n  artist_id,\n  artist_name,\n  duration,\n  release,\n  tempo,\n  time_signature,\n  title,\n  year,\n  current_timestamp()\nFROM\n  takaakiyayoi_catalog.end_to_end_etl.bronze_table\n```\n![Screenshot 2023-11-24 at 16.11.34.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0091f3fa-8e0a-6095-feac-c86e329136b3.png)\n\nこちらも処理が完了すると、カタログエクスプローラでテーブルを確認できます。\n![Screenshot 2023-11-24 at 16.12.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7209a3b1-724a-b9af-f5d5-23fabd5d60c7.png)\n\n# ステップ5：変換されたデータをクエリーする\n\n```sql:SQL\n%sql\n-- どのアーティストが各年で最も曲をリリースしたのか？\nSELECT\n  artist_name,\n  count(artist_name)\nAS\n  num_songs,\n  year\nFROM\n  takaakiyayoi_catalog.end_to_end_etl.silver_table\nWHERE\n  year > 0\nGROUP BY\n  artist_name,\n  year\nORDER BY\n  num_songs DESC,\n  year DESC\n```\n\n一目瞭然です。\n![Screenshot 2023-11-24 at 16.13.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/90cb471b-821c-19d9-97b6-ac14b56a6bb6.png)\n\n# ステップ6：パイプラインを実行するDatabricksジョブを作成する\n\nこれと、**ステップ7：データパイプラインジョブをスケジュールする**はジョブ作成の話なので割愛します。なお、[ステップ3](#ステップ3生データを取り込む)ではSparkのストリーミング処理を使っているので、ソースデータが追加された際に[新規データのみを処理できる](https://qiita.com/taka_yayoi/items/ba4b60024ec209e82a14)というメリットを享受することもできます。今回は静的なデータなので嬉しさが感じられないのですが。\n\n# 追加ステップ: ゴールドテーブルへのロード\n\nまた、このサンプルにはETLのLoad(ロード)処理が含まれていませんが、例えば上記クエリーの結果をゴールドテーブルとして書き込むことでロード処理とすることが可能です。後段の例えばダッシュボードではそのゴールドテーブルを参照して可視化を行うことができます。\n\n```sql:SQL\n%sql\nCREATE OR REPLACE TABLE takaakiyayoi_catalog.end_to_end_etl.gold_table AS\n-- どのアーティストが各年で最も曲をリリースしたのか？\nSELECT\n  artist_name,\n  count(artist_name)\nAS\n  num_songs,\n  year\nFROM\n  takaakiyayoi_catalog.end_to_end_etl.silver_table\nWHERE\n  year > 0\nGROUP BY\n  artist_name,\n  year\nORDER BY\n  num_songs DESC,\n  year DESC\n```\n![Screenshot 2023-11-24 at 16.19.06.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b292dcd8-075d-0b36-fb26-6b5a4d81c81f.png)\n\n最終的な処理結果にテーブルとしてアクセスできるので、可視化も簡単に行えます。これは[Lakeview](https://qiita.com/taka_yayoi/items/4175cac499b5629de5a3)を使っているので、ノーコードで作成したダッシュボードです。\n![Screenshot 2023-11-24 at 16.25.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/374ae021-f186-534f-2611-d0b1c208c3e7.png)\n\nこれで、生データから変換処理、最終テーブルへのロード、可視化という、データエンジニアリングにおける基本的なパスを体験することができました。ぜひお試しください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks共有クラスターにおけるMLflowの活用", "text": "Databricksクラスターでアクセスモードを**共有**にすることで、複数人で一つのクラスターを共有することができますが、いくつかの制限を意識する必要があります。その中でも大きなものは**機械学習ランタイム**を使えないというものです。\n\nhttps://docs.databricks.com/ja/compute/access-mode-limitations.html#shared-access-mode-limitations-on-unity-catalog\n\nこれは、そのままではMLflowでモデルの記録ができないことを意味します。機械学習ランタイムにはMLflowがプレインストールされています。\n\nしかし、Databricksランタイム13.0以降ではシングルノードで動作する機械学習ライブラリ(scikit-learnなど)が動作するようになりました。\n\nこの記事では、実際に共有クラスターでMLflowを活用してみます。ここでは、Databricksランタイム13.3LTSを使っています。\n![Screenshot 2023-11-21 at 10.10.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0350a0e5-9512-6432-9515-9c0916cf59cd.png)\n\nまず、手動でMLflowをインストールします。`dbutils.library.restartPython()`でPythonカーネルを再起動してください。これを行わないと後のステップでエラーになります。\n\n```py\n%pip install mlflow\ndbutils.library.restartPython()\n```\n\nライブラリをインポートします。\n\n```py\nimport mlflow\nimport numpy as np\nimport pandas as pd\nimport sklearn.datasets\nimport sklearn.metrics\nimport sklearn.model_selection\nimport sklearn.ensemble\n```\n\nデータをロードして準備します。\n\n```py\n# データのロードと前処理\nwhite_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-white.csv\", sep=';')\nred_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-red.csv\", sep=';')\nwhite_wine['is_red'] = 0.0\nred_wine['is_red'] = 1.0\ndata_df = pd.concat([white_wine, red_wine], axis=0)\n\n# ワイン品質に基づいた分類ラベルの定義\ndata_labels = data_df['quality'] >= 7\ndata_df = data_df.drop(['quality'], axis=1)\n\n# 80/20でトレーニング/テストデータセットを分割\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n  data_df,\n  data_labels,\n  test_size=0.2,\n  random_state=1\n)\n```\n\nMLflowのオートロギングを有効化します。\n\n```py\n# このノートブックでのMLflow autologgingを有効化\nmlflow.autolog()\n```\n\nモデルのトレーニングとMLflowによるトラッキングを行います。\n\n```py\nwith mlflow.start_run(run_name='gradient_boost') as run:\n  model = sklearn.ensemble.GradientBoostingClassifier(random_state=0)\n  \n  # モデル、パラメータ、トレーニングメトリクスが自動でトラッキングされます\n  model.fit(X_train, y_train)\n\n  predicted_probs = model.predict_proba(X_test)\n  roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])\n  \n  # テストデータに対するAUCスコアは自動で記録されないので、手動で記録します\n  mlflow.log_metric(\"test_auc\", roc_auc)\n  print(\"Test AUC of: {}\".format(roc_auc))\n```\n```\n2023/11/21 00:32:38 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\nTest AUC of: 0.8834365701533531\n```\n\nトラッキングされました！\n![Screenshot 2023-11-21 at 10.12.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c75cdf52-de5d-da90-085f-d067c4b03396.png)\n![Screenshot 2023-11-21 at 10.12.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/12079513-de82-e506-52ba-57cee62f8989.png)\n\nこちらの記事にありますように、共有クラスター自身、今後も改善してまいりますので、是非ご活用ください！\n\nhttps://qiita.com/taka_yayoi/items/c33644f3f1cf23aeb65d\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Pandas API on SparkでpandasとSparkの良いところ取り", "text": "こちらのサンプルを動かしながら、[Pandas API on Spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html)(Koalas)を説明します。\n\nhttps://www.databricks.com/resources/demos/tutorials/data-science-and-ai/pandas-api-with-spark-backend\n\n以前にこちらの記事も翻訳してます。\n\nhttps://qiita.com/taka_yayoi/items/5f1ab08c6a97ba96bb1f\n\n# Pandas API on Sparkとは\n\nその前にpandasの話をさせてください。Pythonを使っている人であれば、まず間違いなく使ったことがあるであろう[pandas](https://pandas.pydata.org/)。テーブルデータを取り扱う際には欠かせないものとなっています。ですので、学校、書籍などでデータサイエンスを学ぶ際には必修科目となっていることでしょう。しかし、pandasには以下のような課題があります。\n\n- メモリに乗り切らない大規模データは処理できません\n- (通常は)並列処理が行えません\n\n一方で、文法はわかりやすく、小規模なデータを取り扱うのであればベストな選択肢と言えます。\n![Screenshot 2023-11-19 at 19.35.00.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/11758bdb-e9a6-87cb-9411-dae305b3916f.png)\n\nしかし、年を経るごとに取り扱うデータ量は増加の一途を辿っており、pandasで全てをやり切ることが困難になっているのも事実です。そう言った背景もあり、Apache Sparkなどの並列処理エンジンの人気が高まり、PythonのAPIであるPySparkも広く利用されるようになっています。\n\nしかし、PySparkの文法はpandasとは結構異なっており、初学者にとっては敷居の高いものとなっていました。\n\n|  | pandasデータフレーム | PySparkデータフレーム |\n|:--|:--|:--|\n| 列指定  | df['col']  |  df['col'] |\n| データフレームの可変性  |  可変 | 不変  |\n| 処理実行  | 即時実行(eagerly)  | 遅延実行(lazily)  |\n| 列の加算  | df['c'] = df['a'] + df['b']  |  df = df.withColumn('c', df['a'] + df['b']) |\n| 列名の変更  | df.columns = ['a','b']  |  df = df.select(df['c1'].alias('a'),<br>df['c2'].alias('b'))<br>df = df.toDF('a', 'b') |\n| 値のカウント  |  df['col'].value_counts()|df.groupBy(df['col']).count().orderBy('count', ascending=False) |\n\nこのようなギャップを埋める目的でスタートしたプロジェクトがKoalasです。\n\n![Screenshot 2023-11-19 at 19.43.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7b001733-907f-636c-78da-6122357feaee.png)\n\n- 2019/4/24に発表\n- Apache Spark上でpandas APIを提供することが狙い\n    - 馴染み深いAPIで2つのエコシステムを統合\n    - 小規模データと大規模データ間のシームレスな移行\n- pandasユーザーのメリット\n    - Koalasを用いてpandasコードをスケールアウト\n    - PySparkの学習をより簡単に\n- PySparkユーザーのメリット\n    - pandasライクの機能でより生産的に\n\nそして、このKoalasプロジェクトはSpark 3.2でSparkに統合されたので、個別にKoalasをインストールしなくてもPandas API on Sparkでpandas APIを活用することができるのです！\n\n| pandas | PySpark | Pandas API on Spark(Koalas) |\n|:--|:--|:--|\n|  import pandas as pd<br>df = pd.read_csv(\"/path/to/my_data.csv\") | df = (spark.read<br>.option(\"inferSchema\", \"true\")<br>.csv(\"/path/to/my_data.csv\"))  | import pyspark.pandas as ps<br>df = ps.read_csv(\"/path/to/my_data.csv\")  |\n| df.columns = ['x', 'y', 'z1']  |  df = df.toDF('x', 'y', 'z1') | df.columns = ['x', 'y', 'z1']  |\n|  df['x2'] = df.x * df.x | df = df.withColumn('x2', df.x * df.x)  | df['x2'] = df.x * df.x  |\n\n# サンプルのウォークスルー\n\nhttps://www.databricks.com/resources/demos/tutorials/data-science-and-ai/pandas-api-with-spark-backend\n\n```py\n# Pandasのread_json(from pandas import read_json)ではなく、Sparkの分散能力を活用するために、単にpyspark pandasをインポートします\nfrom pyspark.pandas import read_json\npdf = read_json(cloud_storage_path+\"/koalas/users\")\nprint(f\"pdf is of type {type(pdf)}\")\ndisplay(pdf)\n```\n\nこのデータフレームはSparkデータフレームではなく、Pyspark.pandasデータフレームであることに注意してください。[はまりそうなポイント](https://kakehashi-dev.hatenablog.com/entry/2022/12/24/090000)が丁寧にまとめられています。以下のどのデータフレームを操作しているのかに注意を払う必要があります。\n\n- pandasデータフレーム\n- Sparkデータフレーム\n- Pandas on Sparkデータフレーム\n\n```\npdf is of type <class 'pyspark.pandas.frame.DataFrame'>\n```\n![Screenshot 2023-11-19 at 19.54.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/fabba614-b0bf-6ccb-793f-ecdd6a3c7741.png)\n\n## pandasデータフレームからPandas on Sparkデータフレームへの変換\n\n```py\nfrom pyspark.pandas import from_pandas\ndates = pd.date_range('20130101', periods=6)\ndf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\n# pandasデータフレームをpandas on sparkに変換し、Sparkのスピードと並列実効性の全てを手に入れます\npdf = from_pandas(df)\nprint(f\"pdf is of type {type(pdf)}\")\n# 標準的なpandasオペレーションを適用します\npdf.mean()\n```\n\n```\npdf is of type <class 'pyspark.pandas.frame.DataFrame'>\nA    0.083945\nB   -0.044224\nC    0.530140\nD    0.577575\ndtype: float64\n```\n\n## SparkデータフレームからPandas on Sparkデータフレームへの変換\n\n```py\n# また、一つの指示でSparkデータフレームをPandas on Sparkデータフレームに変換することができます\n# 例として、Unity Catalogからひとつのテーブルを簡単に読み込んで、データサイエンスの分析にpandas APIを活用することができます\nusers = spark.read.json(cloud_storage_path+\"/koalas/users\").pandas_api()\nprint(f\"users is of type {type(users)}\")\nusers.describe()\n```\n![Screenshot 2023-11-19 at 19.59.38.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ef70ba4a-3621-645d-4e89-f3d577362086.png)\n\n## データセットの探索\n\n```py\nusers[\"age_group\"].value_counts(dropna=False).sort_values()\n```\n```\n10.0     497\n0.0      510\n7.0      941\n3.0      976\n2.0      990\n1.0      991\n9.0      995\n8.0     1005\n5.0     1014\n6.0     1025\n4.0     1056\nName: age_group, dtype: int64\n```\n\n## Pandas on Sparkデータフレームに対するSQL実行\n\n```py\nfrom pyspark.pandas import sql\nage_group = 2\nsql(\"\"\"SELECT age_group, COUNT(*) AS customer_per_segment FROM {users} \n        where age_group > {age_group}\n      GROUP BY age_group ORDER BY age_group \"\"\", users=users, age_group=age_group)\n```\n![Screenshot 2023-11-19 at 20.01.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bfdc5532-4d91-972d-0139-eb4a19d1016e.png)\n\n## 可視化\n\nPandas API on Sparkはインタラクティブなチャート生成でplotlyを活用しています。洞察を得るには、`.plot`メソッドを使用します。\n\n```py\ndf = users.groupby('gender')['age_group'].value_counts().unstack(0)\ndf.plot.bar(title=\"Customer age distribution\")\n```\n![Screenshot 2023-11-19 at 20.02.17.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d10a6dc9-535b-9bcd-cf92-6173af8c851b.png)\n\n# まとめ\n\n(私も)慣れ親しんでいるpandasのAPIを活用しながらも、Sparkの強力な並列分散処理能力を活用できる、Pandas API on Spark(Koalas)、是非ご活用ください！ただ、より複雑なことをやる可能性があるのであれば、このAPIを契機にしてPySparkも学ばれることもお勧めします。Databricksであれば、[AIアシスタントも支援](https://qiita.com/taka_yayoi/items/810bcdf589a33ee8c828)してくれます！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksジョブのウォークスルー", "text": "これも一年以上の記事なので、再度ウォークスルーします。\n\nhttps://qiita.com/taka_yayoi/items/10194fd3b44e9575de26\n\nDatabricksのジョブはマルチタスクジョブなので、個々の処理を**タスク**として定義し、処理フローを組み立ててジョブとして構成することができます。タスクには以下のようなものを定義できます。\n\n- ノートブック\n- Delta Live Tablesパイプライン\n- SQLクエリー\n- ダッシュボード\n- 別のジョブ\n- [if/else](https://qiita.com/taka_yayoi/items/4c333568bfd6c80cf2b7)(日本リージョンでも利用できるようになりました！)\n\nここではシンプルな例として、2つのノートブックタスクから構成されるジョブを構築して実行します。\n\n# ノートブックの作成\n\n## データ取得ノートブック\n\nまず、COVID-19感染者データを取得してテーブルに保存するノートブックを作成します。\n\n```py\nimport re\n\n# Username を取得\nuser_id = spark.sql('select current_user() as user').collect()[0]['user']\n# Username の英数字以外を除去し、全て小文字化。Username をファイルパスやデータベース名の一部で使用可能にするため。\nusername = re.sub('[^A-Za-z0-9]+', '', user_id).lower()\nprint(username)\n```\n\n```py\nimport pandas as pd\n\n# pandasを使ってURLから直接データを取得します\npdf = pd.read_csv(\"https://covid19.mhlw.go.jp/public/opendata/newly_confirmed_cases_daily.csv\")\n# データを表示します\ndisplay(pdf)\n```\n\n```py\n# 縦長に変換\n# id_vars: Dateをidとして、変換後のデータフレームにそのまま残します\n# var_name: variable変数の列名をPrefectureにします\n# value_name: value_name変数はCaseとします\nlong_pdf = pdf.melt(id_vars=[\"Date\"], var_name=\"Prefecture\", value_name=\"Cases\")\ndisplay(long_pdf)\n```\n\nジョブで使用するテーブル名をジョブで使う他のタスクから参照できるように、[タスクバリュー](https://qiita.com/taka_yayoi/items/6f41209c112dccf141df)を活用します。`dbutils.jobs.taskValues.set`でテーブル名をタスクバリューとして設定します。\n\n```py\n# pandasデータフレームをSparkデータフレームに変換します\nsdf = spark.createDataFrame(long_pdf)\n\n# 他のユーザーと重複しないようにテーブル名にユーザー名を埋め込みます\ntable_name = f\"takaakiyayoi_catalog.quickstart_schema.{username}_covid_table\"\n\n# タスクバリューとしてジョブに含める他のノートブックから参照できるようにします\ndbutils.jobs.taskValues.set(key=\"table_name\", value=table_name)\n\n# テーブルとして保存します\nsdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n```\n\n## 可視化ノートブック\n\nテーブルのデータからグラフを作成するノートブックを作成します。`dbutils.jobs.taskValues.get`でタスクバリューとして設定されているテーブル名を取得します。\n\n```py\nimport pyspark.pandas as ps\n\n# タスクバリューからテーブル名を取得します\ntable_name = dbutils.jobs.taskValues.get(\"get_data\", \"table_name\")\nprint(table_name)\n\nsdf = spark.read.table(table_name)\npsdf = ps.DataFrame(sdf)\n#display(psdf)\n```\n\n```py\n%matplotlib inline\n```\n\n```py\n# 東京のデータを可視化\ntokyo_psdf = psdf[psdf[\"Prefecture\"]==\"Tokyo\"][[\"Date\", \"Cases\"]].set_index(\"Date\")\ntokyo_psdf.plot.line()\n```\n\nこのように、2つのノートブックを作成します。\n![Screenshot 2023-11-18 at 14.43.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ca564ae2-27aa-e17d-8b27-2c99a0b72f0a.png)\n\n# ジョブの作成\n\nサイドメニューから**ワークフロー**を選択します。ワークフローとジョブの関係についてはこちらをご覧ください。\n\nhttps://qiita.com/taka_yayoi/items/e881769270f0ec0b7d06\n\n**ジョブを作成**をクリックします。\n![Screenshot 2023-11-18 at 14.45.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ea28e060-041b-d0b1-2ae4-e3739d7323c0.png)\n\nジョブに名前をつけます。\n![Screenshot 2023-11-18 at 14.46.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/182ed9b2-47b1-431e-b336-c5e4d7230b80.png)\n![Screenshot 2023-11-18 at 14.48.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f895f772-bb54-579b-f24b-a1fa06db4ec4.png)\n\nタスクを組み合わせてジョブを構成していきます。1つ目のタスクを作ります。名前は`get_data`にします。なお、タスク名にスペースを含めることはできません。\n\n**種類**はノートブックで、**パス**をクリックして上で作成したノートブックを選択します。\n![Screenshot 2023-11-18 at 14.49.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f3a5523e-35e1-d6dd-91f6-c63b51988b8d.png)\n![Screenshot 2023-11-18 at 14.49.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/06257ae4-ad51-a33a-1755-b219b2829fd8.png)\nこれで、一つ目のタスクができたので**作成**をクリックします。\n![Screenshot 2023-11-18 at 14.49.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/4874c381-4b78-785b-4679-9b567adfd8f4.png)\n\nタスクを追加するので、**タスクを追加**をクリックします。\n![Screenshot 2023-11-18 at 14.51.10.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a38ebdd8-af6e-dcd7-84ec-d5995bc8b57e.png)\n**ノートブック**を選択して、後続のタスクを追加します。\n![Screenshot 2023-11-18 at 14.51.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d51c2c21-d96f-d8a1-6f7e-cf79dbacb1a2.png)\n![Screenshot 2023-11-18 at 14.52.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8201c5aa-6766-f83b-2d7f-1f5f215205a7.png)\n2つ目のタスク名は`visualize`にします。先ほどと同様に可視化のノートブックを選択します。\n![Screenshot 2023-11-18 at 14.52.54.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/07ec1731-c8fc-ff10-6313-9e46aa5ef3ac.png)\n**タスクを作成**をクリックします。\n![Screenshot 2023-11-18 at 14.53.22.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f3781920-ca61-88ab-25c9-fae7e138b78c.png)\n\nこれで、2つのタスクから構成されるジョブが完成しました！\n![Screenshot 2023-11-18 at 14.53.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5db65a84-d877-5f05-e3c8-1c18da87ef2c.png)\n\n# ジョブにおけるクラスター\n\nhttps://docs.databricks.com/ja/clusters/index.html\n\nDatabricksでプログラムの処理を実行する際には、クラスターが必要となります。ジョブにおいても同様です。ただ、ジョブの場合にはジョブクラスターを活用することができます。従来のall purposeクラスターは明示的に停止するか自動停止を活用しない限り稼働し続けますが、ジョブクラスターはジョブの完了後に自動で停止します。本格運用する際にはジョブクラスターの利用をお勧めします。単価もお安くなっています。\n\nただ、ジョブのデバッグを行う際にジョブクラスターを使うと、実行の都度クラスターが停止するので生産性が上がりません。この場合には、ジョブでもall purposeクラスターを使う方が合理的といえます。\n\nジョブの画面で**クラスター**セクションにある**切り替え**ボタンを押して、all purposeクラスターを使用することができます。繰り返しになりますが、本格稼働させる際にはジョブクラスターを選択することを忘れないようにしてください。\n![Screenshot 2023-11-18 at 15.09.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1e6c5c00-d7df-d420-c944-5917756efaae.png)\n![Screenshot 2023-11-18 at 15.09.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/42051778-ff1d-5e20-3f87-139edc55e838.png)\n![Screenshot 2023-11-18 at 15.10.33.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b0fc5736-9dd5-b473-9b39-1cb120e55bcd.png)\n![Screenshot 2023-11-18 at 15.10.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/fcdf3eaf-06ac-05be-5406-07f5036060aa.png)\n\n# ジョブの実行\n\n右上の**今すぐ実行**をクリックしてジョブを実行します。\n![Screenshot 2023-11-18 at 14.53.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/37390114-df9c-62aa-9129-910b784ecb06.png)\n\n\nなお、スケジュール実行したい場合には**スケジュールとトリガー**の**トリガーを追加**をクリックして、スケジュールを指定します。\n![Screenshot 2023-11-18 at 14.54.58.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/53da521b-1737-cee0-57b9-db62c19cf40e.png)\n\n**ジョブの実行**タブに切り替えます。\n![Screenshot 2023-11-18 at 14.56.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c8aaffa5-0787-f1cb-62fd-eb0947be157d.png)\n\nリアルタイムで実行の進捗を確認できます。\n![Screenshot 2023-11-18 at 14.56.45.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2b476ff8-47b0-683d-dbb6-e6742403bc76.png)\n\n少しするとジョブが完了します。開始時刻のリンクをクリックします。\n![Screenshot 2023-11-18 at 15.03.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5b85b2bb-b711-169c-e7c9-07e2a383a8af.png)\n\nタスクごとの実行結果を確認します。タスク`visualize`をクリックします。\n![Screenshot 2023-11-18 at 15.04.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/37a0756e-d3c9-c7e3-328a-a08e1e161c48.png)\n\n可視化の結果を確認できます。\n![Screenshot 2023-11-18 at 15.04.58.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c0587943-e6fa-aac2-d7ac-e4af9c4bb3f4.png)\n\n今回は静的なデータから可視化を行っているだけですが、これが日次で更新される場合であっても、ジョブをスケジュール実行することで常に最新の可視化結果を自動で手に入れることができます。ジョブというと定期実行のためのものとのイメージがありますが、複雑な処理をオーケストレーションできることもジョブの大きなメリットです。是非ご活用ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksにおけるライブラリインストール機能のウォークスルー", "text": "以前、こちらのマニュアルを翻訳しましたが、一年以上経ったこともあり現時点の機能をウォークスルーしてみます。\n\n\nhttps://qiita.com/taka_yayoi/items/0c7fc7d741aad34bf9d4\n\nなお、クラスターにインストールされるDatabricksランタイムには一般的なライブラリはプレインストールされているので、ランタイムに含まれていないライブラリが必要な際にインストールを行います。\n\nそして、利用形態によってライブラリのインストールが異なりますので、注意が必要です。\n\n現時点で利用できるライブラリのインストール形態は以下の2つです。\n\n- ノートブックスコープライブラリ\n- クラスターライブラリ\n\n以前あった[ワークスペースライブラリ](https://docs.databricks.com/ja/libraries/workspace-libraries.html)は非推奨になりました。クラスターへのライブラリのインストールを強制したい場合には、[クラスターポリシー](https://qiita.com/taka_yayoi/items/891cfbe2e04a2d606e81)を使用してください。\n\n\n# ノートブックスコープライブラリ\n\nhttps://docs.databricks.com/ja/libraries/notebooks-python-libraries.html\n\nノートブックスコープライブラリは、現在使用しているノートブックのセッションでのみ有効なライブラリです。同じクラスターにアタッチしている他のノートブックに影響を与えません。実験目的でクイックにライブラリを試したい際に使用します。ノートブックを実行する度にライブラリのインストールが必要となります。\n\nここでは、[drawdata](https://pypi.org/project/drawdata/)というライブラリを試してみます。\n\n```py\n# Importing the drawdata\nfrom drawdata import draw_scatter\ndraw_scatter()\n```\n```\nModuleNotFoundError: No module named 'drawdata'\n```\n\nDatabricksランタイムに入っていないのでエラーになります。なので、ノートブックスコープライブラリとしてライブラリをインストールします。\n\n```py\n%pip install drawdata\ndbutils.library.restartPython()\n```\n\nGoogle Colaboなどでは`!pip install`かと思いますが、Databricksの場合、`%pip install`です。これで、クラスターの全ノードにライブラリがインストールされます。`!pip install`も使えますが、この場合、クラスターのドライバーノードにのみライブラリがインストールされます。\n\nまた、`dbutils.library.restartPython()`も重要です。Databricks Runtime 13.0 以降では、`%pip`コマンドはPythonプロセスを自動的に再起動しません。新しいパッケージをインストールするか、既存のパッケージを更新する場合は、新しいパッケージを確認するために`dbutils.library.restartPython()`を使用する必要がある場合があります。\n\nこれで、ノートブックスコープライブラリとしてインストールされたので、再度トライします。\n\n```py\n# Importing the drawdata\nfrom drawdata import draw_scatter\ndraw_scatter()\n```\n\n動きました。線を描画してプロットからデータを生成することができるんですね。\n![Screenshot 2023-11-17 at 17.59.40.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/58012e05-a2ee-06c1-9ce9-c8c6c3d1f516.png)\n\n# クラスターライブラリ\n\nしかし、上述したようにノートブックスコープライブラリはノートブックを実行する度に、ライブラリのインストールが必要です。この手間を省くにはクラスターライブラリを使用します。クラスターライブラリの場合、当該クラスターにアタッチされるすべてのノートブックがライブラリを利用できるようになります。ですので、恒久的にライブラリを活用することを決断した場合には、クラスターライブラリをお使いください。\n\nクラスターの画面にアクセスし、**ライブラリ**タブを開きます。\n![Screenshot 2023-11-17 at 18.01.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0adc6d6d-bbcb-642a-d1d3-3312f9398b30.png)\n\n新規をインストールをクリックします。PyPI以外にもCRANなどからもライブラリをインストールできます。ライブラリのソースを選択し、パッケージ名を入力しインストールします。\n![Screenshot 2023-11-17 at 18.02.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ab8f89ab-5882-8e62-5e99-ab312c37e34c.png)\n![Screenshot 2023-11-17 at 18.03.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/56756941-1034-7f81-84ad-ecb408324a1a.png)\n\nクラスターライブラリとしてインストールされました。\n![Screenshot 2023-11-17 at 18.04.16.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8c6d2889-07bc-d20a-a560-65da566b0c16.png)\n\n動作確認するためにノートブックをクラスターからデタッチして、再度アタッチします。これでノートブックセッションがクリアされます。\n![Screenshot 2023-11-17 at 18.05.10.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c24aa3e8-69c3-d7a4-b734-66a271a3f3d7.png)\n\n再度drawdataを実行します。\n```py\n# Importing the drawdata\nfrom drawdata import draw_scatter\ndraw_scatter()\n```\n動きました。これで、このクラスターを使っている限り、インストールしたライブラリを利用できるようになりました。\n![Screenshot 2023-11-17 at 18.06.39.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/69eed8ef-5d54-5d9e-e72c-4dec86f6beab.png)\n\nなお、どのようなライブラリがクラスターライブラリやノートブックスコープライブラリとしてインストールされているのかは、画面右にあるライブラリボタンから確認することができます。以下の例では、クラスターライブラリとしてdrawdataがインストールされていることを確認できます。\n![Screenshot 2023-11-17 at 18.05.45.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d4cb3291-2d44-1a13-891c-5eca63a6c2d0.png)\n\n是非ご活用ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksのデータインテリジェンスプラットフォーム", "text": "[What is a data intelligence platform \\| Databricks Blog](https://www.databricks.com/blog/what-is-a-data-intelligence-platform)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n# AIがどのようにデータプラットフォームを根本的に変革するのか、データがどのようにエンタープライズAIを変革するのかに関する我々の視点\n\n[ソフトウェアが世界を食べている](https://www.wsj.com/articles/SB10001424053111903480904576512250915629460)という観察がモダンな技術産業を形作っています。現在では、ソフトウェアは身につける時計から、住宅、工場や農場など我々の生活の至る所に存在しています。Databricksにおいて我々はまもなく**AIがソフトウェアを食べてしまう**と信じています。すなわち、過去数十年に構築されたソフトウェアは、インテリジェントになり、データを活用することでよりスマートなものとなります。このような示唆は広範で多岐に渡り、カスタマーサポートからヘルスケア、教育など全てに影響を及ぼすものと言えます。\n\nこの記事では、AIがどのようにデータプラットフォームを変革するのかに関する見解を述べます。データプラットフォームに対するAIのインパクトはインクリメンタルなものではなく、根本的なものであることを議論します: データへのアクセスの大規模な民主化、手動管理の自動化、カスタムAIアプリケーションのターンキーでの作成などがあります。これらすべては、企業のデータを深く理解する統合プラットフォームの新たな波によって実現されることでしょう。この新世代のシステムを**データインテリジェンスプラットフォーム**と呼びます。\n\n# これまでのデータプラットフォームとその課題\n\nデータウェアハウスは、1980年代に企業における構造化されたビジネスデータを整理するためのソリューションとして出現しました。しかし、企業は2010年までには、AIのような様々なユースケースをサポートするために膨大な量の非構造化データを集め始めることになりました。これに対応するために、すべてのタイプのデータに対してオープンかつスケーラブルなシステムとしてデータレイクが導入されました。2015年までには、ほとんどの企業でデータウェアハウスとデータレイクの両方を運用することが一般的になりました。しかし、この二重のプラットフォームのアプローチは、ガバナンス、セキュリティ、信頼性、管理の面で様々な課題をもたらすことになりました。\n\n5年前、Databricksは両方の世界のベストな部分を結合するために、[レイクハウス](https://qiita.com/taka_yayoi/items/438f762126f57868aa35)というコンセプトを生み出しました。レイクハウスはオープンなフォーマットですべてのデータを格納、制御し、BIからAIに至るワークロードをネイティブでサポートします。当初はレイクハウスは以下のための統合システムを提供していました:\n\n1. 組織のすべてのデータソースを一緒にクエリーする\n1. 統合された方法でデータを使用するすべてのワークロード(BIやAIなど)を管理する\n\nレイクハウスはデータプラットフォームの一カテゴリーとなり、企業に広く導入され、ほとんどのベンダーのスタックに組み込まれています。\n\nこのような進歩にもかかわらず、市場にあるすべての現行のプラットフォームは依然として幾つかの主要な課題に直面しています:\n\n- **技術スキルの障壁:** データのクエリーにはSQL、Python、BIにおける専門知識が必要であり、学習曲線が急峻なものとなります。\n- **データの精度とキュレーション:** 大企業においては適切で正確なデータを見つけ出すことは困難であり、高価なキュレーションと計画が必要となります。\n- **管理の複雑性:** データプラットフォームにおいてはコストが急増し、高い技術力を持つ人材によって管理されない場合には、貧弱なパフォーマンスを体験することになります。\n- **ガバナンスとプライバシー:** 世界中のガバナンス要件は急速に進化しており、AIの進化によって、リネージ、セキュリティ、プライバシーに関する懸念が強まっています。\n- **勃興するAIアプリケーション:** ドメイン固有のリクエストに回答する生成AIアプリケーションを実現するためには、企業は自身のデータから分離されたプラットフォームでLLMを開発、チューニングする必要があり、手動のエンジニアリングを通じて自分たちのデータを接続する必要があります。\n\nこれらの問題の多くは、データプラットフォームが根本的に企業のデータを理解しておらず、どのように活用するのかを知らないために生じています。幸運なことに、生成AIはまさにこれらの課題に取り組むためのパラフルな新たなツールを提供します。\n\n# データインテリジェンスプラットフォームの背後にあるコアのアイデア\n\nデータインテリジェンスプラットフォームは、企業のデータの背マンティクスを深く理解するためにAIモデルを活用することで、データ管理に革命をもたらします。我々はこれを**データインテリジェンス**と呼びます。これらはレイクハウスの基盤上に構築されています - 企業のすべてのデータをクエリー、管理するための統合システム - しかし、新たな能力を追加するためにデータ(コンテンツとメタデータ)とその使用方法(クエリー、レポート、リネージなど)を自動で分析します。このようなデータの深い理解を通じて、データインテリジェンスプラットフォームは以下を実現します:\n\n- **自然言語によるアクセス:** AIモデルを活用することで、DIプラットフォームでは、それぞれの企業の専門用語や略語に対応する形で自然言語でデータを操作することができます。このプラットフォームは企業の用語を学習するために既存のワークロードでどのようにデータが活用されているのかを観察し、非専門家からデータエンジニアに至るすべてのユーザーに対して仕立てられた自然言語のインタフェースを提供します。\n- **意味論に基づくカタログ作成と検索:** 生成AIは、比類なき検索機能を提供するためや自動でデータの利用法を特定するために、それぞれの企業のデータモデル、メトリクス、KPIを理解することができます。\n- **自動化された管理と最適化:** AIモデルはデータの利用法に基づいてデータのレイアウト、パーティショニング、インデックスを最適化することができ、手動での調整や設定変更の必要性を削減します。\n- **強化されたガバナンスとプライバシー:** DIプラットフォームでは自然言語を用いて管理をシンプルにしつつも、センシティブなデータを自動で検知、分類し、誤用を防ぎます。\n- **AIワークロードに対するファーストクラスのサポート:** DIプラットフォームは、適切なビジネスデータに接続できるようにすることで、すべての企業AIアプリケーションをエンハンスし、正確な結果をデリバリーするためにDIプラットフォームによって学習されたせマンティクス(メトリクス、KPIなど)を活用します。AIアプリケーション開発者は不安定なプロンプトエンジニアリングを通じて、インテリジェンスを「ハック」する必要はありません。\n\n何人かの方は、過去数年を通じてBIツールが追加した自然言語のQ&A機能と何が違うのか不思議に思うかもしれません。BIツールは全体的なデータワークロードの(重要ですが)ほんの一部を表現するに過ぎず、このため、生じているワークロードの大部分に対する可視性や、データリネージ、BIレイヤーに到達する前の利用法に対する可視性を持ちません。これらのワークロードに対する可視性無しには、必要な事柄を理解する深いセマンティクスを構築することはできません。このため、これらの自然言語のQ&A能力は広く導入されていないのです。データインテリジェンスプラットフォームによって、より豊富な機能を活用できるようにBIツールは背後のAIモデルを活用できるようになります。だからこそ、我々はこのようなコア機能はデータプラットフォームの中に存在することになるであろうことを信じているのです。\n![](https://cms.databricks.com/sites/default/files/inline-images/data-intelligence-platforms-img-1.png)\n\n# データインテリジェンスプラットフォームとしてのDatabricks\n\nDatabricksにおいて、我々はデータレイクハウス上にデータインテリジェンスプラットフォームを構築しており、それぞれ固有の機能を追加していく中で、これまで以上にデータプラットフォームにおけるAIの可能性に興奮するようになっています。我々は、以下の機能を持つ市場で唯一のプラットフォームとしてのDatabricksレイクハウスに既存のユニークな機能を構築しています。\n\n1. データとAIに対する統合ガバナンスレイヤー\n1. ETL、SQL、機械学習、BIに対する単一の統合クエリーエンジン\n\nさらに、我々がDatabricksIQと呼ぶデータインテリジェンスエンジンはプラットフォームのすべてのパーツを強化し、その中のAIモデルを生成するために買収した[MosaicML](https://www.databricks.com/blog/databricks-mosaicml)を活用しています。\n\nDatabricksIQはすでに、我々の現行のスタックの様々なレイヤーに浸透しています。以下の用途で活用されています:\n\n- 自動でのカラムインデックス、パーティションのレイアウトを含むプラットフォームにおける調整ノブの追加によって、レイクハウスの基盤をさらに強固なものにします。これによって、お客様にTCO削減と優れたパフォーマンスを提供します。\n- Unity Catalog(UC)のすべてのデータ資産に対する説明文やタグを自動で追加することで、UCのガバナンスを改善します。プラットフォーム全体が専門用語、略語、メトリクス、セマンティクスを理解できるようにするために、これらが活用されます。これによって、すぐれた意味論に基づく検索、AIアシスタントの品質の改善、ガバナンス能力の改善を実現します。\n- AIアシスタントにおけるPython、SQLの生成の改善によって、テキストからSQL、テキストからPythonの両方を強化します。\n- 我々のPhotonクエリーエンジンにおけるクエリー計画にデータに関する予測を組み込むことで、クエリーをさらに高速にします。\n- Delta Live Tablesとサーバレスジョブの内部で、ワークロードに関する予測に基づいた最適なオートスケーリングを提供し、コストを最小化します。\n\n最後になりますがより重要なことです。我々はデータインテリジェンスプラットフォームが、企業におけるAIアプリケーションの開発を劇的にシンプルなものすると信じています。我々は、企業の皆様のデータを理解するAIアプリケーションの作成を容易にするために、DatabricksIQを我々のAIプラットフォームであるMosaic AIに直接組み込んでいます。今では、直接企業データをAIに組み込むために、Mosaic AIは以下を含む機能を提供しています:\n\n- あなたのカスタムデータに対する高品質会話エージェントを構築するためのエンドツーエンドのRAG(Retrieval Augmented Generation)を提供しており、「記憶」のためにDatabricksのベクトルデータベースを活用しています。\n- 企業のデータをベースとしてスクラッチからトレーニングするカスタムモデルや、対象ドメインの深い理解を持つAIアプリケーションをさらに強化するためにMPTやLlama 2のような既存モデルに対する継続的な事前学習。\n- 皆様の企業データに対して効率的かつセキュアなサーバレス推論。Unity Catalogのガバナンスや品質モニタリング機能への接続。\n- 人気のあるMLflowオープンソースプロジェクトをベースとしたエンドツーエンドのMLOps。生成されるすべてのデータは自動でアクション可能なものとなり、レイクハウスで追跡、監視することが可能です。\n\n# サマリー\n\n我々は、AIがすべてのソフトウェアを変革し、データプラットフォームがAIを通じてイノベーションが生じる最も成熟した領域のひとつになると信じています。過去を振り返ると、データプラットフォームはエンドユーザーがアクセスするのが困難で、データチームが管理、制御することも困難でした。データインテリジェンスプラットフォームは、これらの課題の両方、データをよりクエリーしやすく、かつ、管理や制御をしやすくするということに直接取り組むことで、このランドスケープを変革することを目的としています。さらに、データとその用法に対する深い理解は、そのようなデータを取り扱う企業AIアプリケーションの基盤となることでしょう。AIがソフトウェアの世界を再構成することで、すべての業界のリーダーが自分たちの組織を強化するためにデータとAIを深く活用する存在になるだろうと我々は信じています。DIプラットフォームはそのような企業の基盤となり、品質、スピード、俊敏性を持つ次世代のデータ&AIアプリケーションを作成できるようにします。\n![](https://cms.databricks.com/sites/default/files/inline-images/data-intelligence-platforms-img-2.jpg)\n*2013の感謝祭を楽しむDatabricksの創業者たち*\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksにおける機械学習モデルトレーニングのウォークスルー", "text": "以前の[エンドツーエンドの機械学習のサンプル](https://qiita.com/taka_yayoi/items/f48ccd35e0452611d81b)に加えて、よりシンプルなサンプルが追加されていました。\n\nhttps://docs.databricks.com/ja/machine-learning/train-model/scikit-learn.html#scikit-learn-classification-notebook\n\n翻訳済みのノートブックはこちらです。\n\nhttps://github.com/taka-yayoi/public_repo_2/blob/master/ml_quickstart/ml-quickstart-training.py\n\n# Databricks MLクイックスタート: モデルのトレーニング\n\nこのノートブックでは、Databricksにおける機械学習モデルトレーニングの概要を説明します。モデルをトレーニングするには、Databricks機械学習ランタイムにプレインストールされているscikit-learnのようなライブラリを活用することができます。さらに、トレーニングしたモデルの追跡にMLflowを活用し、ハイパーパラメータチューニングをスケールさせるためにSparkTrialsとHyperoptを活用することもできます。\n\nこのチュートリアルでは以下をカバーします:\n- Part 1: MLflowトラッキングを用いたシンプルな分類モデルのトレーニング\n- Part 2: Hyperoptを用いたより性能の良いモデルをトレーニングするためのハイパーパラメーターチューニング\n\nモデルライフサイクル管理とモデル推論を含む、Databricksにおける機械学習のプロダクション化の詳細に関しては、MLのエンドツーエンドのサンプルをご覧ください ([AWS](https://docs.databricks.com/ja/mlflow/end-to-end-example.html)|[Azure](https://learn.microsoft.com/ja-jp/azure/databricks/mlflow/end-to-end-example)|[GCP](https://docs.gcp.databricks.com/applications/mlflow/end-to-end-example.html))。\n\n\n\n### 要件\n- Databricksランタイム7.5 ML以降が稼働しているクラスター\n\n![Screenshot 2023-11-15 at 17.37.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/785faca4-e2bd-cb4b-276a-d5567bb7bd1c.png)\n\n\n### ライブラリ\n\n必要なライブラリをインポートします。これらのライブラリは、Databricks機械学習ランタイム([AWS](https://docs.databricks.com/ja/machine-learning/index.html)|[Azure](https://learn.microsoft.com/ja-jp/azure/databricks/machine-learning/)|[GCP](https://docs.gcp.databricks.com/runtime/mlruntime.html))クラスターにプレインストールされており、互換性とパフォーマンスにチューニングされています。\n\n```py:Python\nimport mlflow\nimport numpy as np\nimport pandas as pd\nimport sklearn.datasets\nimport sklearn.metrics\nimport sklearn.model_selection\nimport sklearn.ensemble\n\nfrom hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\nfrom hyperopt.pyll import scope\n```\n\n### データのロード\n\nこのチュートリアルでは、様々なワインのサンプルを記述しているデータセットを使用します。この[データセット](https://archive.ics.uci.edu/ml/datasets/Wine)はUCI機械学習リポジトリから取得しているものであり、DBFS ([AWS](https://docs.databricks.com/ja/dbfs/index.html)|[Azure](https://learn.microsoft.com/ja-jp/azure/databricks/dbfs/)|[GCP](https://docs.gcp.databricks.com/data/databricks-file-system.html))に格納されています。赤ワインと白ワインを品質に基づいて分類することがゴールとなります。\n\nアップロードや他のデータソースからのロードに関する詳細については、データ取扱に関するドキュメント([AWS](https://docs.databricks.com/ja/data/index.html)|[Azure](https://learn.microsoft.com/ja-jp/azure/databricks/data/)|[GCP](https://docs.gcp.databricks.com/data/index.html))をご覧ください。\n\n```py:Python\n# データのロードと前処理\nwhite_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-white.csv\", sep=';')\nred_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-red.csv\", sep=';')\nwhite_wine['is_red'] = 0.0\nred_wine['is_red'] = 1.0\ndata_df = pd.concat([white_wine, red_wine], axis=0)\n\n# ワイン品質に基づいた分類ラベルの定義\ndata_labels = data_df['quality'] >= 7\ndata_df = data_df.drop(['quality'], axis=1)\n\n# 80/20でトレーニング/テストデータセットを分割\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n  data_df,\n  data_labels,\n  test_size=0.2,\n  random_state=1\n)\n```\n\n## Part 1. 分類モデルのトレーニング\n\n### MLflowトラッキング\n\n[MLflowトラッキング](https://www.mlflow.org/docs/latest/tracking.html)を用いることで、お使いの機械学習トレーニングコード、パラメータ、モデルを整理することができます。\n\n[*autologging*](https://www.mlflow.org/docs/latest/tracking.html#automatic-logging)を用いることで、自動でのMLflowトラッキングを有効化することができます。\n\n```py:Python\n# このノートブックでのMLflow autologgingを有効化\nmlflow.autolog()\n```\n```\n2023/11/15 03:18:38 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2023/11/15 03:18:39 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/11/15 03:18:39 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n```\n\n次に、トレーニングされたモデルや関連付けられるメトリクスやパラメータが自動で記録されるMLflowランのコンテキスト内で分類器をトレーニングします。\n\nテストデータに対するモデルのAUCスコアのようなその他のメトリクスを追加することも可能です。\n\n```py:Python\nwith mlflow.start_run(run_name='gradient_boost') as run:\n  model = sklearn.ensemble.GradientBoostingClassifier(random_state=0)\n  \n  # モデル、パラメータ、トレーニングメトリクスが自動でトラッキングされます\n  model.fit(X_train, y_train)\n\n  predicted_probs = model.predict_proba(X_test)\n  roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])\n  \n  # テストデータに対するAUCスコアは自動で記録されないので、手動で記録します\n  mlflow.log_metric(\"test_auc\", roc_auc)\n  print(\"Test AUC of: {}\".format(roc_auc))\n```\n![Screenshot 2023-11-15 at 17.39.06.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2925ba97-8145-4336-a383-decec4561d94.png)\n\n画面右のフラスコマークをクリックすると記録されたモデルを確認することができます。\n![Screenshot 2023-11-15 at 17.39.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/de369658-c5fa-c3b2-2968-a23eec4060e5.png)\n\nこのモデルのパフォーマンスに満足しない場合には、異なるハイパーパラメーターを用いて別のモデルをトレーニングします。\n\n```py:Python\n# 新たなランをスタートし、後でわかるようにrun_nameを割り当てます\nwith mlflow.start_run(run_name='gradient_boost') as run:\n  model_2 = sklearn.ensemble.GradientBoostingClassifier(\n    random_state=0, \n    \n    # n_estimatorsで新たなパラメータ設定をトライします\n    n_estimators=200,\n  )\n  model_2.fit(X_train, y_train)\n\n  predicted_probs = model_2.predict_proba(X_test)\n  roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])\n  mlflow.log_metric(\"test_auc\", roc_auc)\n  print(\"Test AUC of: {}\".format(roc_auc))\n```\n![Screenshot 2023-11-15 at 17.40.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ca7c2633-2e8c-e0ad-1366-c9711214e402.png)\n\nハイパーパラメーターが異なる2つ目のモデルが記録されます。\n![Screenshot 2023-11-15 at 17.41.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ddec35a1-9e33-c459-ef2e-8ca7a7ab7c82.png)\n\n### MLflowランの参照\n\n記録されたトレーニングランを参照するには、エクスペリメントサイドバーを表示するためにノートブック右上の**Experiment**アイコンをクリックします。必要であれば、最新のランを取得、監視するためにリフレッシュアイコンをクリックします。\n\n<img width=\"350\" src=\"https://docs.databricks.com/_static/images/mlflow/quickstart/experiment-sidebar-icons.png\"/>\n\nより詳細なMLflowエクスペリメントページ([AWS](https://docs.databricks.com/ja/mlflow/tracking.html#notebook-experiments)|[Azure](https://learn.microsoft.com/ja-jp/azure/databricks/mlflow/tracking#notebook-experiments)|[GCP](https://docs.gcp.databricks.com/applications/mlflow/tracking.html#notebook-experiments))を表示するために、エクスペリメントページアイコンをクリックすることもできます。このページでは、複数のランを比較し、特定のランの詳細を表示することができます。\n\n![Screenshot 2023-11-15 at 17.42.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/98eb70a9-c01b-e8cd-f732-40ce90aa2b69.png)\n\n### モデルのロード\n\nまた、MLflow APIを用いて特定のランの結果にアクセスすることができます。以下のセルのコードでは、特定のMLflowランでトレーニングされたモデルのロード方法と、予測での利用方法を説明しています。また、MLflowランのページ([AWS](https://docs.databricks.com/ja/mlflow/tracking.html#view-notebook-experiment)|[Azure](https://docs.microsoft.com/azure/databricks/applications/mlflow/tracking#view-notebook-experiment)|[GCP](https://docs.gcp.databricks.com/applications/mlflow/tracking.html#view-notebook-experiment))では特定のモデルをロードするためのコードスニペットを参照することができます。\n\n```py:Python\n# モデルが記録された後では、別のノートブックやジョブでモデルをロードすることができます\n# mlflow.pyfunc.load_modelを用いることで、共通APIを通じてモデルの予測を行うことができます\nmodel_loaded = mlflow.pyfunc.load_model(\n  'runs:/{run_id}/model'.format(\n    run_id=run.info.run_id\n  )\n)\n\npredictions_loaded = model_loaded.predict(X_test)\npredictions_original = model_2.predict(X_test)\n\n# ロードされたモデルはオリジナルと一致すべきです\nassert(np.array_equal(predictions_loaded, predictions_original))\n```\n\n結果が一致するのでassertionを通過します。\n\n## Part 2. ハイパーパラメーターチューニング\n\nこの時点で、シンプルなモデルをトレーニングし、皆様の取り組み結果を整理するためにMLflowトラッキングサービスを活用しました。このセクションでは、Hyperoptを用いてより洗練されたチューニングを実行する方法をカバーします。\n\n### HyperoptとSparkTrialsによる並列トレーニング\n\n\n[Hyperopt](http://hyperopt.github.io/hyperopt/)はハイパーパラメーターチューニングのためのPythonライブラリです。DatabricksにおけるHyperopt活用の詳細については、ドキュメント([AWS](https://docs.databricks.com/ja/machine-learning/automl-hyperparam-tuning/index.html#hyperparameter-tuning-with-hyperopt)|[Azure](https://learn.microsoft.com/ja-jp/azure/databricks/machine-learning/automl-hyperparam-tuning/#hyperparameter-tuning-with-hyperopt)|[GCP](https://docs.gcp.databricks.com/applications/machine-learning/automl-hyperparam-tuning/index.html#hyperparameter-tuning-with-hyperopt))を参照ください。\n\n並列でのハイパーパラメーター探索と複数のモデルのトレーニング実行を行うために、HyperoptとSparkTrialsを活用することができます。これによって、モデルパフォーマンスの最適化に要する時間を削減します。モデルとパラメータを自動で記録するように、MLflowトラッキングはHyperoptとインテグレーションされています。\n\n```py:Python\n# 探索する検索空間の定義\nsearch_space = {\n  'n_estimators': scope.int(hp.quniform('n_estimators', 20, 1000, 1)),\n  'learning_rate': hp.loguniform('learning_rate', -3, 0),\n  'max_depth': scope.int(hp.quniform('max_depth', 2, 5, 1)),\n}\n\ndef train_model(params):\n  # それぞれのワーカーでautologgingを有効化\n  mlflow.autolog()\n  with mlflow.start_run(nested=True):\n    model_hp = sklearn.ensemble.GradientBoostingClassifier(\n      random_state=0,\n      **params\n    )\n    model_hp.fit(X_train, y_train)\n    predicted_probs = model_hp.predict_proba(X_test)\n    # テストデータに対するAUCに基づくチューニング\n    # プロダクション環境では、代わりに別の検証用データセットを活用することができます\n    roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])\n    mlflow.log_metric('test_auc', roc_auc)\n    \n    # fminがauc_scoreを最大化するように、lossを -1*auc_score に設定します\n    return {'status': STATUS_OK, 'loss': -1*roc_auc}\n\n# SparkTrialsがSparkワーカーを用いてチューニングを分散させます\n# 並列度を高めると処理を加速しますが、それぞれのハイパーパラメーターのトライアルが他のトライアルから得られる情報が減少します\n# 小規模なクラスターやDatabricks Community Editionでは parallelism=2 に設定してください\nspark_trials = SparkTrials(\n  parallelism=8\n)\n\nwith mlflow.start_run(run_name='gb_hyperopt') as run:\n  # 最大のAUCを達成するパラメーターを特定するためhyperoptを使用します\n  best_params = fmin(\n    fn=train_model, \n    space=search_space, \n    algo=tpe.suggest, \n    max_evals=32,\n    trials=spark_trials)\n```\n\nハイパーパラメーターチューニングの過程で多数のモデルが生成されますが、それらはすべてMLflowによって追跡・記録されます。\n![Screenshot 2023-11-15 at 17.44.01.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7310cd5e-ca42-a122-3181-c111bd07fd12.png)\n\n### ベストモデルを取得するためにランを検索します\n\nすべてのランはMLflowで追跡されるので、テストデータに対する最大のAUCを持つチューニングのランを特定するために、MLflowのsearch runs APIを用いて、ベストなランのメトリクスとパラメーターを取得することができます。\n\nこのチューニングされたモデルは、パート1でトレーニングされたよりシンプルなモデルよりも良いパフォーマンスであるべきです。\n\n```py:Python\n# テストデータに対するAUCでランをソートします。値が同じ場合には最新のランを使用します\nbest_run = mlflow.search_runs(\n  order_by=['metrics.test_auc DESC', 'start_time DESC'],\n  max_results=10,\n).iloc[0]\nprint('Best Run')\nprint('AUC: {}'.format(best_run[\"metrics.test_auc\"]))\nprint('Num Estimators: {}'.format(best_run[\"params.n_estimators\"]))\nprint('Max Depth: {}'.format(best_run[\"params.max_depth\"]))\nprint('Learning Rate: {}'.format(best_run[\"params.learning_rate\"]))\n\nbest_model_pyfunc = mlflow.pyfunc.load_model(\n  'runs:/{run_id}/model'.format(\n    run_id=best_run.run_id\n  )\n)\nbest_model_predictions = best_model_pyfunc.predict(X_test[:5])\nprint(\"Test Predictions: {}\".format(best_model_predictions))\n```\n\nハイパーパラメーターチューニングによって、AUC(Area Under Curve)は0.91から0.917まで改善しました。\n\n```\nBest Run\nAUC: 0.9174331940947587\nNum Estimators: 935.0\nMax Depth: 5.0\nLearning Rate: 0.09614045842606779\n```\n![Screenshot 2023-11-15 at 17.48.01.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5e4330d7-87b6-13e4-d280-f6c4daeb0959.png)\n\n### UIで複数のランを比較\n\nパート1と同じように、**Experiment**サイドバーの上部にある外部リンクアイコン経由でMLflowエクスペリメントの詳細ページでランを参照、比較できます。\n\nエクスペリメント詳細ページでは、親のランを展開するために　\"+\"　アイコンをクリックし、親を除くすべてのランを選択し、**Compare**をクリックします。メトリックに対する様々なパラメーターの値のインパクトを表示するparallel coordinates plotを用いて、様々なランを可視化することができます。\n![Screenshot 2023-11-15 at 17.49.33.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/cc761d84-984f-b515-0c16-9bbcb0336295.png)\n![Screenshot 2023-11-15 at 17.49.54.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/585e9b57-b74b-ce42-bc2f-4c70211b29ec.png)\n![Screenshot 2023-11-15 at 17.50.16.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f373c651-f3c7-a304-bd99-b55d2adab866.png)\n\n# まとめ\n\n世の中は生成AI一色ではありますが、世の中のユースケースすべてが生成AIという訳ではありません。この例で触れたような、構造化データの特徴量に基づく機械学習モデルのニーズも依然として存在しており、そのようなモデル開発・運用の営みはこれまで以上に増えてくると思います。そういった場合に、MLflowなどによって実現される[MLOps](https://qiita.com/taka_yayoi/items/47dcd3aee7d028aeef79)の重要性も同様に高まってくると考えております。是非、DatabricksでMLflowを活用いただき、MLOpsを実現してください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksでKaggleのSpaceship Titanicコンペティションにチャレンジしてみる", "text": "ある意味、こちらの続きです。\n\nhttps://qiita.com/taka_yayoi/items/7fcfbaf39dac8326407b\n\nしかし、宇宙船タイタニックとは。SF好きにはたまらない。宇宙船のどの乗客が別次元に転送されてしまったのかを予測する問題。\n\nhttps://www.kaggle.com/competitions/spaceship-titanic\n\nこちらからサンプルノートブックを取得します。\n\nhttps://www.kaggle.com/code/gusthema/spaceship-titanic-with-tfdf/notebook\n\nデータはこちらから。\n\nhttps://www.kaggle.com/competitions/spaceship-titanic/data\n\nデータは前回同様にボリュームにアップロードします。パスは`/Volumes/takaakiyayoi_catalog/default/data`です。\n\n```sh\n%sh\nls /Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic.zip\n```\n```\n/Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic.zip\n```\n\n解凍します。\n\n```sh\n%sh \nunzip /Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic.zip -d /Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic/\n```\n```\nArchive:  /Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic.zip\n  inflating: /Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic/sample_submission.csv  \n  inflating: /Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic/test.csv  \n  inflating: /Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic/train.csv  \n```\n\n`tensorflow_decision_forests`が無いとエラーになったのでインストールします。\n\n```py:Python\n%pip install tensorflow_decision_forests\ndbutils.library.restartPython()\n```\n\nデータセットをロードする際にはボリューム上のパスを指定します。\n\n```py:Python\n# Load a dataset into a Pandas Dataframe\ndataset_df = pd.read_csv('/Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic/train.csv')\nprint(\"Full train dataset shape is {}\".format(dataset_df.shape))\n```\n```\nFull train dataset shape is (8693, 14)\n```\n\nこの後でデータの統計情報やデータ型の確認をするのですが、[dbutils.data.summarize](https://docs.databricks.com/ja/dev-tools/databricks-utils.html#data-utility-dbutilsdata)を使うと簡単にプロファイリングできます。\n\n```py:Python\ndbutils.data.summarize(dataset_df)\n```\n![Screenshot 2023-11-14 at 9.46.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/4d4755e6-9446-446b-dad0-ec94ec9c2e01.png)\n\nその他のクレンジングなどを行った後にフィッティングを行います。\n\n```py:Python\nrf = tfdf.keras.RandomForestModel()\nrf.compile(metrics=[\"accuracy\"]) # Optional, you can use this to include a list of eval metrics\n```\n```py:Python\nrf.fit(x=train_ds)\n```\n```\nReading training dataset...\nTraining dataset read in 0:00:05.167006. Found 6925 examples.\nTraining model...\nModel trained in 0:00:56.797938\nCompiling model...\nModel compiled.\n<keras.src.callbacks.History at 0x7f6aa9def760>\n```\n\nモデルを可視化します。これは便利ですね。\n\n```py:Python\ntfdf.model_plotter.plot_model_in_colab(rf, tree_idx=0, max_depth=3)\n```\n![Screenshot 2023-11-14 at 9.48.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/13500ee4-ae6f-b0ba-1fc7-ad30fb36cf2d.png)\n\nこの他、精度検証などを行い、テストデータセットに対する予測を行います。\n\n```py:Python\n# Load the test dataset\ntest_df = pd.read_csv('/Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic/test.csv')\nsubmission_id = test_df.PassengerId\n\n# Replace NaN values with zero\ntest_df[['VIP', 'CryoSleep']] = test_df[['VIP', 'CryoSleep']].fillna(value=0)\n\n# Creating New Features - Deck, Cabin_num and Side from the column Cabin and remove Cabin\ntest_df[[\"Deck\", \"Cabin_num\", \"Side\"]] = test_df[\"Cabin\"].str.split(\"/\", expand=True)\ntest_df = test_df.drop('Cabin', axis=1)\n\n# Convert boolean to 1's and 0's\ntest_df['VIP'] = test_df['VIP'].astype(int)\ntest_df['CryoSleep'] = test_df['CryoSleep'].astype(int)\n\n# Convert pd dataframe to tf dataset\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)\n\n# Get the predictions for testdata\npredictions = rf.predict(test_ds)\nn_predictions = (predictions > 0.5).astype(bool)\noutput = pd.DataFrame({'PassengerId': submission_id,\n                       'Transported': n_predictions.squeeze()})\n\noutput.head()\n```\n\n提出用データセットを出力します。\n\n```py:Python\nsample_submission_df = pd.read_csv('/Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic/sample_submission.csv')\nsample_submission_df['Transported'] = n_predictions\nsample_submission_df.to_csv('/Volumes/takaakiyayoi_catalog/default/data/spaceship-titanic/submission.csv', index=False)\nsample_submission_df.head()\n```\n\n最終的にはボリュームに以下のようにファイルが格納されることになります。\n![Screenshot 2023-11-14 at 9.50.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5949d56b-b5d1-1ad8-ee8c-31e096642292.png)\n\n`submission.csv`をKaggleに提出します。\n![Screenshot 2023-11-14 at 9.39.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2b13cbb5-1e55-0a5e-277c-2e37a157e070.png)\n![Screenshot 2023-11-14 at 9.38.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/51e91155-59f7-c114-715c-230657d1882a.png)\n\n提出しました！\n![Screenshot 2023-11-14 at 9.39.35.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c8b73806-a1a6-beb4-5b78-edec87b0165a.png)\n\n時間できたらもう少し頑張ってみます。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksのクラスターポリシーでライブラリを指定できるようになりました！", "text": "こちらの機能です。これによって、ご利用されている環境において、特定のバージョンのライブラリに固定したいなどの要件に応えることができるようになります。\n\nhttps://docs.databricks.com/ja/administration-guide/clusters/policies.html#add-libraries-to-a-policy\n\n> ライブラリをポリシーに追加して、ライブラリがコンピュート リソースに自動的にインストールされるようにすることができます。\n\nサイドメニューから**クラスター > ポリシー**に移動します。**ポリシーを作成**をクリックします。**ライブラリ**タブが追加されています。\n![Screenshot 2023-11-12 at 12.05.21.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/93985f87-4be5-99cc-5bb5-91d06fb55c0f.png)\n\n通常、クラスターにライブラリを追加するのと同じようにライブラリを指定します。バージョンを固定することもできます。\n![Screenshot 2023-11-12 at 12.13.44.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/070126fa-baff-5257-9c0a-b754343dcb94.png)\n![Screenshot 2023-11-12 at 12.14.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9e7bc730-312d-921a-b664-b2ef409132f5.png)\n\nクラスター作成時に、上で作成したポリシーを選択します。\n![Screenshot 2023-11-12 at 12.06.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c34ee339-fdb8-2230-dca5-db4acbab71ec.png)\n\nポリシーで指定されたライブラリがインストールされます。\n![Screenshot 2023-11-12 at 12.15.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b5ca88ea-7ce3-2c73-92d3-0dac6160b2ca.png)\n\nインストールされました！是非ご活用ください。\n![Screenshot 2023-11-12 at 12.16.39.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a2676e56-961a-6a8e-4591-0754c9a0ebd3.png)\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "GTPs BuilderでDatabricks Q&A botを作ってみる", "text": "どんどん凄いことになってますね。\n\nhttps://chat-gpt.school/first-openai-devday/#index_id18\n\nBuilderにアクセスします。\n\nhttps://chat.openai.com/gpts/editor/\n\n**プロンプト**\n\n> Databricksに関するQA botを作成して\n\n**レスポンス**\n\nこの辺りは英語で返ってきました。\n\n> **GPT Builder**\n> Great! We're going to create a QA bot focused on Databricks. This bot will \n> specialize in answering questions about Databricks, ranging from basic usage to more advanced concepts.\n>\n> How about we name this GPT \"Databricks Helper\"? Does this name work for you, or do you have another name in mind?\n\n名前はこれでいいのかと聞かれたので、\n\n> それでいいです。\n\nさらにはアイコンも作ってくれます。心遣い恐れ入ります。\n\n![Screenshot 2023-11-11 at 17.49.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c7d9fbaf-ca0d-815f-7a0f-32d9243a180d.png)\n\nここからレスポンスが日本語になりました。\n\n> **GPT Builder**\n> 素晴らしいです！それでは、Databricks Helperの役割と目標についてもう少し詳しく定めましょう。\n>\n> Databricks Helperは、Databricksに関するどのような質問に答えることが期待されますか？例えば、基本的な使い方、特定の機能、最適なプラクティス、または高度なデータ分析のテクニックなどについての質問ですか？\n\n**プロンプト**\n\n> はい、マニュアルやブログの情報をベースに詳細な技術的な質問に回答できるようにしてください\n\n挙動の説明があります。\n\n**レスポンス**\n\n> **GPT Builder**\n> The behavior for Databricks Helper is now set. It's designed to assist with detailed technical questions about Databricks, providing answers based on ?official manuals and authoritative sources. The bot will maintain a professional tone, focusing on delivering high-quality technical support.\n>\n> Would you like to test Databricks Helper in the playground? You can try asking it questions like, \"How do I optimize a Databricks cluster?\" or \"Can you explain Delta Lake in Databricks?\"\n> \n>After testing, feel free to let me know if there are any refinements or changes you'd like to make.\n\n右のペインで問い合わせができるようになりました。すごい、お手軽すぎる。\n![Screenshot 2023-11-11 at 17.43.21.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f32912ba-634e-00c5-42b0-b8d1ef3651d6.png)\n\nしかし、レスポンスは英語です。\n![Screenshot 2023-11-11 at 17.43.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/fe0797c1-7ee4-30b3-94a2-704d3693d6f8.png)\n\nちなみに、**Configure**に切り替えるとアプリの設定ができます。\n\n**Create**に戻って、日本語での回答を行うように指示します。\n\n**プロンプト**\n\n> 日本語で回答するようにしてください\n\nこれで画面も日本語に更新されました。すごい。\n![Screenshot 2023-11-11 at 17.45.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/4eece610-9087-94d8-be41-39e2f4a097b4.png)\n\nレスポンスも日本語です。\n![Screenshot 2023-11-11 at 17.46.40.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c1f7a94e-9fbb-288b-1a75-a6c9a6f0c569.png)\n\nさらにLLMの活用形態の領域が広がった感がします。すごい(すごいしか言ってない)。\n\nこれなら、RAG(Retrieval Augumented Generation)いらないんじゃないかと言う論調もあるようですが、私はまだそこまでとは思ってないです。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Azure Databricks東京リージョンにサーバレスSQLウェアハウスがやってきました！", "text": "どれだけ待ち望んでいたことか。\n\nhttps://learn.microsoft.com/ja-jp/azure/databricks/serverless-compute/\n\n# サーバレスとは\n\n従来のDatabricksでは、計算資源はお客様のアカウント、テナントで動作しており、SQLウェアハウスの軌道には数分を要していました。また、VMのクォータ申請、VMが起動するネットワークなどの管理はお客様自身で行う必要がありました。\n\nこのような課題を解決するのがサーバレスです。計算資源はDatabricks側で管理、提供されます。スタンバイされている計算資源を使用するので、計算資源が数秒で起動します。使用しなくなったらすぐにアイドル状態になるのでコスト面でもメリットがあります。\n![Screenshot 2023-11-10 at 10.42.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/52be9a71-4027-8645-27f4-736e040046b7.png)\n\n# ウォークスルー\n\n東京リージョンのAzure Databricksのワークスペースにログインします。SQLウェアハウスにアクセスして、ウェアハウスを作成します。**サーバーレス**の選択肢が表示されています！わーい。\n![Screenshot 2023-11-10 at 10.38.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/eba23698-d3c7-96b6-c9b3-bbd29f9c728d.png)\n起動すると本当に秒で起動が完了します。\n![Screenshot 2023-11-10 at 10.38.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/475b08c5-e461-ee37-9237-7fcf3c56a828.png)\nもちろん、従来通りクエリーを実行できます。\n![Screenshot 2023-11-10 at 10.39.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3f0e9584-0c13-76d5-e4f5-2c495b92e32d.png)\n\nこれまでの(クラシック)SQLウェアハウスの自動停止の期間は最小が10分でしたが、サーバレスは5分に設定できますので、これまで以上のコスト削減が期待できます。\n\n最後に、こちらは起動する様子を記録したものです。10秒以内に起動しています。\n![serverless.gif](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/aadca3c5-caac-9864-570a-8c298e73e383.gif)\n\n\n是非ご活用ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksにおけるヘルスケア関連のサンプル集", "text": "まとめます。\n\n:::note\n**注意**\nリンク先のノートブックは古いものもあるため、そのままでは動作しない場合があります。\n:::\n\n# ソリューションアクセラレータ\n\n具体的なユースケースのサンプルです。\n\nhttps://qiita.com/taka_yayoi/items/3929677d4e0c9dffaef4\n\nhttps://qiita.com/taka_yayoi/items/2ae740ab884c26e5906e\n\nhttps://qiita.com/taka_yayoi/items/fcf396cf75418e87ec76\n\nhttps://qiita.com/taka_yayoi/items/35863e696c72f0d08909\n\nhttps://qiita.com/taka_yayoi/items/144c319882674d736198\n\nhttps://qiita.com/taka_yayoi/items/3c3dcf73136b580d545c\n\nhttps://qiita.com/taka_yayoi/items/dbb6e5230628797bc922\n\nhttps://qiita.com/taka_yayoi/items/8f6929b9777ab83681dc\n\nhttps://qiita.com/taka_yayoi/items/25f0e689de211c17f3d5\n\nhttps://qiita.com/motokazu_ishikawa/items/e89f9c6655216674e4d1\n\nhttps://qiita.com/motokazu_ishikawa/items/b506c015436371418c46\n\n# Glow\n\n[Glow](https://glow.readthedocs.io/en/latest/)は人口規模での遺伝データを操作するためのオープンソースのツールキットです。\n\nhttps://qiita.com/taka_yayoi/items/d218797152fa480b6673\n\nhttps://qiita.com/taka_yayoi/items/72e96f8f7c0f888782a3\n\nhttps://qiita.com/taka_yayoi/items/afb2bc6d90f983ae412e\n\n# ADAM\n\n[ADAM](https://adam.readthedocs.io/en/latest/)はSparkで動作するゲノムデータ処理ツールです。\n\nhttps://qiita.com/taka_yayoi/items/1a00d3d85363a88a1367\n\n# Hail\n\n[Hail](https://hail.is/)は大規模遺伝子データセットの分析のためにSpark上で利用できるライブラリです。\n\nhttps://qiita.com/taka_yayoi/items/620a829729b147446c58\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "pandas使いがDatabricksで詰まりそうなポイントと解決策", "text": "Databricksではpandasを使えるのですが、その過程でやりたいことによっては詰まることもあるかと思います。こちらでは、その際のティップスをまとめます。DatabricksではSparkを前提としている機能が多いので、場合によってはSparkデータフレームへの変換が必要となります。\n\n話がややこしくなるので、Pandas API on Sparkに関しては一旦脇に置いておきます。\n\nhttps://qiita.com/taka_yayoi/items/db8e1ea52afe5c282c94\n\n# pandasデータフレームをテーブルとして保存したい\n\npandasデータフレームでは直接テーブルに書き込めないので、一旦Sparkデータフレームに変換してから保存します。\n\n```py:Python\nimport pandas as pd\n\ndata = [\n    [\"James\", \"\", \"Smith\", 30, \"M\", 60000],\n    [\"Michael\", \"Rose\", \"\", 50, \"M\", 70000],\n    [\"Robert\", \"\", \"Williams\", 42, \"\", 400000],\n    [\"Maria\", \"Anne\", \"Jones\", 38, \"F\", 500000],\n    [\"Jen\", \"Mary\", \"Brown\", 45, None, 0],\n]\ncolumns = [\"FirstName\", \"MiddleName\", \"LastName\", \"Age\", \"Gender\", \"Salary\"]\n\n# pandasデータフレームの作成\npandasDF = pd.DataFrame(data=data, columns=columns)\n\n# データフレームの表示\nprint(pandasDF)\n```\n\n```\n  FirstName MiddleName  LastName  Age Gender  Salary\n0     James                Smith   30      M   60000\n1   Michael       Rose             50      M   70000\n2    Robert             Williams   42         400000\n3     Maria       Anne     Jones   38      F  500000\n4       Jen       Mary     Brown   45   None       0\n```\n\n`saveAsTable`を用いて書き込み先のテーブルを指定します。ここでは、`takaakiyayoi_catalog.default.sample_table`を指定します。\n\n```py:Python\n# Sparkデータフレームへの変換\nsparkDF = spark.createDataFrame(pandasDF) \n\n# テーブルへの書き込み\nsparkDF.write.mode(\"overwrite\").saveAsTable(\"takaakiyayoi_catalog.default.sample_table\")\n```\n\nカタログエクスプローラからテーブルを確認できます。\n![Screenshot 2023-11-08 at 19.12.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ab297823-17ee-b88e-05d9-8dbfa6c84644.png)\n\n# テーブルからpandasデータフレームに読み込みたい\n\nこちらも、一旦Sparkデータフレームに読み込んだあとでpandasデータフレームに変換します。\n\n```py:Python\n# テーブルからの読み込み\nnewSparkDF = spark.read.table(\"takaakiyayoi_catalog.default.sample_table\")\n\n# pandasデータフレームへの変換\nnewPandasDF = newSparkDF.toPandas()\nnewSparkDF.show()\n```\n```\n+---------+----------+--------+---+------+------+\n|FirstName|MiddleName|LastName|Age|Gender|Salary|\n+---------+----------+--------+---+------+------+\n|    James|          |   Smith| 30|     M| 60000|\n|  Michael|      Rose|        | 50|     M| 70000|\n|   Robert|          |Williams| 42|      |400000|\n|    Maria|      Anne|   Jones| 38|     F|500000|\n|      Jen|      Mary|   Brown| 45|  NULL|     0|\n+---------+----------+--------+---+------+------+\n```\n\n\n# 単一のCSVファイルとして保存したい\n\nSparkデータフレームでデータを加工した後で、単一のCSVファイルとして保存したいというのはよくある話かと思います。ただ、何も気にせずにSparkの`write`で保存すると期待した通りに動作しません。以下の例では[ボリューム](https://qiita.com/taka_yayoi/items/6f57f2bebd15fcb49976)にCSVファイルを出力しています。\n\n```py:Python\nsparkDF.write.format(\"csv\").save(\"/Volumes/takaakiyayoi_catalog/default/data/sample_table.csv\")\n```\n\nこのように、ファイルではなくディレクトリが作成されます。\n![Screenshot 2023-11-08 at 19.18.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ea1e29e2-e7b8-c676-8797-4826557670ba.png)\n\nディレクトリには複数のファイルが作成されています。これは、Sparkが分散処理のためにファイルを[パーティション](https://qiita.com/taka_yayoi/items/bb32d5b7abafd600af98)という単位に分割する仕様となっているためです。設定で1ファイルにまとめることはできますが、メモリの制約がなければ素直にpandasデータフレームに変換してから保存した方が楽です。\n![Screenshot 2023-11-08 at 19.19.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2c57b0b8-e25d-99f3-d8c8-430231429a8a.png)\n\n```py:Python\npandasDFtoWrite = sparkDF.toPandas()\npandasDFtoWrite.to_csv(\"/Volumes/takaakiyayoi_catalog/default/data/sample_table_file.csv\")\n```\nこれで単一のCSVファイルが作成されます。ボリュームにはpandas APIからも普通にアクセスできます。\n![Screenshot 2023-11-08 at 19.22.14.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ee86100d-3c80-ec7b-6d17-98acf8fbfd0f.png)\n\nボリュームにはシェルからもアクセスできます。\n\n```sh:shell\n%sh\ncat /Volumes/takaakiyayoi_catalog/default/data/sample_table_file.csv\n```\n\n```\n,FirstName,MiddleName,LastName,Age,Gender,Salary\n0,James,,Smith,30,M,60000\n1,Michael,Rose,,50,M,70000\n2,Robert,,Williams,42,,400000\n3,Maria,Anne,Jones,38,F,500000\n4,Jen,Mary,Brown,45,,0\n```\n\n途中からボリュームが便利だという話に変わってしまった気がしなくもありません。\n\n# 参考資料\n\n- [サンプルを通じたPandasとPySparkデータフレームの比較](https://qiita.com/taka_yayoi/items/5f1ab08c6a97ba96bb1f)\n- [PySparkことはじめ](https://qiita.com/taka_yayoi/items/a7ee6287031374efa88a#pandas%E3%81%A8%E3%81%AE%E3%82%84%E3%82%8A%E3%81%A8%E3%82%8A)\n- [DatabricksにおけるPySpark、pandasデータフレームの変換の最適化](https://qiita.com/taka_yayoi/items/063c2360670f59b38471)\n- [PySparkとPandasのDataFrameの相互変換](https://qiita.com/aikige/items/42d782d000b6959a424d)\n- [PySpark: シングルファイルとしてCSVを保存する \\- connecting the dots](https://ktksq.hatenablog.com/entry/pyspark-save-csv)\n- [Databrikcs\\(Spark\\)のPysparkにて単一ファイルとしてCSVファイルを書き込む方法](https://qiita.com/manabian/items/78b57741885ecda8570a)\n- [Solved: When I save a Spark dataframe using df\\.write\\.forma\\.\\.\\. \\- Databricks \\- 21582](https://community.databricks.com/t5/data-engineering/when-i-save-a-spark-dataframe-using-df-write-format-quot-csv/td-p/21582)\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksアシスタントで自然言語だけでグラフ描画までやり切る", "text": "こちらの機能でどこまで楽できるかをトライしてみます。\n\nhttps://docs.databricks.com/ja/notebooks/databricks-assistant-faq.html\n\n> Databricksアシスタントは、AIベースのコンパニオンペアプログラマとして機能し、ノートブック、クエリー、ファイルの作成効率を高めることができます。コードとクエリーを生成、最適化、完成、説明、修正することで、質問に迅速に答えることができます。\n\n:::note note\n**注意**\nアシスタントが常に同じ回答を返すわけではないことに注意してください。\n:::\n\n# トライする内容\n\nCOVID-19の感染者数のテーブルにアクセスし、都道府県ごとの感染者数合計を集計し、棒グラフにプロットする。\n\n対象のテーブルはこちらになります。\n![Screenshot 2023-11-07 at 14.46.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0f32ff31-499a-f5a5-9902-8e8928633379.png)\n\n\n# アシスタントの活用\n\n最初のプロンプトはこちらです。\n\n> takaakiyayoi_catalog.japan_covid_analysis.covid_casesの内容を表示\n\n説明文は英語ですがコードを取得できます。\n\n![Screenshot 2023-11-07 at 14.47.17.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/427bf4c3-8858-4946-8232-7538b4c8a3d7.png)\n\n```py:Python\nspark.table(\"takaakiyayoi_catalog.japan_covid_analysis.covid_cases\").show()\n```\n\nセルにカーソルを合わせて **>>** をクリックすると、フォーカスされているセルにコードが挿入されますので、これを実行します。\n![Screenshot 2023-11-07 at 14.48.49.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/40be5f7c-cde7-eaf5-b596-ff7e4a3b6fae.png)\n\n次に都道府県で感染者数を集計します。\n\n> taakaakiyayoi_catalog.japan_covid_analysis.covid_casesをPrefectureごとにグルーピングし、Casesの合計を集計\n\n今度は日本語で帰ってきました。コードは言語に関係はないですけど。\n![Screenshot 2023-11-07 at 14.49.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/72425b1e-0798-0965-76cb-126406ee7da4.png)\n\n```py:Python\nfrom pyspark.sql.functions import sum\n\ndf = spark.table(\"takaakiyayoi_catalog.japan_covid_analysis.covid_cases\")\ngrouped_df = df.groupBy(\"Prefecture\").agg(sum(\"Cases\"))\ndisplay(grouped_df)\n```\n![Screenshot 2023-11-07 at 14.50.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5861238e-e101-299e-202f-651244bea34a.png)\n\n最後にグラフの描画です。以下のプロンプトを実行する際に、上で集計を実行したセルが選択されているようにしてください。そうすることでセルのコードなどがLLMに引き渡されます。\n\n> dfを棒グラフで描画\n\n![Screenshot 2023-11-07 at 14.56.34.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0cda94e4-34cd-7e80-27db-3ce68966c69d.png)\n\n\n```py:Python\nimport matplotlib.pyplot as plt\n\npdf = df.groupby(\"Prefecture\").sum(\"Cases\").toPandas()\npdf.plot(kind=\"bar\", x=\"Prefecture\", y=\"sum(Cases)\", rot=90)\nplt.show()\n```\n![Screenshot 2023-11-07 at 14.53.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a655853d-b536-3aa3-4c1c-4b775bd5d32f.png)\n\n一行のコードを書くことなしにここまでやり切れました！多分、もっと複雑なこともできるはずです。\n\n# まとめ\n\nコーディング体験が大きく変化したと感じる今日この頃です。\n\nこのような機能を説明する機会が増えてきているのですが、その際によくいただく質問が「コーディングの知識は不要になるのでしょうか？」というものです。私個人の意見としては、現時点ではNOです。アシスタントが生成したコードの妥当性を評価するのは(当面は)人間ですので、依然としてコーディングの知識は必要です。\n\nとは言っても、生産性が改善されるのは間違いないですし、Pythonが初めての方も敷居低くプログラミングを始めらる助けになるのではないでしょうか。是非お試しください！\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Lakehouse AIはどのようにリアルタイム計算処理でモデルの精度を改善するのか", "text": "[How Lakehouse AI improves model accuracy with real\\-time computations \\| Databricks Blog](https://www.databricks.com/blog/how-lakehouse-ai-improves-model-accuracy-real-time-computations)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n機械学習モデルの予測の品質は、モデルのトレーニングとサービングで使用されるデータの品質を直接反映します。通常は、特徴量、モデルの入力データは事前に計算され、保存され、モデルの推論において検索され、提供されます。これらの特徴量が事前計算できない場合、モデルのパフォーマンスは特徴量の計算で使用されるデータの新鮮度と直接相関するので課題が生じます。このクラスの特徴量のサービングにおける課題をシンプルにするために、オンデマンドの特徴量計算処理を発表できることを嬉しく思っています。\n\nレコメンデーション、セキュリティシステム、不正検知のようなユースケースでは、これらのモデルのスコアリングの際にオンデマンドで特徴量を計算する必要があります。以下のようなシナリオがあります:\n\n1. モデルサービングの際にのみ特徴量の入力データが利用できる場合。例えば、`distance_from_restaurant`はモバイルデバイスで特定されるユーザーの最終地点が必要となります。\n1. 使用されるコンテキストに基づいて、特徴量の値が変化するケース。`device_type`が*mobile*と*desktop*とでは、エンゲージメントのメトリクスは全く違う解釈をする必要があります。\n1. 特徴量を再計算、格納、リフレッシュするコストが膨大になるケース。ビデオストリーミングサービスでは、数百万のユーザーや数万の動画を取り扱っており、`avg_rating_of_similar_movies`のような特徴量の再計算が不可能になってしまいます。\n\nこれらのユースケースをサポートするためには、推論時に特徴量を計算する必要があります。しかし、モデルトレーニングの特徴量の計算は、通常Apache Spark(™)のようにコスト効率が高く、スループットが最適化されているフレームワークを用いて実行されます。このため、リアルタイムのスコアリングでこれらの特徴量が必要となる際に、二つの主要な問題が引き起こされます:\n\n1. **人手による取り組み、遅延、トレーニング/サービングの偏り:** このアーキテクチャでは多くの場合、JavaやC++のようにレーテンシーが最適化された言語を用いて、サーバーサイドで特徴量計算処理を再度記述する必要があります。これは、2つの異なる言語で特徴量が作られることによりトレーニングとサービングで偏りが生じる可能性を持ち込むだけではなく、機械学習エンジニアがオフラインシステムとオンラインシステムの特徴量ロジックを維持、同期する必要が出てきます。\n1. 特徴量を計算し、モデルに提供するための**アーキテクチャ上の複雑性**。これらの特徴量エンジニアリングパイプラインシステムは、サービングされるモデルと一緒にデプロイ、アップデートされる必要があります。新たなバージョンのモデルがデプロイされると、新たな特徴量の定義が必要となります。このようなアーキテクチャは、不必要なデプロイメントの遅延を引き起こします。機械学習エンジニアは、レート制限、リソース制約、ネットワーク帯域を侵害しないように、新たな特徴量計算パイプラインとエンドポイントがプロダクションのシステムと独立になっていることを確実にする必要があります。\n\n![](https://cms.databricks.com/sites/default/files/inline-images/db-807-blogs-img-1.png)\n*オフラインとオンラインの特徴量計算ロジックの動機を必要とする一般的なアーキテクチャ。特徴量定義の更新は灰色で図示。*\n\n上のアーキテクチャでは、特徴量定義の更新は大規模な取り組みとなりる場合があります。アップデートされた特徴量計算処理パイプラインは、古い特徴量定義によるトレーニングとバッチ推論をサポートし続けるオリジナルのパイプラインと一緒に開発、デプロイされる必要があります。アップデートされた特徴量定義を用いて、モデルは再トレーニング、検証される必要があります。デプロイメントの準備ができたら、エンジニアは最初に、特徴量サーバーの特徴量計算ロジックを再度記述し、プロダクションのトラフィックに影響を与えないように、独立した特徴量サーバーのバージョンをデプロイしなくてはなりません。デプロイメントの後でアップデートされたモデルのパフォーマンスが従来と変わらないことを保証するために、開発過程で数多くのテストを実行する必要があります。モデルのオーケストレータは新たなモデルにトラフィックを送信するようにアップデートする必要があります。最終的には、ある程度の時間の後で、古いモデルと古い特徴量サーバーを停止することができます。\n\nこのアーキテクチャをシンプルにし、エンジアリングのスピードを改善し、可用性を高めるために、Databricksではオンデマンド特徴量の計算処理のサポートをローンチします。この機能は、Unity Catalogに直接組み込まれており、モデルを作成、デプロイするエンドツーエンドユーザーのジャーニーをシンプルにします。\n\n> 「オンデマンド特徴量によって、我々の特徴量エンジニアリングの複雑性を劇的に削減する助けとなりました。オンデマンド特徴量によって、我々のクライアントのそれぞれにユニークな複雑な変換処理の管理を避けることができます。そうではなく、ベースとなる特徴量セットからシンプルにスタートし、トレーニングや推論の過程でオンデマンドでそれらの特徴量をクライアントごとに変換します。まさに、オンデマンド特徴量は次世代モデルを構築する我々の能力を解放しました。」- Chris Messier, Senior Machine Learning Engineer at MissionWired\n\n# 機械学習モデルでの関数の活用\n\n[Unity Catalogにおける特徴量エンジニアリング](https://www.databricks.com/blog/simplification-of-AI-data-feature-store-evolved)によって、データサイエンティストはテーブルから事前にマテリアライズされた特徴量を取得し、関数を用いてオンデマンドの特徴量を計算することができます。オンデマンドの計算処理は、Unity Catalogの管理エンティティであるPythonのユーザー定義関数(UDF)として表現されます。関数はSQLで作成され、SQLクエリー、ダッシュボード、ノートブックで活用することができ、今ではリアルタイムのモデルにおける特徴量の計算にも活用できます。\n\nUCのリネージグラフは、データと関数に対するモデルの依存関係を記録します。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-807-blog-img-2.png)\n\n```sql:SQL\nCREATE OR REPLACE FUNCTION main.on_demand_demo.avg_hover_time(blob STRING)\nRETURNS FLOAT\nLANGUAGE PYTHON\nCOMMENT \"Extract hover time from JSON blob and computes average\"\nAS $$\nimport json\n\ndef calculate_average_hover_time(json_blob):\n    # Parse the JSON blob\n    data = json.loads(json_blob)\n\n    # Ensure the 'hover_time' list exists and is not empty\n    hover_time_list = data.get('hover_time')\n    if not hover_time_list:\n        raise ValueError(\"No hover_time list found or list is empty\")\n\n    # Sum the hover time durations and calculate the average\n    total_duration = sum(hover_time_list)\n    average_duration = total_duration / len(hover_time_list)\n\n    return average_duration\n\nreturn calculate_average_hover_time(blob)\n$$\n```\n\nモデルで関数を使うには、`create_training_set`の呼び出しに関数を含めます。\n\n```py:Python\nfrom databricks.feature_store import FeatureStoreClient\n\nfs = FeatureStoreClient()\n\nfeatures = [\n    FeatureFunction(\n        udf_name=\"main.on_demand_demo.avg_hover_time\",\n        output_name=\"on_demand_output\",\n        input_bindings={\"blob\": \"json_blob\"},\n    ),\n ...\n]\n\ntraining_set = fs.create_training_set(\n    raw_df, feature_lookups=features, label=\"label\", exclude_columns=[\"id\"]\n)\n```\n\nお使いのモデルのトレーニングデータを生成するために、Sparkによって関数が実行されます。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-807-blog-img-3.png)\n\nネイティブなPythonやpandasを用いて、リアルタイムサービングでこの関数を実行することもできます。Sparkはリアルタイムの過程に含まれていませんが、トレーニングに使用されるのと同じ計算処理が保証されます。\n\n# 簡素化されたアーキテクチャ\n\nモデル、関数、データの全てはUnity Catalogに共存することになり、統合ガバナンスを実現します。共有カタログによって、データサイエンティストはモデリングで特徴量と関数を再利用することができ、企業においてどのように特徴量が計算されるのかに関して一貫性を保つことができます。サービングの際には、モデルへの入力として使用される関数やテーブルを特定するためにモデルのリネージが活用され、トレーニング・サービングの偏りの可能性を排除します。全体的に、これによって非常に簡素化されたアーキテクチャが実現されます。\n\nLakehouse AIはモデルのデプロイメントを自動化します: モデルがデプロイされると、Databricksモデルサービングは、ライブでの特徴量計算処理に必要なすべての関数を自動でデプロイします。リクエストの際には、事前にマテリアライズされた特徴量がオンラインストアから検索され、それらのPython UDFの本体を実行することでオンデマンドの特徴量が計算されます。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-807-blogs-img-4.png)\n*Databricksモデルサービングが特徴量検索、オンデマンド関数実行、モデルのスコアリングを管理するアーキテクチャ。*\n\n# シンプルな例 - ホバー時間の平均\n\nこの例では、Webページのホバー時間のリストを抽出するために、オンデマンド特徴量はJSON文字列をパースします。これらの時間の平均が計算され、モデルへの特徴量として平均値が引き渡されます。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-807-blogs-img-5.png)\n\nモデルにクエリーするには、ホバー時間を含むJSONのblobを引き渡します。\n\n```sh\ncurl \\\n  -u token:$DATABRICKS_TOKEN \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"dataframe_records\": [\n      {\"json_blob\": \"{\\\"hover_time\\\": [5.5, 2.3, 10.3]}\"}\n    ]\n  }' \\\n  <host>/serving-endpoints/<endpoint_name>/invocations\n```\n\nこのモデルはオンデマンドで平均ホバー時間を計算し、特徴量として平均ホバー時間を用いてモデルをスコアリングします。\n\n[シンプルなデモ](https://www.databricks.com/wp-content/uploads/notebooks/on-demand-simple-demo.html)\n\n# 洗練された例 - レストランへの距離\n\nこの例では、レストランのレコメンデーションモデルは、ユーザーの位置とレストランのIDを含むJSON文字列を受け取ります。レストランの位置は、オンラインストラに公開されている事前にマテリアライズされた特徴量から検索され、オンデマンド特徴量はユーザーからレストランへの距離を計算します。この距離はモデルへの入力として引き渡されます。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-807-blogs-img-6.png)\n\nこの例には、レストランの位置の検索し、このレストランからユーザーへの距離を計算するために後続の変換処理を行うことに注意してください。\n\n[レストランのレコメンデーションのデモ](https://www.databricks.com/wp-content/uploads/notebooks/on-demand-restaurant-recommendation-demo.html)\n\n# より詳細は\n\nAPIのドキュメントや追加のガイドについては、[コンピュート Python ユーザー定義関数を使用したオンデマンド機能](https://docs.databricks.com/ja/machine-learning/feature-store/on-demand-features.html)をご覧ください。\n\nDatabricksと共有したいユースケースがありますか？[on\\-demand\\-features@databricks\\.com](mailto:on-demand-features@databricks.com)にコンタクトしてください。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksのUnity CatalogはどのようにAmgenの企業規模のデータガバナンスを実現したのか", "text": "[How Databricks Unity Catalog Helped Amgen Enable Data Governance at Enterprise Scale \\| Databricks Blog](https://www.databricks.com/blog/how-databricks-unity-catalog-helped-amgen-enable-data-governance-enterprise-scale)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n> この記事はAmgenのSenior Manager, Information SystemsであるJaison DominicとZS AssociatesのDirector of Architecture and EngineeringであるLakhan Prajapatiによるものです。\n\n世界最大の独立系バイオテック企業である[Amgen](https://www.amgen.com/)は長きにわたりイノベーションの代名詞となっています。40年間、我々は新たな創薬プロセスでのパイオニアであり、命を救う薬品を開発し、世界中の数百万人の命にポジティブなインパクトを与えています。\n\n我々のビジネス戦略においてデータとAIは重要なものとなっています。我々の企業にある膨大なデータを認識することで我々のビジョンは、データ分析者がセルフサービスのガバナンス能力を通じてアクセス可能となるようなデータドリブンの企業を確立するというものでした。モダン化の追求の中で、我々のデジタルトランスフォーメーションのジャーニーの基盤として注意深く[Databricksレイクハウスプラットフォーム](https://www.databricks.com/jp/product/data-lakehouse)を選定しました。この戦略的な意思決定によって、様々な部門においてデータとAIの真のポテンシャルを解放することができ、オペレーションの効率性を改善し、創薬を加速しました。我々は、様々な領域のデータレイクを制限がありセンシティブなデータで継続的に補強しているので、我々のインパクトはさらに拡大しています。\n\nさらに、我々は自身の取り組みを補うために、強化された[データガバナンス](https://www.databricks.com/discover/data-governance)を必要としていることに気づきました。これまでのデータガバナンスソリューションは複雑で、管理が大変で、きめ細かいアクセスコントロールができませんでした。これらの障害に取り組み、企業に我々のガバナンス能力を広く行き渡らせるために、最近になって我々のガバナンスプロセスに[Databricks Unity Catalog](https://www.databricks.com/jp/product/unity-catalog)をインテグレーションしました。このインテグレーションは、我々のジャーニーにおける重要なマイルストーンであり、きめ細かいアクセスコントロールを提供しつつも管理をシンプルにし、ユーザーフレンドリーである堅牢なソリューションを提供することで、データガバナンスを支援します。\n\n本日は、皆様が我々のジャーニーから学びを得、ご自身のビジネス戦略に適用することになればと思い、これまでの我々の進捗と成功を共有します。\n\n# ガバナンスでのIAMロールの利用は管理が困難で、きめ細かいアクセスコントロールに欠けていました\n\nAmgenは、コンプライアンスが我々のオペレーションの基盤である高度に規制された業界でビジネスを行っています。我々は、規制されたあるいはセンシティブなデータの全てに対する適切なガバナンスや監査可能性が非常に重要であることを理解しています。我々のエンタープライズデータレイクプロジェクトにおいては、すべてのAmgenユーザーが利用可能なデータにアクセスできるようになるデータの民主化が元々の目標でした。しかし、データレイクにセンシティブなデータを取り込むことで、より堅牢なデータアクセスのガバナンスの重要性が明らかになりました。\n\nこれまでは、エンタープライズデータカタログとしてAWSのGlue、ロールベースのアクセス管理ではAWSのidentity and access management (IAM)に依存していました。これには、特定のユースケースに合わせた個別のIAMロールの作成、固有のクラスターへの関連付けが含まれていました。しかし、膨大な数のグループの管理とクラスターリソースへの関連付けは、大きな課題を生み出すことになりました。さらに、IAMロールはストレージへのアクセスのみを制御するので、メタデータには誰でもアクセスできてしまいました。きめ細かいアクセスコントロールの欠如によって監査は複雑なタスクとなり、データアクセスを監査し、効果的にクエリーを実行する我々の能力の妨げとなっていました。\n\nこれらの課題に取り組むために、ユーザーレベルのアクセスやユーザーの属性ベースのアクセス制御に移行すべき必要性を認識しました。例えば、ユーザーはコストセンターのような属性に割り当てられ、Financeのデータは割り当てられたコストセンターに基づいて制御されることになるでしょう。しかし、IAMによるユーザー属性ベースのアクセスコントロールの実装には、膨大な数のロールの作成が必要となり、非常に大きな管理負荷が発生することになります。\n\n我々はいくつかのoff-the-shelfのガバナンスツールを評価しました。いくつかのツールはデータベースレベルでのテーブル管理のように直近の要件を満たしていましたが、EDW (Finance)やWorkday (HR)のような行動に制限されたデータドメインでは不適切であることがわかりました。さらに、Databricksクラスター上のこれらのツールをバイパスして潜在的な脆弱性を引き起こしてしまうことや、すべてのクラスターに対して包括的なカバレッジを保証し、ソリューションをスケールさせることに関して懸念を持ちました。加えて、特定のクラスターでのプラグインを維持管理することは、スクリプトの一貫性や実行中のメンテナンスの観点での課題を引き起こすことになりました。\n\n# Unity Catalogに移行することでアクセス管理がシンプルになり、コンプライアンスへの非準拠やセキュリティ事故が撲滅されました\n\n現在、我々のユースケースの90%はDatabricks上にあります。このため、長期においてはDatabricksネイティブのガバナンスソリューションが必要であると感じました。この方向に一歩を踏み出すために、Unity Catalogに目を向けました。\n\nUnity Catalogを導入することで、いくつかのメリットをすぐに享受することができました。\n\n- 初めに、少なくとも**120+のIAMロール**を作成、管理する必要がありませんでした。Unity CatalogやUnity Catalogが提供するAPIを通じてアクセスをコントロールできます。すべてはアクセスコントロールリスト(ACL)やダイナミックビューによって管理されます。これによって、数百のIAMロールから**わずか1つあるいは2つのプリンシパルIAMロール**に移行しました。\n- 我々が理解した二つ目のメリットは、容易な監査可能性です。Unity CatalogのALCの編集はIAMポリシーのパーシングよりもはるかに簡単であり、誰が何にアクセスできるのかを特定できます。これによって、**機能に対する監査の工数を50%削減しています**。クエリー履歴によって、ある特定の時点で誰がどのデータにアクセスしたのかを確認することができます。\n- Unity Catalogは管理が容易です。ユーザーとロールベースのアクセスコントロールによって、専用のクラスターベースのアクセスから共有クラスタープールに移行することができ、**Databricksのコストを10-20%削減できました**。\n- 中央の場所で全てを統合することで、**シームレスな機能横断のデータ分析を可能**とし、Databricksエコシステムとの密なインテグレーションは真の差別化要因を提供します。\n\n現時点で、500程度のオブジェクト(増加中です)をUnity Catalogでマッピングしており、ACLで管理しています。Unity Catalogに移行して以来、データガバナンスやコンプライアンスへの準拠に対してさらなる自信を持つことができています。より多くの機能をオンボーディングし始めることで、これらのメリットが増大することを予想しています。\n\n# 我々のDatabricks Unity Catalogによる成功をさらに推し進めます\n\nこれは我々のジャーニーの始まりに過ぎません。今後我々はより大きなビジョンを持っており、AWS Glueから大部分のデータ資産をUnity Catalogに移行するというゴールに向けて、推進するための戦略を入念に組み立てています。我和の企業におけるデータランドスケープには数多くのデータドメイン、数千のデータベース、数百万のオブジェクトが含まれており、Unity Catalogが我々のデフォルトのカタログになる予定です。この戦略的なシフトは、我々のデータエコシステムを改善し、統合することになり、我々の広範囲なデータ資産のシームレスな管理と探索を可能にします。\n\n観測可能性を強化するために、我々はUnity Catalogのデータリネージ機能を活用し、データ作成における自信を確立し、我々のデータ資産におけるセンシティブなデータの利用を追跡する予定です。さらに、外部データ共有でUnity CatalogでDelta Sharingを活用することについて前向きに検討しています。現時点では内部でのデータ共有ですが、我々はアクティブにコレクションとDelta Sharingを通じた外部データの共有を探索しています。\n\nまとめとなりますが、Unity Catalogのインテグレーションは、FinanceやWorkdayを含むAmgenの厳密に制限されたデータセットに対する正確で複雑なガバナンスポリシーを実装する我々の能力を強化しました。この特筆すべき偉業によって、我々のデータエンジニアリング部門で非常に大きなやる気が発生し、Unity Catalogが中央のメタストアやアクセス管理サービスとして動作することで、我々のデータプラットフォームへの投資が増加しました。来年を見据えると、Unity CatalogがAmgenにおけるアプリケーションデータ利用の80%以上を促進し、10,000以上のアクティブユーザーの広大なユーザーベースのメリットになることを予想しています。このシフトによって、監査やアクセス管理において60-80%の効率性の改善の達成する見込みであり、我々の分析能力の拡張を刑ぞkすることで、我々の企業が成功に向かっていくものと固く信じています。\n\n詳細はData and AI Summit 2023のプレゼンテーションをご覧ください。\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/EfOfUc0RiJE?si=LYoaXnL9aIcgjEu_\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksジョブのif/else条件タスクを試す", "text": "これは嬉しいエンハンス。\n\nhttps://docs.databricks.com/ja/workflows/jobs/conditional-tasks.html#add-branching-logic-to-your-job-with-the-ifelse-condition-task\n\nこれまでにも、依存するタスクの成功・失敗に基づいて判断を行う [`Run if`の判定機能](https://qiita.com/taka_yayoi/items/0dd9a25629c3464e36ac#databricks%E3%82%B8%E3%83%A7%E3%83%96%E3%81%A7%E6%9D%A1%E4%BB%B6%E4%BB%98%E3%81%8D%E3%81%A7%E3%82%BF%E3%82%B9%E3%82%AF%E3%82%92%E5%AE%9F%E8%A1%8C%E3%81%99%E3%82%8B)がありましたが、これにさらに機能が追加される形になります。[ジョブやタスクのパラメーター変数](https://docs.databricks.com/ja/workflows/jobs/parameter-value-references.html)や[タスクバリュー](https://docs.databricks.com/ja/workflows/jobs/share-task-context.html)に基づいて処理を切り替えることができます。\n\nhttps://docs.databricks.com/ja/workflows/jobs/conditional-tasks.html#run-if-condition-options\n\n早速動かしてみます。\n\n# ノートブックの準備\n\nノートブックを3つ用意します。\n\n最初のノートブックではタスクバリュー`gender`に値を設定します。\n\n```py:start\ndbutils.jobs.taskValues.set(key = 'gender', value = 'Male')\n```\n\nタスクバリューの値に応じて呼び出されるノートブックを二つ用意します。\n\n```py:male\nprint(\"男性です\")\n```\n\n```py:female\nprint(\"女性です\")\n```\n\n# ジョブの設定\n\nジョブを構成します。最初のノートブックをタスクとして追加します。\n![Screenshot 2023-11-03 at 17.14.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/cbced428-43f9-e551-774f-c4dd47ded335.png)\n\nタスクを追加して、一番下に表示される`if/else condition`を選択します。\n![Screenshot 2023-11-03 at 17.14.38.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/af4e6a98-1c0d-e269-679b-ff7eee53fc2e.png)\n\n判定ロジックを記述します。タスクバリューは`{{tasks.<タスク名>.values.<タスクバリューのキー>}}`でアクセスできます。\n![Screenshot 2023-11-03 at 17.16.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e1cdeae5-6c78-a526-7948-63b780c82e94.png)\n\n呼び出されるノートブックをタスクとして追加します。True/Falseのどちらに接続するのかは**依存先**で選択します。\n![Screenshot 2023-11-03 at 17.17.44.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a6785fed-c880-4bc7-6234-bd0f01ed4fd3.png)\n![Screenshot 2023-11-03 at 17.18.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/389e8cd7-f3dc-75ad-4e49-a5b5aaf28346.png)\n\n# ジョブの実行\n\nジョブを実行します。実行結果からどのフローが実行されたのかを簡単に確認できます。\n![Screenshot 2023-11-03 at 17.33.14.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/064fd2c2-798d-fdb1-c530-393e07a756f0.png)\n![Screenshot 2023-11-03 at 17.33.53.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ce93094e-8b74-05d2-faa1-884bd24a7eb4.png)\n\nノートブック`start`を編集します。\n\n```py:start\ndbutils.jobs.taskValues.set(key = 'gender', value = 'Female')\n```\n\n(当たり前ですが)分岐が変化してます！\n![Screenshot 2023-11-03 at 17.37.35.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1e588042-4b19-a422-427e-56d70b386998.png)\n![Screenshot 2023-11-03 at 17.38.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1397fda2-d83f-5680-bb4a-4bac05809052.png)\n\nジョブの実行画面でも分岐を一覧することができます。\n![Screenshot 2023-11-03 at 17.39.53.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d2316eb3-27ea-213b-d4a6-e98df65cadd3.png)\n\n\nこれまで以上にジョブで柔軟にロジックを構成することができます。是非ご活用ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "CyberAgentLM2-7B-ChatをDatabricksで動かしてみる", "text": "最近、大規模言語モデル触ってなかったのでLLM欲が高まってました。\n\nhttps://www.cyberagent.co.jp/news/detail/id=29479\n\n> 今回公開したモデルは、日本語および英語データで事前学習を行なった70億パラメータのベースモデルであるCyberAgentLM2-7Bと、チャット形式でのチューニングを行ったCyberAgentLM2-7B-Chatの2種類です。CyberAgentLM2-7B-Chatは入出力の長さとして32,000トークンに対応しており、日本語の文章として約50,000文字を一度に処理することができます。\n>\n> モデルは商用利用可能なApache License 2.0で提供されており、以下のURLからご利用いただけます。\n\n\nモデルはこちら。\n\nhttps://huggingface.co/cyberagent/calm2-7b-chat\n\nいつものGPUクラスターで。\n![Screenshot 2023-11-02 at 21.16.54.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/fee45310-f93f-cb98-79b1-a19804460355.png)\n\ntransformerなどは最新の`4.34.1`出ないと動かないのでアップグレードします。\n\n```py\n%pip install -U transformers accelerate flash_attn\ndbutils.library.restartPython()\n```\n\n```py\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n\nassert transformers.__version__ >= \"4.34.1\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\"cyberagent/calm2-7b-chat\", device_map=\"auto\", torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"cyberagent/calm2-7b-chat\")\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\nprompt = \"\"\"USER: AIによって私達の暮らしはどのように変わりますか？\nASSISTANT: \"\"\"\n\ntoken_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\noutput_ids = model.generate(\n    input_ids=token_ids.to(model.device),\n    max_new_tokens=300,\n    do_sample=True,\n    temperature=0.8,\n    streamer=streamer,\n)\n```\n結果はストリーミングされて出力されます。\n![Screenshot 2023-11-02 at 21.17.34.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d5c130ba-3243-8b52-184d-79494b1d2d8b.png)\n![Screenshot 2023-11-02 at 21.17.58.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/30242c31-e672-b8cd-e5c9-4e0b329ceab2.png)\n\n```\n人工知能（AI）は、コンピューターシステムがタスクを自動化するプロセスです。 AIは、データとアルゴリズムを用いて、人間が直面する問題に対する解決策を自動的に生成します。 これにより、AIは人間の生活の多くの側面で影響を与える可能性があります。\n\n以下は、AIが人間の生活にどのように影響するかの例です。\n\n1. 仕事\nAIによって自動化される仕事は増えています。 たとえば、データ入力や会計処理などの単純なタスクは、AIによって自動化することができます。 また、AIによって自動化されることで、人間はより高度なタスクに集中することができます。\n2. 健康\nAIは、医療分野でますます活用されています。 AIは、医療診断や治療法の選択を行うことができます。 AIによって、より正確な診断と治療法の選択ができるようになります。\n3. 交通\nAIによって、自動運転車が開発されており、人間が運転しなくても車が移動できるようになっています。 これにより、交通事故が減少すると期待されています。\n4. エンターテインメント\nAIによって、人間に近い音声や表情を持つロボットの開発が進んでいます。 これらのロボットは、人間とのコミュニケーションをより自然なものにします。\n5. エンターテインメント\nAIによって、音楽の制作やゲームの開発などが自動化されています。 これにより、音楽はより多く創造性があり、ゲームはよりエキサイティングなものになる可能性があります。\n\n以上がAIによって人々の生活が変わる可能性のあるものの例です。 AIは、人間の生活の多くの側面で影響を与える可能性がありますが、AIはあくまでも人間によって\n```\n\n7Bでこれはすごいですね(肌感覚)。\n\n他の質問も。\n\n```py\nprompt = \"\"\"USER: 日本の少子高齢化にはどのように対策すべきですか？\nASSISTANT: \"\"\"\n\ntoken_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\noutput_ids = model.generate(\n    input_ids=token_ids.to(model.device),\n    max_new_tokens=300,\n    do_sample=True,\n    temperature=0.8,\n    streamer=streamer,\n)\n```\n```\n少子高齢化により、労働力人口の減少と労働力不足が懸念されています。そこで、以下のような対策が考えられます。\n\n1. 女性や高齢者の活用： 労働力人口の減少を補うため、女性や高齢者の活用が重要となります。女性がより働きやすく、高齢者がより活躍しやすい環境を整備することで、労働力人口の確保が図られます。\n2. 外国人労働者の受け入れ： 労働力人口の減少を補うため、外国人労働者の受け入れも必要です。外国人の高度な技術や知識を活用することで、労働力不足を補うことができます。\n3. ワーク・ライフ・バランスの推進： 子育てや介護との両立が可能となるよう、長時間労働を是正し、ワーク・ライフ・バランスを推進することで、働きやすく、働きがいのある環境を整備することが重要です。\n4. 科学技術の活用： 人工知能やロボット等の先端技術を活用し、業務の効率化を図ることも必要です。これにより、労働力人口の減少を補うことができます。\n5. 社会保障制度の充実： 高齢者の医療費や介護費が増大しているため、社会保障制度の充実が必要です。医療や介護にかかる費用の軽減を図ることも必要です。\n\n以上のように、日本の少子高齢化に対しては、女性や高齢者の活用、外国人労働者の受け入れ、ワーク・ライフ・バランスの推進、科学技術の活用、社会保障制度の充実等、様々な対策が必要です。\n```\n\n```py\nprompt = \"\"\"USER: 日本で生成AIの活用は進みますか？\nASSISTANT: \"\"\"\n\ntoken_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\noutput_ids = model.generate(\n    input_ids=token_ids.to(model.device),\n    max_new_tokens=300,\n    do_sample=True,\n    temperature=0.8,\n    streamer=streamer,\n)\n```\n```\n日本で生成AIの活用は進むことが予想されます。\n\n生成AIは、データから学習して新しいデータを生成することができるAIです。この特性を活かして、さまざまな業界での活用が期待されています。\n\n例えば、製造業では、生産ラインで生成AIを活用して、不良品の検出を行うことができます。これにより、不良品の混入率を大幅に下げることが可能になります。\n\nまた、AIによる自動翻訳技術も日々向上しており、翻訳業務の負荷を下げることができます。これにより、他の業務に時間を割けるようになり、生産性を高めることができます。\n\nさらに、生成AIは、画像や動画など、テキスト以外のデータに対しても適用が可能です。そのため、マーケティングの施策立案や、医療画像の診断など、様々な業界での活用が進むことが予想されます。\n```\n\nいや、これすごいですね。ChatGPT的な使い方であれば十分のような。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksでKaggleのTitanicコンペティションにチャレンジしてみる", "text": "こちらにチャレンジしてみます。Kaggleのアカウント作成していましたが、一つもチャレンジしたことなかったのでした。お恥ずかしい。\n\nhttps://www.kaggle.com/competitions/titanic\n\n> このコンペティションはシンプルです: どの乗客がTitanic沈没を生き延びたのかを予測するモデルを作成するために機械学習を用います。\n\nチャレンジの項目も翻訳します。\n\nタイタニックの沈没は、歴史上最も悪名高い沈没事故の一つです。\n\n1912年4月15日、処女航海の勝っていで、「不沈」と広く知られていたRMS Titanicは氷山との衝突後に沈没しました。不幸なことに、全員分の救命ボートが無く、結果として2224人の乗員乗客のうち1502人が亡くなりました。\n\n生死を分けた要因にはある程度の運も含まれましたが、あるグループの人々が他のグループよりも生存した可能性があるようです。\n\nこのチャレンジでは、乗客データ(氏名、年齢、性別、社会・経済的なクラス)を用いて「どのような人々がより生存する可能性が高かったのか？」という質問に回答する予測モデルを構築することになります。\n\n# チュートリアルの活用\n\nここまで読み進めたら、チュートリアルがありましたのでこちらをウォークスルーします。\n\nhttps://www.kaggle.com/code/alexisbcook/titanic-tutorial/notebook\n\n右上の3点リーダーをクリックして**Download code**をクリックします。`titanic-tutorial.ipynb`がダウンロードされます。\n![Screenshot 2023-11-02 at 14.31.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1e2c4e9a-ddea-4bd1-b3a3-14953711e909.png)\n\nチャレンジページの**Data**タブをクリックし、画面を下にスクロールすると表示される**Download All**をクリックします。`titanic.zip`がダウンロードされます。\n![Screenshot 2023-11-02 at 14.33.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b471854c-cf94-5de9-15ac-6cca44999df6.png)\n\n# データのアップロード\n\n[ボリューム](https://qiita.com/taka_yayoi/items/453026daacf71ba5662f)の機能を使ってクイックにzipファイルをDatabricksに取り込みます。\n\nボリュームを選択して**このボリュームにアップロード**をクリックして、`titanic.zip`をドラッグ&ドロップします。\n![Screenshot 2023-11-02 at 13.52.12.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/81ba28ce-e0d0-b581-7c49-d524a1796c83.png)\n![Screenshot 2023-11-02 at 13.52.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ace0d3b1-0752-e09d-cf08-c8297ac2be63.png)\n\n# ノートブックのインポート\n\nDatabricksワークスペースでフォルダを作成して、メニューから**インポート**を選択します。`titanic-tutorial.ipynb`をドラッグ&ドロップします。\n![Screenshot 2023-11-02 at 14.38.13.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/68e1fe89-0d4e-c15b-0643-c00253725af2.png)\n\n# zipファイルの解凍\n\n上でインポートしたノートブックにセルを追加してもいいですが、ここでは新規ノートブックを作成します。`%sh`マジックコマンドを使ってシェルコマンドを実行していきます。\n\n```sh:shell\n%sh\nls /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic.zip\n```\n```\n/Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic.zip\n```\nファイルを解凍します。\n```sh:shell\n%sh \nunzip /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic.zip -d /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic/\n```\n```\nArchive:  /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic.zip\n  inflating: /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic/gender_submission.csv  \n  inflating: /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic/test.csv  \n  inflating: /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic/train.csv  \n```\n```sh:shell\n%sh\nls /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic\n```\n3つのcsvファイルが解凍されました。\n```\ngender_submission.csv\ntest.csv\ntrain.csv\n```\n\n\n# ノートブックの実行\n\nインポートした`titanic-tutorial`ノートブックを実行していきます。\n\nファイルを読み込みます。`train.csv`はトレーニング用のデータセットです。\n\n```py\ntrain_data = pd.read_csv(\"/Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic/train.csv\")\ntrain_data.head()\n```\n![Screenshot 2023-11-02 at 14.42.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e6406d22-1e4f-d6e9-7e02-410489efc0a8.png)\n\nせっかくなので、Databricksの`display`関数を活用します。\n```py\ndisplay(train_data)\n```\nよりリッチな一覧が表示されます。\n![Screenshot 2023-11-02 at 14.43.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d38a3753-f223-1968-255f-140c9fc2af6f.png)\n\nこれで終わりではありません。一覧の上にある **+** をクリックして**データプロファイル**を選択します。\n![Screenshot 2023-11-02 at 14.43.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/122353d2-06dd-74ad-9613-f80c445f5b69.png)\n\nこれだけで、データの概要を掴む助けとなるデータプロファイルが生成されます。\n![Screenshot 2023-11-02 at 14.44.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2e19ba17-21c8-ba4e-5f36-f8fe2ffd7193.png)\n\nさらにデータの可視化も行います。 **+** をクリックして**可視化**を選択します。\n\nチケットのクラスと年齢のグラフを設定します。\n![Screenshot 2023-11-02 at 14.55.10.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/942480a8-459f-8659-dec0-6c3ccf662f5f.png)\nこれだけでグラフが追加されます。\n![Screenshot 2023-11-02 at 14.55.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a5f7a7e9-0f62-5ec0-bdd3-4025cde7628a.png)\n\nテストデータも確認します。\n\n```py\ntest_data = pd.read_csv(\"/Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic/test.csv\")\ntest_data.head()\n```\n![Screenshot 2023-11-02 at 14.57.12.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7c159799-15a0-46cb-74b1-2754d0120222.png)\n\nパターンを探索します。\n\n```py\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n```\n```\n% of women who survived: 0.7420382165605095\n```\nこちらも可視化で確認することができます。\n![Screenshot 2023-11-02 at 15.02.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ca2aa9a3-a4ab-271d-4410-1f16b233685f.png)\n\nRandom Forestで分類器を構築します。\n\n```py\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('/Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/titanic/submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n```\n\nボリュームに予測結果が保存されます。さらにDatabricksでは、[MLflow](https://qiita.com/taka_yayoi/items/1a4e82f7e20c56ba4f72)によってモデルが自動で記録されます。\n![Screenshot 2023-11-02 at 15.03.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/10fa73d5-b625-a68f-972b-95c9d035186f.png)\n![Screenshot 2023-11-02 at 15.06.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5c97b9b2-6c70-9a29-70d7-dd46ba48d449.png)\n\n# 結果のダウンロードと提出\n\nボリュームから`submission.csv`をダウンロードします。\n![Screenshot 2023-11-02 at 14.17.51.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c4bc1f07-f8fa-2e70-90aa-3d54d0550969.png)\n![Screenshot 2023-11-02 at 14.18.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2531411e-9f93-742d-d342-a261258caf03.png)\n\nKaggleで**Submit Predictions**をクリックします。`submission.csv`をアップロードします。\n![Screenshot 2023-11-02 at 14.19.01.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ad6e879b-9e4a-250d-f243-351ffd82704c.png)\n\n提出できました！\n![Screenshot 2023-11-02 at 15.11.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9d53cf38-5272-0dce-64d4-efeb881cf70c.png)\n\nこの後はおそらく、パラメーターチューニングなどで精度を改善していくことになるのですが、そういった場合でもMLflowが活躍します。可視化機能含めて是非ご活用ください！\n\nhttps://qiita.com/taka_yayoi/items/44a5bb667e4f6dc7491d\n\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksのワークスペースファイルを有効化・無効化する", "text": "こちらで触れたワークスペースファイル、便利な機能ですが、セキュリティ要件によってはオフ西台頭ニーズもあるかと思います。\n\nhttps://qiita.com/taka_yayoi/items/d61a77747ec7325d6526\n\nこちらのドキュメントには`管理者が REST API を使用して無効にすることができます。`との記載があります。\n\nhttps://docs.databricks.com/ja/files/workspace.html\n\nしかし、具体的な方法が記載されておらず右往左往しました。\n\nヒントはこちらにありました。\n\nhttps://docs.databricks.com/ja/repos/enable-disable-repos-with-api.html\n\nこちらのノートブックを修正して実行することで、ワークスペースファイルを有効・無効にすることができます。ドキュメントチームにはフィードバックしておきます。\n\nデフォルトではワークスペースファイルは有効化されているので、以下のように`py`ファイルなどを作成することができます。\n![Screenshot 2023-11-01 at 16.07.28.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/006563ae-f91a-b336-1889-158f1e9e30e6.png)\n![Screenshot 2023-11-01 at 16.07.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/db5ce3a0-d347-a858-323c-32092a9ef32a.png)\n\n# ワークスペースファイルの無効化\n\ndatabricks-sdkを使いますが、REST API呼び出しでもいけるはずです。\n\n```py\n%pip install databricks-sdk --upgrade\ndbutils.library.restartPython()\n```\n\nAPIクライアントを初期化します。\n\n```py\nfrom databricks.sdk.core import ApiClient\n\nclient = ApiClient()\n```\n\nキー`enableWorkspaceFilesystem`を指定して、現在の設定を確認します。\n\n```py\nclient.do(\"GET\", \"/api/2.0/workspace-conf\", {\"keys\": \"enableWorkspaceFilesystem\"})\n```\n\n有効化されています。\n```json\n{'enableWorkspaceFilesystem': 'true'}\n```\n\n無効化します。\n\n```py\nclient.do(\"PATCH\", \"/api/2.0/workspace-conf\", body={\"enableWorkspaceFilesystem\": \"false\"}, headers={\"Content-Type\": \"application/json\"})\n```\n\n```json\n{}\n```\n\n設定を反映させるために、ブラウザをリロードします。\n\nメニューから**ファイル**が表示されなくなります。\n![Screenshot 2023-11-01 at 16.11.12.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f4109742-8dbb-c0b9-391b-a7d0ec6b9c6c.png)\n\n# ワークスペースファイルの有効化\n\n再度有効化する際には、キー`enableWorkspaceFilesystem`を`true`に設定します。\n\n```py\nclient.do(\"PATCH\", \"/api/2.0/workspace-conf\", body={\"enableWorkspaceFilesystem\": \"true\"}, headers={\"Content-Type\": \"application/json\"})\n```\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "MLflow 2.8における審判としてのLLMの発表、およびRAGアプリケーションにおけるLLM評価のベストプラクティスパート2", "text": "[Announcing MLflow 2\\.8 LLM\\-as\\-a\\-judge metrics and Best Practices for LLM Evaluation of RAG Applications, Part 2 \\| Databricks Blog](https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n本日、時間とコストを削減しつつも、人間による判断と近いメトリクスを提供するLLM-as-a-judgeメトリクスの[MLflow 2.8](https://www.mlflow.org/docs/2.8.0/index.html)におけるサポートを発表できることを嬉しく思います。[前回のレポート](https://qiita.com/taka_yayoi/items/7efb0df8024daf491b9e)では、[DatabricksドキュメントのAIアシスタント](https://docs.databricks.com/en/index.html)において、どのようにLLM-as-a-judgeテクニックが効率の改善、コストの削減を行い、人間のスコアと80%以上の一貫性を維持することで、時間(人間による2週間の作業から、LLM審判によって30分に)、コスト(タスクごとのコストが`$20`から`$0.2`に)の大きな削減に寄与しているのかに関してケーススタディを議論しました。また、以下のパート2ではRAG(Retrieval Augmented Generation)アプリケーションにおけるLLM-as-a-judge評価のベストプラクティスに関して[前回のレポート](https://qiita.com/taka_yayoi/items/7efb0df8024daf491b9e)の続きを説明します。皆様自身のRAGアプリケーションのパフォーマンスを評価し、チューニングするために、データクリーニングと組み合わせて同様の方法論をどのように適用できるのかをウォークスルーします。前回のレポートと同様、LLM-as-a-judgeはLLMベースのアプリケーションの効果を継続する際に必要となる評価テクニックのスイートにおいて重要なツールとなります。多くの状況では、スイートスポットを表現するものと考えます: 自動かつ高速、低コストで(チャットbotのレスポンスのように)構造化されていないアウトプットを評価します。このように考えると、このテクニックは、遅く高コストですがモデルの評価における黄金律を表現する人間の評価の同僚として価値のあるものと考えることができます。\n\n評価においてサードパーティのLLMサービス(OpenAIなど)を使用する場合、LLMサービスの利用規約の対象になる場合があります。\n\n# MLflow 2.8: 自動評価\n\nLLMコミュニティでは、自動評価のための「審判としてのLLM」の活用を探索しており、我々はそれらの[理論](https://arxiv.org/abs/2306.05685)を[プロダクションのプロジェクト](https://qiita.com/taka_yayoi/items/7efb0df8024daf491b9e)に適用しました。そして、それぞれの評価指標に対して単一の評価例と、GPT、MPT、Llama2モデルファミリーのような最先端のLLMを用いた自動評価を行うことで、コストと時間を大きく削減できることがわかりました。MLflow 2.8では、LLM評価のためのパワフルでカスタマイズ可能なフレームワークを導入します。GenAIメトリクスと評価例をサポートするように、[MLflow Evaluation API](https://mlflow.org/docs/latest/models.html#evaluating-with-llms)を拡張しました。公正性、解答の正しさ、解答の類似性のように、デフォルトの審判としてGPT-4を使用するGenAIメトリクスとともに、毒性、レーテンシー、トークン数などのメトリクスをすぐに利用できます。GenAIメトリクスにおいても、常にカスタムのメトリクスをMLflowに追加することができます。いくつかの例を通じて実際にMLflow 2.8を見ていきましょう！\n\nLLM-as-a-judgeテクニックを用いてカスタムGenAIメトリックを作成する際、審判として必要とするLLMを選択し、評点スケールを提示し、スケールにおけるそれぞれの評点の例を提示しなくてはなりません。MLflow 2.8で`Professionalism`のGenAIメトリックを定義する例を以下に示します:\n\n```py\nprofessionalism = mlflow.metrics.make_genai_metric(\n    name=\"professionalism\",\n    definition=(\n        \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is \"\n        \"tailored to the context and audience. It often involves avoiding overly casual language, slang, or \"\n        \"colloquialisms, and instead using clear, concise, and respectful language.\"\n    ),\n    grading_prompt=(\n        \"Professionalism: If the answer is written using a professional tone, below are the details for different scores: \"\n        \"- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for \"\n        \"professional contexts.\"\n        \"- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in \"\n        \"some informal professional settings.\"\n        \"- Score 3: Language is overall formal but still have casual words/phrases. Borderline for professional contexts.\"\n        \"- Score 4: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \"\n        \"- Score 5: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal \"\n        \"business or academic settings. \"\n    ),\n    examples=[professionalism_example_score_1, professionalism_example_score_2, professionalism_example_score_3, professionalism_example_score_4, professionalism_example_score_5],\n    model=\"openai:/gpt-4\",\n    parameters={\"temperature\": 0.0},\n    aggregations=[\"mean\", \"variance\"],\n    greater_is_better=True,\n)\n```\n\n前回のレポートで見たのと同じように、評価の例(上のスニペットの`examples`リスト)はLLMによって判断されるメトリックの精度改善の役に立ちます。Mlflow 2.8によって、評価例の定義が容易になります:\n\n```py\nprofessionalism_example_score_2 = mlflow.metrics.EvaluationExample(\n    input=\"What is MLflow?\",\n    output=(\n        \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps \"\n        \"you track experiments, package your code and models, and collaborate with your team, making the whole ML \"\n        \"workflow smoother. It's like your Swiss Army knife for machine learning!\"\n    ),\n    score=2,\n    justification=(\n        \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and \"\n        \"exclamation points, which make it sound less professional. \"\n    ),\n)\n```\n\n我々は皆様が必要とする共通のメトリクスを知っているので、MLflow 2.8ではすぐにいくつかのGenAIメトリクスを利用することができます。\"question-answering\"のようにお使いのアプリケーションの`model_type`を指定することで、MLflow Evaluate APIは自動で共通のGenAIメトリクスを生成します。以下の例のように\"Answer Relevance\"を指定するなどして、\"extra\"のメトリクスを追加することができます:\n\n```py\nfrom  mlflow.metrics.genai.metric_definitions import answer_relevance\n\nanswer_relevance_metric = answer_relevance()\n\neval_df = pd.DataFrame() # Index(['inputs', 'predictions', 'context'], dtype='object')\n\neval_results = mlflow.evaluate(\n    data = eval_df, # evaluation data\n    model_type=\"question-answering\",\n    predictions=\"predictions\", # prediction column_name from eval_df\n    extra_metrics=[answer_relevance_metric]\n)\n```\n\nさらにパフォーマンスを改善するには、すぐに利用できるこれらのGenAIメトリクスに対する審判モデルとプロンプトを変更することができます。以下に、EvaluationタブでビジュアルでGenAIのメトリクスをクイックに比較する助けになるMLflow UIのスクリーンショットを示します:\n![](https://cms.databricks.com/sites/default/files/inline-images/db-791-blog-img-1.png)\n\nまた、対応する`eval_results_table.json`を参照したり、さらなる分析のためにPandasデータフレームにロードすることができます。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-791-blog-img-2.png)\n\n# RAGアプリケーションへのLLM評価の適用: パート2\n\n我々の調査の次のラウンドでは、入力データの品質を改善することでパフォーマンスを改善できるかどうかを確認するために、[Databricks Documentation AI Assistant](https://docs.databricks.com/)のプロダクションアプリケーションを再訪します。この調査から、高い適切性、チャットbotの回答の可読性の改善、コスト削減とスピード改善につながるトークン数の削減を達成するクリーンなデータを自動で生成するワークフローを開発しました。\n\n## RAGアプリケーションにおける効果的な自動評価のためのデータクリーニング\n\nチャットbotレスポンスのパフォーマンスに対するデータ品質のインパクトと、パフォーマンスを改善するための様々なデータクリーニングのテクニックを探索しました。これらの知見は汎化でき、皆様のチームで効果的にRAGベースのチャットボットを評価する助けになるものと信じています:\n\n- データのクリーニングによって、最大 **+20%** (評点スケール1-5において3.58から4.59に) LLMが生成する回答の適切性を改善しました。\n- 予期しなかったデータクリーニングの効果として、必要トークン数が削減されることでコストを低減できるというものがあります。データクリーニングによって、コンテキストに対するトークン数が最大 **-64%** (インデックス作成されるデータにおける965538トークンからクリーニング後の348542トークンに) 削減されました。\n- 様々なデータクリーニングコードによって様々なLLMの挙動が改善されました。\n\n![](https://cms.databricks.com/sites/default/files/inline-images/db-791-blog-img-3.png)\n![](https://cms.databricks.com/sites/default/files/inline-images/db-791-blog-img-4.png)\n\n## RAGアプリケーションにおけるデータの課題\n\nRAGアプリケーションに対しては様々な[入力データタイプ](https://gpt-index.readthedocs.io/en/v0.6.3/reference/readers.html)が存在します: ウェブサイトのページ、PDF、Google Doc、Wikiページなど。業界において我々がよく目撃した、あるいは、お客様から伺うデータタイプはウェブサイトとPDFです。我々のDatabricks Document AI Assistantは、データソースとして公式のDatabricksドキュメント、ナレッジベース、Sparkドキュメントページを活用します。ドキュメントのウェブサイトは人間によって読むことができますが、LLMが理解するには困難なフォーマットかもしれません。以下に例を示します:\n![Screenshot 2023-11-01 at 12.41.22.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1dd17855-14cc-83c7-0a16-d00547a47b88.png)\n\nこれはマークダウンフォーマットとコードスニペットであり、言語オプションは対応するそれぞれの言語に対して理解しやすいUIを提供します。しかし、このUIがLLM向けに単なるマークダウンフォーマットに変換されると、コンテンツは複数回繰り返されるコードブロックに変換され理解が困難になります。このため、mpt-7b-chatに「別のデフォルトカタログ名をどのように設定するのか？」と尋ねると、このコンテキストに基づいて、コードブロックのシンボルの繰り返しである「\\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\``` \\```」という回答が示されます。他のケースでは、LLMは指示に従うことに失敗し、質問を繰り返すようになります。同様に、Webページには様々なアイコン、画像、優れたUIをレンダリングするための特殊文字が含まれることがあり、これらもまたLLMを混乱させます。他のアプローチとしてはフォーマットのためのマークアップを無視するようにプロンプトを変更するというものがあることに注意してください。しかし、我あれは特定のアプリケーション向けにさらにプロンプトをチューニングする予定であり、ここで過度に複雑にしたり、不安定性を持ち込むことを避けることにします。\n\n## データクリーニングによるパフォーマンスの改善\n\nマークダウンファイルやWebページから余計なフォーマッティングトークンを削除するためにLLM-as-judgeを活用し、繰り返し[クリーンアップコード](https://gist.github.com/suanmiao/7b1d82dfb94d457a8d521f3cc35613f5)を提示するクイックなワークフローを作成しました。以下にクリーンアップ前後の単一のドキュメントがどのように見えるのかの例を示していますが、両方において構造と意味を保持していることがわかります:\n![](https://cms.databricks.com/sites/default/files/inline-images/db-791-blog-img-7.jpg)\n\nまた、ドキュメントのクリーンアップによって、LLMのコンテキストウィンドウで使用されるトークンの数が劇的に削減されており、コストと時間を節約できることがわかります。データクリーニングの後で、MPT-7B-Chatの回答での改善を確認できています:\n![Screenshot 2023-11-01 at 14.29.14.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/842c12ea-04d3-a2db-9b04-31b6cd004341.png)\n\n# 自動評価のためにMLflow 2.8を使い始める\n\n我々の分析のパート2では、LLM-as-a-judgeを用いてRAGアプリケーションを評価するためにMLflow 2.8を活用しました。データクリーニングと自動評価を用いることで、皆様のRAGアプリケーションの要件を通じた検証を行うために、クイックかつ効率的に様々なLLMを比較、対照することができます。使い始める助けとなるいくつかのリソースを以下に示します:\n\n- [MLflow 2\\.8 Documentation](https://www.mlflow.org/docs/2.8.0/index.html)\n- [Tutorial for MLflow Evaluation for RAG](https://www.mlflow.org/docs/2.8.0/llms/llm-evaluate/notebooks/rag-evaluation.html)\n- [Tutorial for MLflow Evaluation for Q&A](https://www.mlflow.org/docs/2.8.0/llms/llm-evaluate/notebooks/question-answering-evaluation.html)\n- [LLM推論のパフォーマンスエンジニアリング：ベストプラクティス](https://qiita.com/taka_yayoi/items/7efb0df8024daf491b9e)\n- [Data cleaning code sample \\(gist\\)](https://gist.github.com/suanmiao/7b1d82dfb94d457a8d521f3cc35613f5)\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "GAになったDatabricksのUNDROPコマンドを試してみる", "text": "以前、こちらを翻訳した際にはパブリックプレビューでしたが、先週GA(正式提供)になりました。\n\nhttps://qiita.com/taka_yayoi/items/1bc3dd06fc4b0f845e68\n\n# UNDROP TABLEコマンドとは\n\n日本語マニュアルもあります。\n\nhttps://docs.databricks.com/ja/sql/language-manual/sql-ref-syntax-ddl-undrop-table.html\n\n> `UNDROP` コマンドは、 Unity Catalog にあるマネージ テーブルまたは外部テーブルが誤って削除または削除されるという問題に対処します。 デフォルトでは、このコマンドは、指定されたテーブル名のユーザーが所有する最後にドロップされたテーブルをアンドロップ (リカバリ) します。 親スキーマとカタログが存在する必要があります。 この機能は、7 日間の保持期間内に削除されたテーブルを回復することをサポートします。\n\n:::note\n**注意**\nテーブル権限、列仕様、プロパティなどのテーブルメタデータがリカバリされます。 主キー制約と外部キー制約は、 `UNDROP` コマンドでは回復されません。\n:::\n\n# テーブルの作成\n\nUnity Catalogでカタログ・スキーマを作成し、その中にテーブルを作成します。`drop_table_schema`というあからさまなスキーマ名にしています。\n![Screenshot 2023-10-31 at 20.38.03.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/93c09c16-9f2f-671f-82ed-bf76505d4474.png)\n\n# 削除済みテーブルの確認\n\nまず、この状態で削除済みのテーブルが無いか確認します。\n\nhttps://docs.databricks.com/ja/sql/language-manual/sql-ref-syntax-aux-show-tables-dropped.html\n\n```sql:SQL\nSHOW TABLES DROPPED IN takaakiyayoi_catalog.drop_table_schema;\n```\n\nこの時点では当該スキーマでテーブルは削除されていないのでレコードは返却されません。\n\n```\nクエリー結果が返されませんでした\n```\n\n# テーブルの削除\n\n```sql:SQL\nDROP TABLE takaakiyayoi_catalog.drop_table_schema.japan_cases_20220818;\n```\nテーブルが削除されました。\n![Screenshot 2023-10-31 at 20.41.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1dcf111c-9ad2-072e-e24f-065ea33dce63.png)\n\n再度、`SHOW TABLES DROPPED`を実行します。\n\n```sql:SQL\nSHOW TABLES DROPPED IN takaakiyayoi_catalog.drop_table_schema;\n```\n削除されたテーブルの情報を確認できます。\n![Screenshot 2023-10-31 at 20.42.34.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/34d5fd3a-afd2-e17c-442a-da016e557071.png)\n\n# テーブルのリカバリ\n\n```sql:SQL\nUNDROP TABLE takaakiyayoi_catalog.drop_table_schema.japan_cases_20220818;\n```\n\nテーブルが復旧しました！\n![Screenshot 2023-10-31 at 20.45.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/683451b2-9ef0-7b53-0f79-ef6ab24d79b7.png)\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksにおけるLLMOps", "text": "こちらのイベントで話した内容です。\n\nhttps://machine-learning15minutes.connpass.com/event/297997/\n\n資料のPDFはこちら。\n\nhttps://sajpstorage.blob.core.windows.net/yayoi/LLMOps%20on%20Databricks%20-%20ML15min.pdf\n\n今回の内容に関係するガバナンスのウェビナーを11月17日に開催します。講師は私です。\n\nhttps://events.databricks.com/20231027-unitycatalog-jp\n\n会社紹介。\n![Screenshot 2023-10-28 at 15.40.31.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7af65f94-3f58-15b5-c568-95c457c32429.png)\n\n# MLOpsとは\n\nMLOpsとは機械学習モデルライフサイクル全般を円滑にするために必要な体制・基盤・手法全般を意味します。\n\n> MLOps(**M**achine **L**earning **Op**eration**s**)とは、データサイエンスチーム、運用チームなど、機械学習モデルの構築・運用に関わるチームが協調し、円滑に機械学習モデルを運用していくための体制・基盤を構築すること、その概念全般を意味します。\n\nLLMOpsはこの考え方をLLMに拡張したものです。\n\n> LLMOps(**L**arge **L**anguage **M**odel **Op**eration**s**)とは、データサイエンスチーム、運用チームなど、LLMの構築・運用に関わるチームが協調し、円滑にLLMを運用していくための体制・基盤を構築すること、その概念全般を意味します。\n\nMLOpsはLLMで何が変わるのでしょうか？\n\n| LLMの特性 | MLOpsの示唆 |\n|:--|:--|\n| 様々な形態でLLMを利用可能:<ul><li>課金API経由での一般的なプロプライエタリモデル、OSSモデル<li>すぐに利用できるオープンソースモデル<li>特定のアプリケーションにファインチューンされたカスタムモデル<li>カスタムの事前トレーニング済みモデル<ul>  |  **開発プロセス:**<ul><li>インクリメンタルな開発<li>APIからスタートしカスタムモデルへ<ul> |\n| LLMは入力として自然言語のプロンプトを受け入れ:<ul><li>期待するレスポンスを得るためにプロンプトエンジニアリングが可能</ul>  | **開発プロセス:**<ul><li>LLMに問い合わせを行うテストテンプレートの設計が開発プロセスの重要な一部に</ul>**アーティファクトのパッケージング:**<ul><li>パッケージングされるアーティファクトとプロダクションにおけるプロンプトはモデルというよりパイプラインに</ul>  |\n|  LLMにはサンプルやコンテキストを伴うプロンプトを指定可能 |  **サービングのインフラストラクチャ:**<ul><li>適切なコンテキストの検索に使用されるベクトルデータベースのような外部ツール</ul> |\n| サードパーティのAPIプロバイダー経由でプロプライエタリモデルやOSSモデルを利用可能  | **APIガバナンス:**<ul><li>APIプロバイダーをスイッチできる選択可能性や柔軟性を持つために、APIガバナンスのための集中管理されたシステムを持つことが重要に</ul>  |\n| LLMは非常に大きなディープラーニングモデルであり、多くの場合、数Gバイトから数百Gバイトに  |  **サービングのインフラストラクチャ:**<ul><li>LLMのサービングにはGPUが必要<li>モデルを動的にロードする必要がある場合には、高速なストレージが重要に</ul> |\n| LLMにおいては、多くの場合、単一の「適切な」回答が存在しないため、従来のMLメトリクスを通じた評価が困難  | **人間のフィードバック:**<ul><li>LLMの評価、テストで必要になることが多い<li>将来的なファインチューニングのためには、テスト、モニタリングを含むMLOpsに直接組み込むことが重要に</ul>  |\n\nこれらの新たな要件に適合するために、多くの既存ツール、既存プロセスの修正は軽微です\n- 開発、ステージング、プロダクションの分離は変わりません\n- パイプラインやモデルをプロダクションに移行する際に、Gitのバージョン管理と[Unity CatalogにおけるMLflowモデルレジストリ](https://docs.databricks.com/ja/mlflow/models-in-uc.html)は依然として主要なパイプラインとなります。\n- データ管理に対するレイクハウスアーキテクチャは、効率性のために依然として適切で重要です。\n- 既存のCI/CDインフラストラクチャには変更はありません。\n- モデルトレーニングのためのパイプライン、モデル推論のためのパイプラインなどを用いた、\n- モジュール化されたMLOpsの構造は同じです。\n\n![Screenshot 2023-10-28 at 15.49.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/750053f9-c9e8-e2e0-d025-d6c366b5ab7b.png)\n\n# Lakehouse AI\n\nAIライフサイクルにおけるサイロ化の課題はLLMでも同様です。データセットの準備からモデルの開発、アプリケーションのデプロイメントでシステムがサイロ化していると効率は上がらず、エラーが混入します。\n![Screenshot 2023-10-28 at 15.49.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0b817678-2092-f109-c7d2-74542aadd2de.png)\n\n- モデル開発に必要なクリーンで信頼できるデータの取得が困難\n- インテグレーション欠如による開発の遅延\n- 完全なAIライフサイクルを制御、管理、追跡できないことによるリスクの増加\n\nこれらの課題を解決するために、我々はデータ中心アプローチのLakehouse AIを提供します。\n![Screenshot 2023-10-28 at 15.52.00.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d42976ef-07e4-55f0-8779-ce6114362d2a.png)\n\n- データプラットフォーム(レイクハウス)上に構築\n- 高速なデプロイメント\n- ビルトインのガバナンスとモニタリング\n\n# Lakehouse AIのコンポーネント\n\n![Screenshot 2023-10-28 at 15.52.45.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b36d560a-3580-1c39-857b-e0a89ebd6b4a.png)\n\n## Unity Catalogにおける特徴量エンジニアリング\n\n- 特徴量の効率的かつ高信頼な作成、管理、提供\n- Unity Catalogのテーブルをトレーニングやサービングにおけるソースとして活用可能\n- レイクハウスのDeltaテーブルに基づいて継続的に更新\n- モデルサービングや外部エンドポイントを使用しているアプリにオンデマンドのサービング\n\n## ベクトル検索(Vector Search)\n\n- Unity Catalogのデータから自動更新されるベクトル検索インデックスの作成\n- ビルトインのガバナンス、権限管理、リネージ\n- フルマネージド、サーバレス、すぐに利用可能\n- エンベディング生成においてMLflowやモデルサービングとインテグレーション\n\n## MosaicML\n\n- 大規模AIモデルのトレーニングを最大7倍高速、安価に\n- 簡素化され、スケーラブルでコスト効率の高い大規模AIモデルのトレーニング \n- ご自身のセキュアな環境で自分のデータを用いて、自分の生成AIモデルをトレーニングあるいはファインチューニング\n- モデルとデータプライバシーに対する完全なコントロール\n\n## AutoML\n\n- エンベディングのファインチューニングやモデル作成のためのローコードツール\n\n![Screenshot 2023-10-28 at 15.55.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8a94fd04-d672-7357-4d43-530433165f5b.png)\n\n## MLflow\n\n- エンドツーエンドのMLOpsとLLMOps\n- 月間1,100万ダウンロードのMLOpsのスタンダード\n- モデルの実験、追跡、評価、管理\n- 権限管理やガバナンスを自動化するためのAI GatewayのようなLLMOpsの新機能\n\n![Screenshot 2023-10-28 at 15.55.53.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7475cc0d-3a20-8445-dc0a-45dd914d6258.png)\n\n# モデルサービング\n\n- 最大10倍のレーテンシー削減、コスト削減を提供するモデルのリアルタイム推論\n- 高可用性、低レーテンシー、ゼロまで自動スケールするサービング\n- デプロイメントを自動化し、エラーを削減するための、自動特徴量検索、監視、統合ガバナンス\n- 最大10倍のレーテンシー、コスト削減を実現するオープンソース生成AIモデルに最適化\n\n# レイクハウスモニタリング\n\n- インテリジェントなデータとモデルのモニタリング\n- 数分でデータとモデルに対する完全な可視性を提供し、市場投入に要する時間とコストを削減\n- データとモデルの異常をプロアクティブかつシンプルに検知\n- 異常の根本原因を追跡することでシームレスにデバッグ\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks Unity CatalogにおけるAI生成ドキュメントのパブリックプレビューの発表", "text": "[Announcing Public Preview of AI Generated Documentation In Databricks Unity Catalog \\| Databricks Blog](https://www.databricks.com/blog/announcing-public-preview-ai-generated-documentation-databricks-unity-catalog)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n# 生成AIによるデータのドキュメンテーションとディスカバリーの効率化\n\n本日、[Databricks Unity Catalog](https://www.databricks.com/jp/product/unity-catalog)におけるAI生成ドキュメントのパブリックプレビューを発表できることを嬉しく思っています。この機能はテーブルやカラムの説明文、コメントの追加を自動化することで、みなさまの組織におけるデータとAI資産のドキュメント作成、キュレーション、発券をシンプルにするために生成AIを活用します。\n\n現在のデータドリブンの世界において、どこにデータがあるのかは情報に基づく意思決定の基盤となり、シームレスなデータの発見可能性や明確さをベースとしてチームワークにおける強固な基盤を確立します。しかし、データチームは多くの場合重要な課題に苦戦しています: 包括的なデータ記述の不在によって、文脈の理解が困難となります。この不足はユーザーがデータの潜在能力を活用する妨げとなっており、これらのギャップの橋渡しをするために簡素化されたデータの説明文の必要性を強調しています。\n\nさらに、テーブルやカラムの適切なメタデータや説明文が存在しないことが問題を複雑にし、いくつかの課題を引き起こしています:\n\n- **データの曖昧さ:** テーブルやカラムの目的やコンテンツに関する明確さがないことによって、ユーザーの意思決定能力の大きな妨げとなります。\n- **手作業の負荷:** データの所有者は資産に対する重要な文脈、チーム間でのコラボレーションを促進するための重要な要件を記載するために、説明文やコメントを手動で追加する責任を負っています。\n- **非効率的なデータ探索:** 多くの場合、ユーザーはデータから洞察を導き出すために、複雑なクエリーに依存しなければならず、貴重な時間やリソースを消費することになっています。\n- **貧弱なデータ品質:** 不適切、不正確なドキュメントは、誤解、データのエラー、妥協したデータ品質を引き起こします。特にIDCによると、データアナリストはデータの準備、クレンジングに自分たちの時間の最大80%を費やしており、これは多くの場合、説明文の不足を含む不適切なデータの説明文によって引き起こされています。\n\n# Unity CatalogにおけるAI生成ドキュメントによる効率の強化と洞察の加速\n\nこれらの課題に取り組み、データオーナーが説明文を追加するために十分な文脈を持たないケースにおいて支援を行うために、Unity Catalogではテーブルやカラムの説明文を提案するようになりました。ユーザーはこれらの提案を受け入れたり、必要に応じて修正したりすることで、アシスト的でユーザーフレンドリーな体験を確実なものとします。\n\n## 動作原理\n\n- **データ探索:** カタログエクスプローラに移動し、所有・管理するテーブルにアクセスすると、テーブルやカラムに対して自動生成されたメタデータが表示されます。\n![](https://lh3.googleusercontent.com/EAphkGq2GceZRUnb_PXBxY9A3kJkWzKtMIlxuZmez6So-2IpKg1dSURORKIOv6K1Vjnv1zYeQqDLEcgHcvOIW6jGnhWks8PJLMPApeMFwqxQorPc1dNBtMO9_364_YWI4ewkDGNBhegSkBhbQ2WatDk)\n- **ユーザーのレビューと編集:** ユーザーは生成されたメタデータをレビュー、編集、承諾することができます。このステップによって、説明文は特定のユースケースやドメイン知識と平仄を取ることになります。\n![](https://lh4.googleusercontent.com/5Ky-7lXOJLhj0_MWSZX3CCW5HVbReytJ_8WMPGO06IjGNg_yHzmCKLs7EcbegGLgPD2CBHi74lMS_upXNSJ4FxnPQo_8Bsi0ukke9rFn4PQb03v6ncaKWDzpfSZQSchAgp2Kix0brvmFfceLAKbT1ZI)\n- **メタデータのストレージ:** ユーザーが生成されたドキュメントを承認すると、それらはUnity Catalogに保存されます。このドキュメントは、自動生成された説明文に基づいた効率的な検索のように、様々な手段でデータの利用者を支援するために活用されます。\n![](https://lh5.googleusercontent.com/mlZKIwCAjQAa5WsxPnNlG--odpnvo28o-pcUVaDuxBocEiQauZ0qx6evGkRpOsiJEgEnhspR-SP0o5tdvaL9CBeSgWuS1T-FfBId1QKqwCFZyQc0txyRhgJ6kscUDxunR2iJYBVAIp551RMtck8_xQA)\n\n## Unity CatalogのAIドキュメンテーション活用によるメリット:\n\n- **時間とリソースの効率性:** ドキュメント生成の自動化によって時間を節約し、データの説明文の記述に要する工数を削減します。\n- **簡素化されたデータ探索:** ユーザーはテーブルやカラムのコンテンツや目的をクイックに理解することができ、複雑なクエリーを実行する必要性が低減されます。\n- **強化されたデータの明確性:** 正確かつ包括的な説明文はデータの明確性を保証し、誤解を防ぎます。\n- **改善されたDatabricks検索** 生成メタデータはワークスペースでのテーブル検索をサポートしており、みなさまのすべてのデータユースケースにおいて適切なデータの発見可能性を改善します。\n- **ユーザーのコントロール:** ユーザーはドキュメント生成プロセスに対するコントロールを保持しており、特定の要件によりマッチするように説明文を編集、カスタマイズすることができます。\n\n# Unity CatalogiにおけるAIガバナンス\n\nUnity Catalogによって企業は、いかなるデータプラットフォーム、クラウドにおけるファイル、テーブル、MLモデル、ノートブック、ダッシュボードをセキュアに発見、アクセス、監視、コラボレーションできるようになり、生産性を向上するためにAIを活用しつつも、レイクハウス環境の完全なポテンシャルを解放します。このAI生成ドキュメントは我々の包括的な製品ロードマップで重要な位置を占めており、ガバナンスワークフローやオペレーションの効率性を強化するために、AIのパワーを活用することを狙いとしています。[LakehouseIQ](https://www.databricks.com/jp/blog/introducing-lakehouseiq-ai-powered-engine-uniquely-understands-your-business)や[Lakehouse Monitoring](https://www.databricks.com/product/machine-learning/lakehouse-monitoring)のような機能によって、企業はパワフルなデータインテリジェンスとモニタリング機能を入手することができます。さらに、コンテキストを解するAIアシスタントである[Databricksアシスタント](https://qiita.com/taka_yayoi/items/855e15def68150bdea55)はさらにユーザー体験を強化し、オペレーションをより直感的かつレスポンシブなものにします。Unity CatalogにおけるこのAI技術との戦略的なインテグレーションは、レイクハウスプラットフォームとネイティブにインテグレーションされる最先端のデータ&AIガバナンスソリューションに対するイノベーションと継続的な改善に対する、我々のコミットメントを強調するものです。\n\n# 使い始める\n\nUnity Catalogを皆様のレイクハウスの基盤として受け入れることで、皆様のすべてのデータとAIの領域に渡る柔軟でスケーラブルなガバナンス実装のパワーを解放することができます。使い始めるのはとても簡単です！お使いのワークスペースでUnity Catalogを有効化しているのであれば、カタログエクスプローラで所有・管理するテーブルに移動するだけです。詳細については、[AWS](https://docs.databricks.com/ja/data-governance/unity-catalog/create-tables.html#add-ai-generated-comments-to-a-table)、[Azure](https://learn.microsoft.com/ja-jp/azure/databricks/data-governance/unity-catalog/create-tables#--add-ai-generated-comments-to-a-table)、[GCP](https://docs.gcp.databricks.com/data-governance/unity-catalog/create-tables.html#add-ai-generated-comments-to-a-table)のUnity Catalogのガイドをご覧ください。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks Partner ConnectからArcionを試してみる", "text": "こちらのニュースを受けて動かしてみました。「アーキオン」と発音するそうです。\n\nhttps://www.databricks.com/company/newsroom/press-releases/databricks-agrees-acquire-arcion-leading-provider-real-time\n\n# Arcionとは\n\nローコードでデータレプリケーション(複製)パイプラインを構築できるクラウドプラットフォームを提供しています。\n\nhttps://www.arcion.io/\n\n# Databricksからの接続\n\n今回のシナリオでは、SnowflakeのテーブルをDatabricksに複製します。\n\nDatabricks Partner Connectに初回アクセスすると、データ取り込みのガイドが表示されます。パートナーでArcionを選択します。\n![Screenshot 2023-10-24 at 16.13.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/104f3e3c-c97b-e447-094c-22e7bc9d806c.png)\n\n複製先のカタログを選択します。hive_metastoreを選択します。\n![Screenshot 2023-10-24 at 16.13.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d4ced341-8bb0-8b7c-085a-160da968aaba.png)\n![Screenshot 2023-10-24 at 16.13.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d5bc971e-c726-58af-9033-d0cf9fec59b9.png)\n\nArcion連携のために生成されるリソースを確認します。SQLウェアハウスやパーソナルアクセストークンなどが生成されます。\n![Screenshot 2023-10-24 at 16.13.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1b4bc29d-bdef-365f-e756-663ebaeaa317.png)\n![Screenshot 2023-10-24 at 16.13.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3bc40304-4517-c1a2-a35c-65c6f0c836be.png)\n\nArcionのサイトに移動します。\n![Screenshot 2023-10-24 at 16.14.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a88c37e5-ae81-6bbd-8bd1-76cc3c612503.png)\n\nユーザー情報はすでに入力されているので、サインアップします。\n![Screenshot 2023-10-24 at 16.14.30.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9c88737e-5715-9a16-9ded-db3b22483e17.png)\n\nアクティベーションのメールが送信されるので、アクティベートします。\n![Screenshot 2023-10-24 at 16.15.38.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/69886db1-db57-4b89-710a-10c876417131.png)\n\n# Arcionでの設定・操作\n\n複製(Replication)を作成します。どこから(Source)どこに(Destination)どのように複製するのか(Replication mode)を組み立てることになります。\n\n今回は一番簡単そうな**Snapshot**を選択します。リアルタイムの複製や追記モードもあるようです。\n![Screenshot 2023-10-24 at 16.16.28.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0f92ef07-4ca9-0b53-7f97-366cf21c68c8.png)\n\n**Source**を設定します。\n![Screenshot 2023-10-24 at 16.16.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7a9d99e5-2676-b385-2aca-bf92b4e5eef6.png)\n\nSnowflakeの接続設定をします。**Test Connection**でエラーが出たのですが、結果的に動きました。\n![Screenshot 2023-10-28 at 9.23.45.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5ecec725-bfac-8d47-4874-8f01b0ad4aaf.png)\n\nSnowflakeでは複製するテーブルを作成しておきます。\n![Screenshot 2023-10-28 at 8.45.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/329a4d81-e4a5-ec0a-ff96-ba6fc21037a1.png)\n\n**Destination**を設定しますが、必要な情報はすでに入力されているので、Databricksを選択するだけです。\n![Screenshot 2023-10-24 at 16.17.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2b7fab42-deaf-c5ff-9cc2-1f01619d94b1.png)\n\n接続確認もしておきます。\n![Screenshot 2023-10-24 at 16.18.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2406153a-2f52-58d7-e2a0-9f32e193037c.png)\n\n**Filter**を設定します。複製するテーブルを選択します。\n![Screenshot 2023-10-28 at 9.23.34.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7220db30-73c9-76ae-7e2c-ac30a4e13e20.png)\n![Screenshot 2023-10-28 at 9.24.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/4c2f54ac-4fb0-992d-2355-7005281833bc.png)\n\n**Start Replication**をクリックします。\n![Screenshot 2023-10-28 at 9.24.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0bc78f86-0f2e-a53d-b313-e916d35ae665.png)\n\nダッシュボードに移動します。\n![Screenshot 2023-10-28 at 9.25.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8aaf49d7-1e0e-0523-92db-4e606dbc7dc9.png)\nRUNNING状態に移行して複製が始まります。\n![Screenshot 2023-10-28 at 9.25.54.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0f26ad0d-fffb-95da-4eb9-325adeda6670.png)\n\nログなども確認できます。\n![Screenshot 2023-10-28 at 9.29.21.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/96b31fc7-74c0-ee83-4ce5-2d1f259fdc56.png)\n\n:::note\n**注意**\nDatabricksのSQLウェアハウスが起動していない場合、起動待ちのタイムアウトでエラーになることがありました。その場合には複製を再度実行してみてください。\n:::\n\n処理が進むと**Progress**に水が溜まり始めます。\n![Screenshot 2023-10-28 at 9.33.17.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/260bd6db-caad-691c-a105-78931a225efd.png)\n\n複製が完了しました。\n![Screenshot 2023-10-28 at 9.33.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d927fdb9-dc86-a20e-06af-5a83ac51d986.png)\n![Screenshot 2023-10-28 at 9.34.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a52c2a35-6a8f-7001-15cc-96cb4fe7517e.png)\n\nDatabricksで複製されたテーブルを確認できます。\n![Screenshot 2023-10-28 at 10.25.01.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/31dc7870-66de-168d-d46d-f36d0eaa2f7f.png)\n\n初めてArcionを触ってみましたが、いろいろなシステムとの連携が容易になりそうです。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksファイルシステム、ワークスペースファイル、ボリュームの違い", "text": "こちらの記事を書いてから10ヶ月ほど経ちました。\n\nhttps://qiita.com/taka_yayoi/items/075c6b3aeafac54c8ac4\n\nこちらの記事では、[DBFS(Databricks File System)](https://docs.databricks.com/ja/dbfs/index.html)とクラスターローカルのファイルシステムのみに言及しました。\n\nその後、ファイルを取り扱う目的で以下の機能がDatabricksに導入されました。\n\n- [ワークスペースファイル](https://docs.databricks.com/ja/files/workspace.html)\n- [ボリューム](https://docs.databricks.com/ja/data-governance/unity-catalog/create-volumes.html)\n\nこちらの記事では、これらの新機能とDBFSの違いをまとめます。\n\n# サマリー\n\n| 観点 | DBFS | ワークスペースファイル | ボリューム |\n|:--|:--|:--|:--|\n| ワークスペース横断でのアクセス  | 不可  | 不可  | 可能  |\n| アクセスコントロール  | 不可  |   ファイル単位に設定可能| ボリューム単位に設定可能  |\n| ファイルエディタのサポート  | 無し  | 有り  | 無し  |\n| ファイルサイズの制限  | アップロードできるファイルサイズは最大2GB  | 200MB  | 無し  |\n| 想定用途  |  一時ファイルの保存 |  Pythonモジュールのインポート、git連携 | 大規模ファイルの読み書き  |\n| その他  |  プロダクション用途での利用は非推奨 |   | <ul><li>Unity Catalogが前提<li>ボリュームのファイルからテーブルを作成するとリネージを生成</ul>  |\n\nここでの**ワークスペース横断でのアクセス**とは、複数のDatabricksワークスペースから同じファイルにアクセスできるかどうかを意味しています。\n\nノートブックからインポートするpyファイルであれば**ワークスペースファイル**、それ以外の非構造化データファイルであれば**ボリューム**を使うのが推奨となります。\n\n# DBFSとは\n\n(少なくとも私がDatabricksに入社した3年前の)昔から存在しているファイルシステム。アクセスコントロールができないので、プロダクション用途での利用はお勧めしていません。現時点では、一時ファイルを格納しておくくらいしか使い道はありません。\n\nhttps://qiita.com/taka_yayoi/items/897264c486e179d72247\n\n- ワークスペース横断でのアクセス: 不可\n- アクセスコントロール: 不可\n- ファイルエディタのサポート: 無し\n- ファイルサイズの制限: アップロードできるファイルサイズは最大2GB\n- 想定用途: 一時ファイルの保存\n- その他: プロダクション用途での利用は非推奨\n\nGUIからであれば、カタログエクスプローラから参照することができます。これ以外にPythonなどのプログラミング言語から操作することができます。\n![Screenshot 2023-10-26 at 16.39.44.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0b079c45-0dd4-c524-60bb-aab0bada2ff8.png)\n\nパスの指定方法は使用するコマンドによって変わります(これがややこしい)。\n\nPySparkなどの分散ファイルシステム前提のコマンドでは何も変更を加えずにファイルへのパスを指定します。以下の例では明示的に`dbfs:`スキーマを付与しています。\n\n```py\ndf = spark.read.format(\"delta\").load('dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\ndisplay(df)\n```\n\nPandasのようなローカルファイル前提のコマンドでは、パスの先頭に`/dbfs`を追加します。\n\n```py\nimport pandas as pd\n\ndf = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-red.csv\", sep=\";\")\ndisplay(df)\n```\n\n# ワークスペースファイルとは\n\nノートブックと同じ場所でノートブック以外のPythonファイルなどを格納、編集できるようにした機能。元々は[Repos](https://docs.databricks.com/ja/repos/index.html)限定の機能でしたが、2023/4からワークスペースのどこでも利用できるようになりました。\n\nhttps://qiita.com/taka_yayoi/items/ff068885af8f84faa1d2\n\n- ワークスペース横断でのアクセス: 不可\n- アクセスコントロール: ファイル単位に設定可能\n- ファイルエディタのサポート: 有り\n- ファイルサイズの制限: 200MB\n- 想定用途: Pythonモジュールのインポート、git連携\n\n以下のようにpyファイルを作成します。\n![Screenshot 2023-10-26 at 16.40.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/716cf43b-71a4-17bf-4b3d-82dee5ea372b.png)\n\n:::note\n**注意**\nワークスペースファイルを作成する際には、ノートブックではなく**ファイル**を作成してください。ノートブックの名称を`xxx.py`としてもワークスペースファイルとしては認識されません。\n![Screenshot 2023-10-27 at 9.48.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b2485812-ff77-94e3-0218-ac37eb7b1bbb.png)\n:::\n\nノートブックから直接`import`することができます。\n![Screenshot 2023-10-26 at 16.41.21.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/48179250-56b3-65d2-b0b0-0c02e950cafa.png)\n\nワークスペースファイルのパスの指定方法は以下の通りとなります。どのようなコマンドであってもパスの指定方法は依存しません。\n\n**Repos内のファイル**\n\n`file:/Workspace/Repos/<user-folder>/<repo-name>/path/to/file`\n\n**それ以外の場所のファイル**\n\n`file:/Workspace/Users/<user-folder>/path/to/file`\n\n以下のようにしてアクセスできます。\n\n```sh\n%sh\nhead /Workspace/Users/takaaki.yayoi@databricks.com/20231025_volumes/power.py\n```\n```\ndef n_to_mth(n,m):\n  print(n, \"の\", m, \"乗は\", n**m)\n```\n\n# ボリュームとは\n\nファイル格納の機能としては最新のものです。2023/7にパブリックプレビュー。\n\nhttps://qiita.com/taka_yayoi/items/3acb6c69458f9c1253be\n\n- ワークスペース横断でのアクセス: 可能\n- アクセスコントロール: ボリューム単位に設定可能\n- ファイルエディタのサポート: 無し\n- ファイルサイズの制限: 無し\n- 想定用途: 大規模ファイルの読み書き\n- その他: Unity Catalogが前提。ボリュームのファイルからテーブルを作成するとリネージを生成\n\nボリューム上のファイルもカタログエクスプローラから確認できます。\n![Screenshot 2023-10-26 at 16.43.17.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e16f2c72-bb45-e56e-6a35-fd2da7fe2d1f.png)\n\nボリューム上のファイルからテーブルを作成すると、リネージも作成されます。\n![](https://camo.qiitausercontent.com/fc02d88320cf9ef92f8ac3f19fa28c3bc12603cf/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313136383838322f37343731326539612d653531642d613231362d303832392d3137356139646533363765322e706e67)\n\nワークスペースファイルと同じようにPythonからアクセスできます。\n![](https://camo.qiitausercontent.com/053b566f1964c036c23c53dc07ec10ea51807c8c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313136383838322f62396331306237642d343665312d366465362d626263642d3234643230306338386236362e706e67)\n\nパスの指定方法は一番シンプルです。コマンドに関係なく以下の通りとなります。\n\n`/Volumes/<catalog>/<schema>/<volume>/<path>/<file-name>`\n\n```sh\n%sh\nls /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/catsanddogs/test_set/test_set/cats\n```\n```\n_DS_Store\ncat.4001.jpg\ncat.4002.jpg\ncat.4003.jpg\ncat.4004.jpg\ncat.4005.jpg\ncat.4006.jpg\ncat.4007.jpg\ncat.4008.jpg\ncat.4009.jpg\ncat.4010.jpg\ncat.4011.jpg\n```\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksのボリューム機能による画像ファイルの取り扱い", "text": "こちらの記事で触れているボリューム機能を画像ファイルの取り扱いを通じてウォークスルーします。\n\nhttps://qiita.com/taka_yayoi/items/3acb6c69458f9c1253be\n\n# 画像アーカイブファイルのダウンロード\n\nこちらのzipファイルをローカルマシンにダウンロードします。\n\nhttps://www.kaggle.com/datasets/tongpython/cat-and-dog/\n\n![Screenshot 2023-10-25 at 19.21.44.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/563fc362-84a4-3c3a-c70f-28f92138dd21.png)\n\n# ボリュームへのアーカイブファイルのアップロード\n\nすでに作成済みのボリュームにアップロードします。作成方法は[こちら](https://qiita.com/taka_yayoi/items/453026daacf71ba5662f)をご覧ください。\n\nカタログエクスプローラにアクセスして、対象のボリュームにアクセスします。そして、**このボリュームにアップロード**ボタンをクリックします。\n![Screenshot 2023-10-25 at 19.24.03.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/13aaca8d-b4ad-da4b-e7e9-76aef26d3316.png)\n\nダウンロードしたアーカイブファイルをドラッグ&ドロップします。\n![Screenshot 2023-10-25 at 19.24.21.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c36da30f-e17c-9810-2cc2-aa245fca3187.png)\n![Screenshot 2023-10-25 at 19.24.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/76743d7d-1aaa-6147-afbe-20536780f84a.png)\n\n**アップロード**をクリックすると、アップロードが開始しますので少々待ちます。\n![Screenshot 2023-10-25 at 19.24.43.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3ba8fdac-5b07-3633-83df-e24be9700586.png)\n\nアップロードされました。\n![Screenshot 2023-10-25 at 19.28.06.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/74937935-3747-bbfd-bf2e-dc82b1a3e945.png)\n\n# ボリュームのファイルの操作\n\nノートブックから操作するので、新規にノートブックを作成します。ノートブック左側の![Screenshot 2023-10-25 at 19.36.31.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/23399de2-c22b-2578-55ef-95b010b99659.png)**カタログ**をクリックすると、ノートブックの隣にカタログエクスプローラの簡易版が表示されます。アーカイブファイルにマウスカーソルを移動すると3点リーダーが表示されるので、そちらをクリックすると**volume fileパスをコピー**メニューが表示されるので選択します。これでボリューム上のファイルのパスがコピーされます。\n\n以下のコマンドを実行して、ファイルをunzipします。\n\n```sh:shell\nunzip /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/archive.zip -d /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/catsanddogs/\n```\n\n普通のファイルシステム同様に解凍処理が行われます。\n![Screenshot 2023-10-25 at 19.39.41.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8f9e4d10-62df-75ac-63fb-7e4a0ceec633.png)\n\n約1万ファイルあるので、それなりに時間がかかります。\n\n```sh:shell\n%sh\nfind /Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/catsanddogs/ -type f | wc -l\n```\n```\n10032\n```\n\nって、普通にシェルコマンド使えますね。\n\nこのセクションの最後として画像を確認します。\n\n```py:Python\nfrom PIL import Image\n\nimage_to_classify = \"/Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/catsanddogs/test_set/test_set/cats/cat.4010.jpg\"\nimage = Image.open(image_to_classify)\ndisplay(image)\n```\n\nむはー、って普通のファイルシステムと同じようにアクセスできます。\n![Screenshot 2023-10-25 at 20.30.54.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b9c10b7d-46e1-6de6-bbcd-24d200c88b66.png)\n\n\n# テーブルへの登録\n\nまず、[Sparkの画像データソース](https://qiita.com/taka_yayoi/items/8d4b1b61699d68a34e58)を使ってデータフレームにロードします。\n\n```py:Python\n# 画像データソースを用いて画像データソースを作成します\nimage_df = spark.read.format(\"image\").load(\"/Volumes/takaakiyayoi_catalog/quickstart_schema/quickstart_volume/catsanddogs/test_set/test_set/cats\")\n\n# データフレームを表示します\ndisplay(image_df) \n```\n\nむふー、これだけで癒されます。猫派です。\n![Screenshot 2023-10-25 at 20.18.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8643f55d-b231-f423-3c70-556fcf0053cc.png)\n\nこれをテーブルとしてUnity Catalogに保存します。\n\n```py:Python\nimage_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"takaakiyayoi_catalog.quickstart_schema.cat_test_image\")\n```\n\nカタログエクスプローラでテーブルを確認できます。\n![Screenshot 2023-10-25 at 20.24.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/031737e4-6562-107e-40af-56c2f93a7ea4.png)\n\nリネージを確認するために**依存関係**をクリックします。**パス**をクリックすると、どのボリュームからこのテーブルが作成されたのかを確認できます。\n![Screenshot 2023-10-25 at 20.25.15.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e6b9f5f4-761e-04ac-6b19-ab57c347e75d.png)\n\nさらに、**リネージグラフを表示**をクリックすると依存関係をグラフィカルに確認することができます。\n![Screenshot 2023-10-25 at 20.26.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/74712e9a-e51d-a216-0829-175a9de367e2.png)\n\n\n# まとめ\n\nこれまではDBFSにファイルをアップロードするアプローチしか無かったのですが、ボリュームを使うことでより洗練されたGUIでファイルを操作でき、かつ、Unity Catalogによるアクセスコントロールやリネージの機能を活用できます。ぜひご活用ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks Unity Catalogにおけるボリュームのパブリックプレビューの発表", "text": "[Announcing Public Preview of Volumes in Databricks Unity Catalog \\| Databricks Blog](https://www.databricks.com/blog/announcing-public-preview-volumes-databricks-unity-catalog)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n:::note info\n2023/7/13の記事です。\n:::\n\n\n# 任意の非テーブルデータを発見、制御、クエリー\n\nData & AIサミット2023で、Databricks Unity Catalogにおける[ボリュームをご紹介](https://www.databricks.com/blog/whats-new-unity-catalog-data-and-ai-summit-2023)しました。この機能によって、ユーザーはUnity Catalogのテーブルデータに加え、構造化データ、半構造化データ、構造化データを含むいかなる非テーブルデータを発見、制御、処理、リネージの追跡が可能となります。本日、AWS、Azure、GCPで利用できる**ボリューム**のパブリックプレビューを発表できることを嬉しく思っています。\n\n本記事では、非テーブルデータに関連する一般的なユースケースを議論し、Unity Catalogのボリュームの主要な機能を説明し、ボリュームの実践的なアプリケーションをデモンストレーションする実際に動作するサンプルをお見せし、ボリュームを使い始める方法の詳細を説明します。\n\n# 非テーブルデータのガバナンス、アクセスに関する一般的なユースケース\n\nDatabricksレイクハウスプラットフォームでは、さまざまなフォーマットの大量のデータを格納、処理することができます。このようなデータの多くはテーブルとして管理されますが、特に機械学習やデータサイエンスワークロードにおいては、テキスト、画像、音声、動画、PDF、XMLファイルのような非テーブルデータへのアクセスを必要とするユースケースが多数存在します。\n\n**我々のお客様から聞いた一般的なユースケースには以下のようなものがあります:**\n\n- 画像、音声、動画、PDFファイルのような非構造化データの大コボなコレクションに対する機械学習の実行。\n- モデルのトレーニングに使用されるトレーニング、テスト、検証データセットの永続化や共有、ログやチェックポイントのディレクトリのようなオペレーションデータのロケーションの定義。\n- データサイエンスにおけるデータ探索ステージでの非テーブルデータファイルのアップロードとクエリー。\n- ネイティブにクラウドオブジェクトストレージAPIをサポートせず、クラスターマシンのローカルファイルシステムにファイルが存在することを前提とするツールの操作。\n- クラスターライブラリ、ノートブックスコープライブラリ、ジョブの依存関係の設定の前に、.whlや.txtのような任意のフォーマットのライブラリ、証明書、設定ファイルのワークスペース横断での格納、セキュリティ保護。\n- Auto LoaderやCOPY INTOを用いるなどしてテーブルにロードされる前の取り込みパイプラインの初期ステージにおける生データファイルのステージングや前処理。\n- ワークスペース、リージョン、クラウド、データプラットフォームの内外での他のユーザーとの大規模ファイルコレクションの共有。\n\nボリュームを用いることで、フォーマットに関係なしに非テーブルデータの大規模コレクションをクラウドストレージのパフォーマンスで読み込み、処理できるスケーラブルなファイルベースのアプリケーションを構築することができます。\n\n# ボリュームとは何？どのように使うのか？\n\n**ボリューム**は、Unity Catalogにおいてディレクトリやファイルのコレクションを*カタログ化*する新たなタイプのオブジェクトです。ボリュームは、クラウドオブジェクトストレージにおけるストレージの論理的な*ボリューム*を表現し、構造化、半構造化、非構造化データを含む任意のフォーマットのデータへのアクセス、格納、管理する機能を提供します。これによって、Unity Catalogでテーブルデータやモデルとともに非テーブルデータの制御、管理、リネージ追跡が可能となり、統合された発見、ガバナンス体験を提供します。\n\n## ボリュームに格納されている画像データセットに対する画像分類の実行\n\nボリュームと実際のアプリケーションをさらに理解するために、サンプルを見ていきましょう。猫と犬の画像から構成される[データセット](https://www.kaggle.com/datasets/tongpython/cat-and-dog)を用いた画像分類のために機械学習(ML)を活用したいものとします。最初のステップは、ローカルマシンにこれらの画像をダウンロードすることになります。我々の目的は、データサイエンスの用途でDatabricksにこれらの画像を取り込むことです。\n\nこれを達成するには、データエクスプローラ(現カタログエクスプローラ)のUIを活用します。Unity Catalogのスキーマに新規ボリュームを作成することで、自分自身がオーナーになります。その後で、これぼレーションする相手にアクセス権を許可し、画像ファイルを含むアーカイブをアップロードします。書き込み権限を持っている既存のボリュームにファイルをアップロードする選択肢もあることを付け加えておきます。あるいは、ノートブックやSQLエディタで、SQLコマンドを用いて自分のボリュームを作成し、権限を管理することができます。\n\n```sql:SQL\nCREATE VOLUME my_catalog.my_schema.my_volume;\n\nGRANT READ VOLUME, WRITE VOLUME\nON VOLUME my_volume\nTO `user group`;\n```\n\n![](https://cms.databricks.com/sites/default/files/inline-images/db-696-blog-img-1.gif)\n\nこれによって、よく使われるunzipユーティリティを用いて画像アーカイブの抽出に進むことができます。コマンドにボリュームに対応するパスを含めていることに注意してください。このパスは、Unity Catalogリソースの階層構造に対応しており、Unity Catalogで定義されているアクセス権に従います。\n\n```sh:Shell\n%sh unzip /Volumes/my_catalog/my_schema/my_volume/catsanddogs.zip -d\n /Volumes/my_catalog/my_schema/my_volume/catsanddogs/\n```\n\nこのコマンドを実行するには、サイドバーを通じてボリュームのコンテンツに簡単にアクセスできるノートブックを使用することができます。さらに、ファイルの一覧や画像の表示のようにファイルに関連するコマンドをタイプするプロセスを簡単にするために「パスのコピー」機能を活用することができます。\n\n```py:Python\nfrom PIL import Image\n\nimage_to_classify = \"/Volumes/my_catalog/my_schema/my_volume/catsanddogs/cat3999.jpg\"\nimage = Image.open(image_to_classify)\ndisplay(image)\n```\n\n以下のスクリーンショットでは、完全なインタラクションの流れを示しています。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-696-blog-img-2.gif)\n\n事前に定義されたリストのラベルを用いて画像を分類するために、Unity Catalogの[MLflowモデルレジストリ](https://docs.databricks.com/ja/mlflow/models-in-uc.html)に事前に登録されている[ゼロショット画像分類](https://huggingface.co/docs/transformers/tasks/zero_shot_image_classification)モデルを活用します。以下のコードスニペットでは、どのようにモデルをロードし、分類を行い、判定結果を表示するのかを示しています。以下に示しているスクリーンショットでは、これらのインタラクションを示しています。\n\n```py:Python\nclassification_labels = [\"fox\", \"bear\", \"dog\", \"cat\"]\n\nimport mlflow\nmlflow.set_registry_uri(\"databricks-uc\")\nregistered_model_name = \"my_catalog.my_schema.my_model_name\"\nmodel_uri = f\"models:/{registered_model_name}/1\"\nloaded_model = mlflow.transformers.load_model(model_uri)\npredictions = loaded_model(image_to_classify, candidate_labels = classification_labels)\n\nprint(f\"Picture has classification result: {predictions[i]}\")\n```\n\n**以下のリンクをクリックすると別タブで画像が開きます**\n\nhttps://cms.databricks.com/sites/default/files/inline-images/db-696-blog-img-3.gif\n\n## Unity Catalogにおける重要なボリュームの機能\n\n**Unity Catalogにおける非テーブルデータの管理。** ボリュームはテーブル、モデル、関数とともにUnity Catalogのスキーマでカタログ化され、Unity Catalogのオブジェクトモデルのコア原則に従いますので、デフォルトでデータはセキュリティ保護されます。十分な権限を持つデータスチュワードはボリュームを作成することができ、そして、ボリュームの所有者となりコンテンツにアクセスできる唯一のユーザーとなります。他のユーザーやグループにボリュームのコンテンツを読み書きする権限を与えることができます。ボリュームに格納されるファイルはワークスペース横断でアクセスすることができ、適切なワークスペースに親のカタログをバインディングすることで、ボリュームへのアクセスを特定のワークスペースに限定することもできます。\n\n**マネージドあるいは外部ボリュームによる柔軟なストレージ環境。** *マネージド*あるいは*外部*ボリュームを設定する選択肢があります。*マネージドボリューム*はUnity Catalogスキーマのデフォルトストレージロケーションにファイルを格納するので、ローカルマシンからアップロードしたファイルからクイックなデータ探索を行うなどのケースで、クラウドストレージへのアクセスを設定する初回のオーバーヘッドなしに、ファイルに対して制御されたロケーションを必要とする際には、便利なソリューションとなります。外部ボリュームは、ボリュームを作成する際に参照する外部のストレージロケーションにファイルを格納し、Databricks内からアクセスするために、他のシステムによって生成されるファイルをステージングする必要がある場合には有用です。例えば、IoTデバイスや医療機器によって大量の画像、動画データが生成されるクラウドストレージロケーションに対する直接のアクセスを提供することができます。\n\n**クラウドストレージのパフォーマンスとスケーラビリティでのデータ処理。** ボリュームの背後にはクラウドオブジェクトストレージがあり、クラウドストレージの堅牢性、可用性、スケーラビリティ、安定性のメリットを享受することができます。クラウドストレージのパフォーマンスで高トラフィックのワークロードを実行するためや、ペタバイト以上のデータを大規模に処理するためにボリュームを活用することができます。\n\n**最新のユーザーインタフェースによる生産性の向上。** ボリュームは、[データエクスプローラ](https://docs.databricks.com/ja/data/index.html)、ノートブック、[リネージ](https://www.databricks.com/blog/2022/12/12/announcing-general-availability-data-lineage-unity-catalog.html)、[データ追加](https://www.databricks.com/blog/easy-ingestion-lakehouse-file-upload-and-add-data-ui)、クラスターライブラリ設定ユーザーインタフェースを含むDatabricksのプラットフォーム体験とシームレスに統合されています。さまざまなアクションでユーザーインタフェースを活用することができます: ボリュームのアクセス権や所有者の管理。ボリュームエンティティの作成、名前変更、削除のようなアクションを通じたボリュームのライフサイクル管理。ファイルの一覧、アップロード、ダウンロードを含むVol.コンテンツの管理。ノートブックの隣からボリュームとコンテンツの参照。リネージの調査。クラスターのソース、ジョブライブラリの設定などです。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-696-blog-img-4.png)\n*ボリュームのリネージ*\n\n**ファイルの操作で慣れ親しんだツールを活用。** ボリュームでは、ファイルにアクセスするために、Unity Catalogの階層構造を反映し、Databricksで使用する際には定義されたアクセス権に準拠する専用のパスフォーマットが導入しています:\n\n```\n/Volumes/<catalog>/<schema>/<volume>/<path_to_file>\n```\n\nApache Spark™やSQLコマンド、REST API、Databricksファイルシステムユーティリティ(dbutils.fs)、Databricks CLI、Terraform、さまざまなOSのライブラリ、ファイルユーティリティを使う際にファイルを参照するためにこのパスを使用することができます。\n![Screenshot 2023-10-25 at 18.04.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3d13c0d9-b086-4189-522c-f9e3132285de.png)\n\n**Unity Catalogで管理されるデータに対する新たな処理能力の解放。** ボリュームは、[s3a://](https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Introducing_the_Hadoop_S3A_client.)、[abfss://](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction-abfs-uri)、[gs://](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage)のようなクラウド固有のAPIやHadoopコネクタに対する抽象化レイヤーを提供し、Apache Spark™のSparkアプリケーションやネイティブでオブジェクトストレージをサポートしないツールで、クラウドに格納されているデータファイルの操作を容易にします。ボリュームの専用のファイルパスを用いることで、実際の処理は背後のクラウドストレージにマッピングされますが、ファイルがクラスターノードのローカルに存在するかのようにボリュームのコンテンツにアクセス、参照、処理できるようになります。これは特に[Pandas](https://pandas.pydata.org/)、[scikit\\-learn](https://docs.databricks.com/ja/machine-learning/train-model/scikit-learn.html)、TensorFlow [keras](https://www.tensorflow.org/api_docs/python/tf/keras)などの様々なデータサイエンス、MLライブラリを操作している際に有益となります。\n\n# Unity Catalogでボリュームを使い始める\n\nUnity CatalogのボリュームはDatabricksのエンタープライズ、プロプランのDatabricks Runtime 13.2以降で利用できます。ボリュームを簡単に使い始められるように、[ステップバイステップのガイド](https://app.getreprise.com/launch/MXxjqY6/)をご用意しました。すでにDatabricksアカウントをお持ちであれば、初めてのボリュームを作成するための詳細な手順を示しているドキュメントに従ってください([AWS](https://docs.databricks.com/ja/data-governance/unity-catalog/create-volumes.html) | [Azure](https://learn.microsoft.com/ja-jp/azure/databricks/data-governance/unity-catalog/create-volumes) | [GCP](https://docs.gcp.databricks.com/data-governance/unity-catalog/create-volumes.html))。ボリュームを作成した、コンテンツを探索するためにデータエクスプローラを活用し([AWS](https://docs.databricks.com/ja/data/explore-volumes.html) | [Azure](https://learn.microsoft.com/ja-jp/azure/databricks/data/explore-volumes) | [GCP](https://docs.gcp.databricks.com/data/explore-volumes.html))、ボリューム管理のためのSQL構文を学習することができます([AWS](https://docs.databricks.com/ja/sql/language-manual/sql-ref-volumes.html) | [Azure](https://learn.microsoft.com/en-gb/azure/databricks/sql/language-manual/sql-ref-volumes) | [GCP](https://docs.gcp.databricks.com/sql/language-manual/sql-ref-volumes.html))。また、ボリュームを最大限に活用するためのベストプラクティスを確認することをお勧めします([AWS](https://docs.databricks.com/ja/data-governance/unity-catalog/best-practices.html) | [Azure](https://learn.microsoft.com/ja-jp/azure/databricks/data-governance/unity-catalog/best-practices) | [GCP](https://docs.gcp.databricks.com/data-governance/unity-catalog/best-practices.html))。Databricksが初めてであり、まだアカウントをお持ちでない場合には、はじめにUnity Catalogのボリュームのメリットを体験するために、[フリートライアル](https://databricks.com/jp/try-databricks)にサインアップしてください。\n\nDelta Sharingによるボリュームの共有や、アップロード、ダウンロード、ファイル削除のようなファイル管理オペレーションのためのREST APIを含むエキサイティングなボリュームの機能を楽しみにしていてください。\n\n[What's new with Unity Catalog](https://www.databricks.com/dataaisummit/session/whats-new-unity-catalog-live-demos/)でのData & AIサミットのセッションや、ベストな実装プラクティスのための[ディープダイブセッション](https://www.databricks.com/dataaisummit/session/technical-deep-dive-unity-catalogs-practitioner-playbook/)や、[Everything You Need to Know to Manage LLMs](https://www.databricks.com/dataaisummit/session/llmops-everything-you-need-know-manage-llms/)をご覧いただくことも可能です。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksジョブで実行したSQLクエリーの結果を参照する際のアクセス権", "text": "意外とハマりました。\n\n# クエリー結果を参照できる条件\n\nジョブの結果からSQLクエリーの実行結果を参照するには、以下の条件を全て満たす必要があります。\n\n1. 参照者がジョブの参照権限を持っている\n1. 参照者が実行されるクエリーの参照権限を持っている\n1. 以下のいずれかを満たしている\n    1. ジョブの実行者とクエリーの実行者が一致している\n    1. ジョブの実行者とジョブ結果の参照者が一致している\n\nジョブの実行者はジョブの画面の**ジョブの詳細**で設定できます。\n![Screenshot 2023-10-24 at 7.48.23.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/449630fa-747f-91fa-25fa-d94ae594e3f5.png)\n\nクエリーの実行者はクエリーの権限設定で選択します。この際に注意しなくてはならないのは資格情報です。**所有者として実行**と**閲覧者として実行**が選択できます。\n\n- 所有者として実行：クエリーの作成者の権限でクエリーが実行されます。\n- 閲覧者として実行：クエリーを参照するユーザーの権限でクエリーが実行されます。\n\n![Screenshot 2023-10-23 at 9.22.31.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c1337990-409e-6072-4054-fa89c84d5fa7.png)\n\n条件1と条件2は設定すれば良い話ですが、条件3-1を満たすのか、条件3-2を満たすのかでアプローチが変わってきます。\n\n# 条件3-1「ジョブの実行者とクエリーの実行者が一致」を満たすアプローチ\n\n**ジョブの実行者とクエリーの実行者が一致している**必要がありますが、ジョブの実行者は設定可能ですが固定です。クエリーの資格情報が**所有者として実行**になっているのであれば、ジョブの実行者もクエリーの所有者にすればOKです。\n\n以下の例では、一番上のユーザーがクエリー`last_login`の所有者(クエリー所有者)です。このユーザーの権限でクエリーが実行されるようにします。また、参照者(takaaki.yayoi@databricks.com: 参照ユーザー)に対してクエリーの参照権限を付与します。\n![Screenshot 2023-10-24 at 7.55.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/4d4deefd-65ad-ba2c-7985-542d1195fe2f.png)\n\nこのSQLクエリーを実行するジョブの実行者もクエリー所有者にします。\n![Screenshot 2023-10-24 at 7.57.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/39cc775c-e3ba-da7f-5443-fce73765d307.png)\n\nこれで、参照ユーザーもSQLクエリーの実行結果を参照できます。\n![Screenshot 2023-10-24 at 8.01.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7a6327a8-c982-ab9c-c938-d587ce1e1450.png)\n\nクエリーへの参照権限がないとエラーになります。\n![Screenshot 2023-10-24 at 8.02.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/98a01895-0889-4822-5425-0af3aaa5055b.png)\n\n# 条件3-2「ジョブの実行者とジョブ結果の参照者が一致」を満たすアプローチ\n\nクエリーの資格情報を**所有者として実行**に設定できないケースがあります。その場合にはこちらのアプローチを取ることになります。**クエリー結果の参照者をジョブの実行者にする**というものです。注意点としては、特定のユーザーに対して設定できるものであり、グループには設定できないというものです。\n\nクエリーの資格情報を**閲覧者として実行**を選択します。そして、参照者(takaaki.yayoi@databricks.com: 参照ユーザー)に対してクエリーの実行権限を付与します。また、クエリーで操作するテーブルに対して適切な参照権限を付与します。\n![Screenshot 2023-10-24 at 8.05.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/75ec86b2-0d1f-d58c-4937-947ccc20990b.png)\n\nジョブの実行者を**参照ユーザー**に設定してジョブを実行します。\n![Screenshot 2023-10-24 at 8.09.44.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9dacb104-3499-515e-ef0b-fde7e1ac6ef4.png)\n\n参照ユーザーとしてジョブの実行結果にアクセスするとクエリーの結果を確認できます。\n![Screenshot 2023-10-24 at 8.10.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/28142a26-79c7-54fe-bd9f-128ff3686095.png)\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksにおけるYamathonデータの可視化", "text": "昨日はこちらのイベントに参加してきました。\n\nhttps://tokyo-yamathon.com/ja/home-yamathon/\n\n> ヤマソンは、2人～4人のチームが東京を歩いたり走ったりするために競う募金活動です。\n>\n> 有名なJR山手線の30の駅すべてを12時間以内に訪れます。\n\n一応、日常的にジョギングはしているのですがこれだけの長丁場は初です。結果完走できました。\n\n![IMG_9830.jpg](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/92a736aa-452a-d1a9-4c67-037b06f7b29c.jpeg)\n\nこれだけで終わると普通のブログになってしまうので、完走を記念してデータの可視化をします。というか筋肉痛や疲労で難しいこと考えられないので詳細な分析はできないです。\n\n# データのダウンロード\n\niPhone持って歩いていたのでGoogle MapsのTimelineとしては記録されています。これを以下のChrome拡張でダウンロードします。\n\nhttps://chrome.google.com/webstore/detail/free-google-maps-timeline/mjjpapfcbfpgbkabmfkmacjfediepccp\n\nなお、`Begin Time`などの時刻タイプの列はいくつか存在しており、それぞれフォーマットが異なるので注意してください。列には`Latitude`と`Longitude`を含めるようにします。\n\nこんな感じのCSVです。本当はもう少し粒度の細かいデータが欲しいところですが。\n![Screenshot 2023-10-22 at 19.41.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/35d5bc18-20e7-2aad-2148-82adfca4148a.png)\n\n# データのアップロード\n\nDatabricksのサイドメニューから**新規 > データの追加**を選択します。CSVをドラッグ&ドロップします。\n![Screenshot 2023-10-22 at 19.42.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/22b5a31c-805e-9297-00c6-d32e67152f6e.png)\n\nテーブルを作成するカタログとデータベース(スキーマ)を選択します。この時点でもデータ型を指定できるのですが、除外しなくてはいけない文字列などがあるのでデフォルトのままでテーブルを作成します。\n\n# テーブルの確認\n\nカタログエクスプローラでテーブルを確認します。せっかくなので[AIコメント](https://qiita.com/taka_yayoi/items/d001e0b1b48a336fa65f)も承認しておきます。\n![Screenshot 2023-10-22 at 19.43.11.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e7578dd4-45b1-8bec-8096-e76456a61989.png)\n\nデータも確認しておきます。`Distance in Meter`にカンマやmが入っていたりとなかなか前処理が大変そうです。\n![Screenshot 2023-10-22 at 19.44.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e76da24b-1f1b-98c5-424a-a2662a89b327.png)\n\n# クエリーとダッシュボードの作成\n\n[最近リリースされたLakeview](https://qiita.com/taka_yayoi/items/161fe382efad41415198)を使ってもいいのですが、残念なことにまだ地図がサポートされていません。なので、素直にDatabricks SQLのクエリーとダッシュボードを使います。\n\n不要な文字列を除外してあるべきデータ型に変化します。時間もタイムゾーンなどの処理をしなくてはいけないのですが、(疲れているので)今日はスキップします。\n\n```sql:SQL\nSELECT\n  `Begin Time`,\n  Name,\n  CAST(Latitude AS FLOAT),\n  CAST(Longitude AS FLOAT),\n  CAST(\n    REPLACE(TRIM(\" m\", `Distance in Meter`), \",\", \"\") AS INTEGER\n  ) AS Distance_meter,\n  CAST(\n    REPLACE(TRIM(\" sec\", `Duration In Seconds`), \",\", \"\") AS INTEGER\n  ) AS Duration_sec\nFROM\n  takaakiyayoi_catalog.default.timeline_viewer_oct_21_2023_oct_22_2023\n```\n![Screenshot 2023-10-22 at 20.24.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/cbd2e04a-82f1-7f77-454d-5c032fb02bd1.png)\n\nあとは、駅のマーカーを示す地図と移動時間・移動距離の散布図を作ります。\n![Screenshot 2023-10-22 at 20.13.45.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0413bbae-39ca-e83c-557a-8b48ce1af55b.png)\n![Screenshot 2023-10-22 at 20.13.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2111e985-39d9-99f3-d4a5-f60095f6ad91.png)\n\nこれをダッシュボードにまとめます。移動距離と移動時間は相関しているようですが、一部外れ値がありますね。あとで深掘りしたいところ。\n![Screenshot 2023-10-22 at 20.14.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d88c4712-8376-08f8-ec1a-019df9f7f0cf.png)\n\nこれ以外にも時間帯と移動速度の相関といった分析観点がありそうです。ただ、今日はもう寝ます(年寄りは二日目の筋肉痛の方がキツい)。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n\n"}
{"title": "Lakehouse AIでプロダクションMLOpsをシンプルに", "text": "[Simplifying Production MLOps with Lakehouse AI \\| Databricks Blog](https://www.databricks.com/blog/simplifying-production-mlops-with-lakehouse-ai)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n機械学習(ML)とは単にモデルを開発するというものではありません。それらを現実世界、すなわちプロダクションのシステムへの取り込みに関することです。しかし、プロトタイプからプロダクションに移行することは簡単ではありません。通常、モデルやデータの複雑性を理解し、様々なシステムと繋ぎ合わせ、Kubernetesのようなツールをマスターする必要があります。DataOps、ModelOps、DevOpsを一つの統合されたワークフローに統合することは、多くの場合「MLOps」と呼ばれます。\n\nDatabricksにおいては、皆様の企業に効果的にMLOpsのプラクティスを導入するには、統合され、データ中心のAIプラットフォームが必要であると信じています。本日、容易かつ大規模にMLOpsシステムをデプロイ、維持するために皆様のチームが必要とするすべてを提供するDatabricksレイクハウスAIプラットフォームにおけるいくつかの機能を発表できることを嬉しく思っています。\n\n> 「MLとMLOpsでDatabricksを活用することで、Comexはモデルのトレーニングからプロダクションのデプロイメントに容易かつクイックに移行できるようになりました。MLOpsスタックは、我々のチームにおけるMLワークフローを自動化、標準化し、より多くのプロジェクトに取り組むことができ、迅速に市場に投入できるようになりました。」— Daniel Natanael García Zapata -Global Data Science at Cemex\n\n# データ & AIに対する統合ソリューション\n\nMLOpsのライフサイクルは、定常的にデータを消費し、データを生成しますが、多くのMLプラットフォームは、データとAIに対してサイロ化されたツールを提供しています。DatabricksのUnity Catalog(UC)は点をつなぎ、今では[モデル](https://docs.databricks.com/ja/mlflow/models-in-uc.html)と[特徴量エンジニアリング](https://docs.databricks.com/ja/machine-learning/feature-store/uc/ui-uc.html)のサポートを正式提供しています。MLライフサイクルを通じてチームがシームレスに作業できるように、一つの集中管理された場所で、チームは特徴量、モデル、データ資産を発見、管理、制御することができます。このような洞察を把握することが難しい場合があるため、だからこそ我々はこの統合された世界でいくつかの機能を提供しています:\n\n## ガバナンス\n\n- **ワークスペース横断のガバナンス(正式提供):** MLOpsに関して最もいただいたリクエストは、プロダクションの特徴量とデータを、開発環境で利用できるようにするということでした。今ではすべてがUCにあるので、権限を制御する単一の場所が存在しています: チームはモデル、特徴量、トレーニングデータに対するワークスペースの読み込み/書き込みアクセスを許可することができます。これによって、開発、プロダクションのインフラストラクチャの分離を維持しつつも、ワークスペース横断の共有とコラボレーションを実現することができます。\n- **エンドツーエンドのリネージ(パブリックプレビュー):** データとAIを共存させることで、チームは全体のMLライフサイクルのエンドツーエンドのリネージを手に入れることができます。プロダクションのMLモデルに歪みが生じた場合には、インパクトを理解し、根本原因分析を行うためにリネージを活用することができます。リネージは、コンプライアンスのための監査レポートをの生成に役立つ、モデルのトレーニングで使用された正確なデータや、推論テーブルのデータを提供します。\n- **最先端モデルへのアクセス(パブリックプレビュー):** UCから管理、デプロイできる最先端のサードパーティモデルを[Databricksマーケットプレイス](https://qiita.com/taka_yayoi/items/22c635906275f112a4d0)からダウンロードすることができます。\n\n> 「我々の継続的な再トレーニングの要件において、最低限のレーテンシーで入出力をシームレスに連携できる推論テーブルが重要だったのでDatabricksを選択しました。さらに、Deltaテーブルにデータを送信するわかりやすい設定によって、再トレーニングのモニタリング、デバッグ、自動化において、馴染みのあるSQLやワークフローを活用することができます。これによって、我々のお客様は常に最新のモデルのメリットを享受できるようになりました。」— Shu Ming Peh, Lead Machine Learning Engineer at Hipages Group\n\n## デプロイメント\n\n- **1クリックのモデルデプロイメント(正式提供):** UCにおけるモデルは、1クリックで[Databricksによるモデルサービング](https://docs.databricks.com/ja/machine-learning/model-serving/index.html)のAPIとしてデプロイすることができます。もはや、Kubernetesのエキスパートになる必要はありません。モデルサービングは、CPUやGPUのサーバレスアーキテクチャを用いて、お使いのモデルのトラフィックに対応するために、自動でスケールアップ、スケールダウンします。ステージ分けされたロールアウトのための、A/Bテストのためのトラフィック分割はシンプルなUIの設定あるいはAPIで行うことができます。\n- **リアルタイムのオンデマンド特徴量のサービング(正式提供):** 我々のリアルタイム特徴量エンジニアリングサービスは、特徴量の検索や再計算のためのインフラストラクチャの構築の必要性を排除します。レイクハウスAIプラットフォームは、モデルの推論でどのようなデータや変換処理が必要なのかを理解し、特徴量の検索、結合のための低レーテンシーサービスを提供します。これは、オンライン/オフラインの偏りを防ぐだけではなく、複数のプロジェクトでこれらのデータ変換処理を共有することができます。\n- **MLOpsスタックによるプロダクション化(パブリックプレビュー):** 改善された[Databricks CLI](https://docs.databricks.com/ja/dev-tools/cli/index.html)は、Databricks REST APIをベースとしたワークフローを開発し、CI/CDと連携するためのビルディングブロックを提供します。[Databricks Asset Bundles](https://www.databricks.com/jp/blog/announcing-public-preview-databricks-assets-bundles-apply-software-development-best-practices)やBundlesによって、チームはレイクハウスにおいてプロジェクトをどのようにテストし、デプロイするのかを含むエンドツーエンドのプロジェクトをコード化することができます。本日、最新の[Big Book of MLOps](https://www.databricks.com/resources/ebook/the-big-book-of-mlops)で定義されている、MLOpsのベストプラクティスをカプセル化した[MLOps Stacks](https://github.com/databricks/mlops-stack)のパブリックプレビューを発表しました。MLOpsスタックは、堅牢かつ自動化された方法でモデルのプロダクション化を行うすぐに利用できるソリューションを提供するために、レイクハウスAIプラットフォームのすべてのピースをつなげるためにバンドルを活用しています。\n\n## モニタリング\n\n- **自動ペイロード記録(パブリックプレビュー):** [推論テーブル](https://docs.databricks.com/ja/machine-learning/model-serving/inference-tables.html)は、レイクハウスパラダイムの究極の具現化です。これらは、モデルのリクエストとレスポンスを格納するUCマネージドのDeltaテーブルです。推論テーブルは非常にパワフルであり、モニタリング、診断、トレーニングデータセットの作成、コンプライアンスの監査で活用することができます。バッチ推論においては、多くのチームではすでにこのようなテーブルを作成しています。オンライン推論においては、ペイロードの記録を自動化するためにお使いのエンドポイントで推論テーブルを有効化することができます。\n- **品質モニタリング(パブリックプレビュー):** [レイクハウスモニタリング](https://docs.databricks.com/ja/lakehouse-monitoring/index.html)を用いることで、モデルやデータパフォーマンスのドリフトに対するリアルタイムのアラートを行うために、推論テーブルやUnity Catalogの他のDeltaテーブルをモニタリングすることができます。モニタリングはパフォーマンスのメトリクスを可視化するためのダッシュボードを自動生成し、メトリクスが閾値を超えた際のリアルタイム通知を送信するためにアラートを設定することができます。\n\nこれらの機能のすべては、一つの集中管理されたガバナンスレイヤーでデータとAIの資産の両方を管理するレイクハウスAIプラットフォームでのみ実現可能です。そして、これらはMLOpsの美しい像を描きます: データサイエンティストはプロダクションデータを用いてモデルをトレーニングし、モニタリングダッシュボードを検証することでモデル品質の劣化を検知、デバッグし、プロダクションの推論テーブルを用いてモデルの予測結果にディープダイブし、オンラインのプロダクションモデルとオフラインのモデルを比較します。これによって、MLOpsのプロセスを加速し、モデルとデータの品質を改善、維持することができます。\n\n# 次のステップ\n\n上述した機能のすべてはパブリックプレビューあるいはGAです。[Big Book of MLOpsをダウンロード](https://www.databricks.com/resources/ebook/the-big-book-of-mlops)し、レイクハウスAIプラットフォームでのMLOpsジャーニーをスタートしましょう。プロフェッショナルサービスの参画やMLOpsのウォークスルーを希望される場合には、Databricksアカウントチームにコンタクトしてください。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksワークフローとジョブとDelta Live Tablesの関係性", "text": "自分もはじめはうまく理解できていなかったので、改めてここで整理します。まず、サマリーから書きます。\n![Screenshot 2023-10-19 at 17.03.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5aa4c841-a848-f3cd-9d99-bbde955a0ae5.png)\n\n\n- Databricksでオーケストレーションを一手に担っているのがDatabricksワークフローで、ワークフローにはDatabricksジョブとDelta Live Tablesが含まれています。ワークフロー自体は概念を指す言葉なので、実際の機能はDatabricksジョブとDelta Live Tables(DLT)が提供しています。\n- Databricksジョブではマルチタスクをサポートしており、タスクとしてはノートブックや**Delta Live Tablesのパイプラインを実行する**ことができます。\n- Delta Live Tablesはデータパイプラインを構築・運用するためのフレームワークであり、少ないコーディングで高信頼・高パフォーマンスのパイプラインを構築することができます。\n\n| 機能 | 用途 | 処理対象 |\n|:--|:--|:--|\n| Databricksジョブ  | マルチタスクのオーケストレーション  |  Databricks上の資産(ノートブック、SQLクエリー、Delta Live Tablesパイプライン、ダッシュボードなど) |\n| Delta Live Tables  | データパイプラインの構築・運用  | Delta Lakeテーブル  |\n\n\n# Databricksワークフローとは\n\nhttps://www.databricks.com/jp/product/workflows\n\nDatabricksワークフローは、Databricksのマネージドオーケストレーションサービスを意味します。実態としては、以下の2つの機能から構成されています。ワークフロー自体の機能は現時点では存在しておらず、以下の2つを包含する概念と理解していただくのがよろしいかと思います。\n\n- [Databricksジョブ](#databricksジョブとは)\n- [Delta Live Tables](#delta-live-tablesとは)\n\n# Databricksジョブとは\n\nhttps://docs.databricks.com/ja/workflows/jobs/create-run-jobs.html\n\nDatabricksジョブは、ノートブックのようなインタラクティブな方法で処理を行うのではなく、スケジュール実行、トリガー実行など人間を介さない形で処理を実行するための機能を提供するジョブスケジューラです。マルチタスクをサポートしており、ノートブック、SQLクエリー、Delta Live Tablesパイプラインなどの[タスク](https://docs.databricks.com/ja/workflows/jobs/create-run-jobs.html#task-type-options)を組み合わせてジョブを実行することができます。条件分岐をサポートしており[処理の並列化](https://www.databricks.com/jp/blog/2023/08/07/databricks-workflow-new-features)などで活用することができます。\n\n# Delta Live Tablesとは\n\nhttps://www.databricks.com/jp/product/delta-live-tables\n\nDatabricksでのデータパイプラインの構築を容易にするフレームワークです。SQLあるいはPythonでデータパイプラインの各ステップを宣言していただくだけで、複雑なパイプラインを容易に構築することが可能です。この他にもエラー時の対応、イベントログの取得、自動テストなどデータパイプラインの運用に不可欠な機能を提供しているので、データエンジニアの方はパイプラインのロジックの構築にフォーカスすることができます。\n\n# DatabricksジョブとDelta Live Tablesの組み合わせパターン\n\n1. Delta Live Tablesを用いて、S3に到着する新規ファイルを自動で取り込み、クレンジングや集計を行うデータパイプラインを開発します。\n![Screenshot 2023-10-19 at 17.06.53.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d7fca86c-e5b2-ea0b-acb0-65a13b61c6c9.png)\n1. パイプラインを実行してデバッグを行います。この際には**開発**モードを使用します。[開発モード](https://docs.databricks.com/ja/delta-live-tables/updates.html#development-and-production-modes)の場合、パイプラインが失敗しても自動リトライが実行されないので、繰り返しの開発が容易になります。また、クラスターも処理完了後に即時には停止しません。プロダクションモードの場合は、自動リトライが実行され、パイプライン終了後即座にクラスターが停止します。パイプラインの動作が確認できたら、プロダクションモードに切り替えます。\n![Screenshot 2023-10-19 at 16.54.41.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0bf77b58-4742-ab6d-6c4a-d19fdb2ffc46.png)\n1. テーブルの存在確認を行うなど前処理のSQLクエリーを作成します。\n1. Databricksジョブを作成します。最初のタスクでは上で作成したクエリーを実行するようにします。\n![Screenshot 2023-10-19 at 17.12.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/431ea80f-f4de-9290-bc63-70114fdecf5d.png)\n1. 以下の例では、存在確認が失敗した場合、すなわち、テーブルが存在しない場合にはDelta Live Tablesパイプラインを実行します。\n![Screenshot 2023-10-19 at 17.12.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f3a85522-9599-3129-4603-a5f2a3e76a7d.png)\n1. 存在確認が成功した場合、すなわち、テーブルが存在する際にはテーブルを削除した後に、Delta Live Tablesパイプラインを実行します。\n![Screenshot 2023-10-19 at 17.12.44.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2481efc7-11a1-19c6-5eee-a66efbc1b5c0.png)\n1. マルチタスクの各ステップの成功・失敗は**ジョブの実行**から確認することができます。\n![Screenshot 2023-10-19 at 17.15.01.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/db2aca5b-a59b-dc94-9959-95b83b65cc2b.png)\n\nこのジョブをスケジュール実行あるいはファイル到着を契機として実行することができます。また、上で示したように、即時実行するケースにおいても、複雑な処理をまとめ上げたいという用途でもご活用いただけます。\n![Screenshot 2023-10-19 at 17.16.05.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/77725829-512d-1825-620b-10aaa251f905.png)\n![Screenshot 2023-10-19 at 17.16.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/367faef8-d6db-1608-6d5d-05d45ef04157.png)\n\n非常に強力なツールですので、是非ご活用ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksでセマンティック検索がサポートされました！", "text": "大規模言語モデル(LLM)がどんどん組み込まれてきています。\n\nこちらの機能です。\n\nhttps://docs.databricks.com/en/search/index.html#semantic-search\n\n# セマンティック検索とは\n\n自然言語、かつ、言語の意味を加味した検索を行います。対象はUnity Catalogのテーブルとなります。現時点では、テーブルのメタデータのみを検索するものであり、テーブル内のデータは検索しません。\n\n# ウォークスルー\n\n普段使っている検索ボックスで自然言語による検索がサポートされています。日本語でも動きます。\n\n![Screenshot 2023-10-18 at 8.00.20.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/890c290c-2130-c543-2b92-ee2c9fce54ff.png)\n\n`列数の多いテーブル`のような検索も可能です。\n![Screenshot 2023-10-18 at 8.01.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7524534c-9347-f870-4782-27df4e2a866b.png)\n\n是非ご活用ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksのテーブルに対するAI提案コメントがサポートされました！", "text": "日本リージョンでもデプロイされたので記事にまとめます。\n\n# AI提案コメントとは\n\nテーブルのメタデータ(テーブル名、カラム名、データ型など)からコメントを自動生成する機能です。\n\n# ウォークスルー\n\nカタログエクスプローラからテーブルにアクセスするとテーブルのコメントが提案されます。そのまま提案を受け入れる場合には**承認**、編集する場合には**編集**を選択します。\n![Screenshot 2023-10-14 at 20.49.45.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/eea5b81c-de48-d10a-f165-54f27af702ba.png)\n\n列一覧の右上にある**AI生成**をクリックすると、コメントの無い列のコメントが生成されます。チェックマークをクリックすることで承認となります。\n![Screenshot 2023-10-14 at 20.51.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/be4df800-d36b-d2f2-30ad-c855dc0ecb09.png)\n\n![Screenshot 2023-10-14 at 20.52.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/fe9c10be-c343-a00b-0582-6c39aa7084c8.png)\n\n正直なところ、言語設定に合わせてコメントの言語も切り替えてほしいですよね…。是非、フィードバックお願いします！\n![Screenshot 2023-10-17 at 15.47.50.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/31411d91-3cfc-860b-c871-1fe7726907d1.png)\n\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksマーケットプレイスでMaaS Tech Japanの地理空間データを試す", "text": "気づいたらこちらも公開されていました。[unerry様](https://qiita.com/taka_yayoi/items/af35659038f8d77a59a9)に続き、続々と各社様にプロバイダーになっていただいています！\n\nhttps://marketplace.databricks.com/details/9fa49603-c4c7-4152-8faa-208aad1b1a40/MaaS-Tech-Japan-inc_H3-IndexResolution-12\n\n![Screenshot 2023-10-16 at 12.44.30.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9513879b-4b2a-4521-65af-dedc2ef2009b.png)\n\nカタログに取り込みます。\n![Screenshot 2023-10-16 at 12.44.54.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ef3768ac-33c7-c7aa-9286-167d7763c39a.png)\n\n取り込まれました。\n![Screenshot 2023-10-16 at 12.45.14.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ef1bbca1-53a1-ef2c-0648-f287b1cbd8fa.png)\n\n![Screenshot 2023-10-16 at 12.45.30.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/55121204-0789-f1e2-f2ba-08957f6a4f8f.png)\n\nまさに地理空間情報。\n![Screenshot 2023-10-16 at 12.53.31.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1515c6a2-9214-c7cf-9c08-86fa1a9723a7.png)\n\nDatabricks SQLでクエリーを行い、Mapタイプのビジュアライゼーションを使います。\n\n```sql\nSELECT * FROM districts;\n```\n\n![Screenshot 2023-10-16 at 12.54.52.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c4cf5708-d853-861c-1e27-7843562ad2e0.png)\n\nポップアップの書式を変更します。\n![Screenshot 2023-10-16 at 12.56.41.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f3d29fcb-0189-faa2-7740-f618d4e7f279.png)\n\nこれだけの手順でデータを可視化することができました。\n![Screenshot 2023-10-16 at 12.57.36.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/03b72c4a-fcf3-6e0c-f027-4981f0e79060.png)\n\n[地理空間関数](https://docs.databricks.com/ja/sql/language-manual/sql-ref-h3-geospatial-functions.html)の練習にも使えそうです。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "コンピュータビジョンとリアルタイムサーバレス推論を用いたプリント基盤(PCB)の製品品質調査", "text": "こちらで説明されているノートブックとは別に、より簡易的で理解しやすいものが公開されていました。\n\nhttps://qiita.com/taka_yayoi/items/887343d522fceb7e793c\n\nこちらの最初に表示されている[Get started](https://d1r5llqwmkrl74.cloudfront.net/notebooks/MFG/cv-quality-inspection/index.html)のリンクからアクセスできるノートブックです。\n\nhttps://www.databricks.com/solutions/accelerators/product-quality-inspection\n\n日本語にローカライズしたノートブックはこちらです。\n\nhttps://github.com/taka-yayoi/public_repo_2/tree/master/cv-quality-inspection-jpn\n\n# クラスターの準備\n\nGPUクラスターを使用します。テーブルはUnity Catalogに保存していますので、シングルユーザーアクセスモードを指定しています。\n![Screenshot 2023-10-15 at 15.16.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1bc7c22f-29a7-6811-d4ee-b135a67fd08e.png)\n\n\n# 00_IngestionPCB\n\n![](https://raw.githubusercontent.com/databricks-industry-solutions/cv-quality-inspection/main/images/PCB1.png)\n\nこのソリューションアクセラレータでは、製品の品質調査のためのエンドツーエンドのパイプラインのデプロイにおいて、Databricksがどのように役立つのかを説明します。モデルは[サーバレスリアルタイム推論](https://docs.databricks.com/archive/serverless-inference-preview/serverless-real-time-inference.html)を用いてデプロイされます。\n\n[Visual Anomaly (VisA)](https://registry.opendata.aws/visa/)検知データセットを用い、PCB画像の以上を検知するためのパイプラインを構築します。\n\n## なぜ画像による品質調査を？\n\n画像による品質調査は製造業の文脈においては一般的な課題となっています。スマートマニュファクチャリングを提供する際の鍵となります。\n\n## プロダクションレベルのパイプラインの実装\n\n事前学習済みディープラーニングモデル、転送学習、高レベルのフレームワークによって、近年では画像分類問題は簡単になってきています。データサイエンスチームはそのようなモデルをクイックにデプロイすることはできますが、プロダクションレベルのエンドツーエンドのパイプラインの実装、画像の利用、MLOps/ガバナンスの必要性、最終的な結果の提供においては、依然として本当の課題が存在し続けています。\n\nDatabricksレイクハウスは、このような全体的なプロセスをシンプルにするように設計されており、データサイエンティストはコアのユースケースにフォーカスすることができます。\n\n品質調査モデルを構築するために、Torchvisionを使用します。しかし、他のライブラリで同じアーキテクチャを活用することも可能です。TorchvisionライブラリはPyTorchプロジェクトの一部であり、ディープラーニングで人気のフレームワークとなっています。Torchvisionは、モデルアーキテクチャ、よく使われるデータセット、画像のトランスフォーマーと共に提供されています。\n\nパイプライン構築の最初のステップは、データの取り込みです。Databricksでは、画像(非構造化データ)を含む任意のソースのデータをロードすることができます。これは、効率的かつ分散された方法で画像のコンテンツとともにテーブルに格納され、ラベルと関連づけられます。\n\n## 品質調査画像パイプライン\n\nこれが我々が構築するパイプラインです。2つのデータセットを取り込みます。すなわち:\n\n* PCBを含む生のサテライトイメージ(jpg)\n* CSVとして保存されている不良のタイプを示すラベル\n\n最初にこのデータをインクリメンタルにロードするデータパイプラインにフォーカスし、最終的なゴールドテーブルを作成します。\n\nこのテーブルは、我々の画像からリアルタイムで異常を検知するために、ML分離モデルをトレーニングするために活用されます！\n\n![](https://raw.githubusercontent.com/databricks-industry-solutions/cv-quality-inspection/main/images/pipeline.png)\n\n### https://registry.opendata.aws/visa/ からデータセットをダウンロード\n\n[https://registry.opendata.aws/visa/](https://registry.opendata.aws/visa/)からデータセットをダウンロードするために`bash`コマンドを使います。\n\nデータはAWS S3に格納されているので、AWS CLIライブラリ(`awscli`)をインストールする必要があります。\n\n```py\n%pip install awscli\n```\n\n```sh\n%sh\nmkdir -p /tmp/data\naws s3 cp --no-progress --no-sign-request s3://amazon-visual-anomaly/VisA_20220922.tar /tmp\n```\n```\ndownload: s3://amazon-visual-anomaly/VisA_20220922.tar to ../../tmp/VisA_20220922.tar\n```\n\n```sh\n%sh\nmkdir -p /tmp/data\ntar xf /tmp/VisA_20220922.tar --no-same-owner -C /tmp/data/ \n```\n\n## いくつかのPCB画像を見てみましょう\n\nネイティブなPythonの方法で`matplotlib`を使って画像を表示することができます。\n\n正常な画像がどのようなものであるのか、異常があるものがどのようなものであるかを見てみましょう。\n\n```py\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef display_image(path, dpi=300):\n    img = Image.open(path)\n    width, height = img.size\n    plt.figure(figsize=(width / dpi, height / dpi))\n    plt.imshow(img, interpolation=\"nearest\", aspect=\"auto\")\n\n\ndisplay_image(\"/tmp/data/pcb1/Data/Images/Normal/0000.JPG\")\ndisplay_image(\"/tmp/data/pcb1/Data/Images/Anomaly/000.JPG\")\n```\n![Screenshot 2023-10-15 at 15.15.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/aef618d6-1de0-fd30-441f-9832d779ab33.png)\n\n## データをDBFSに移動しましょう\n\n:::note\nクイックな覚書: Databricksファイルシステム(DBFS)は、Databricksワークスペースにマウントされ、Databricksクラスターから利用できる分散ファイルシステムです。DBFSは、ネイティブなクラウドストレージのAPIコールをUnixライクなファイルシステムコールにマッピングする、スケーラブルなオブジェクトストレージ上の抽象化レイヤーとなります。\n:::\n\n```sh\n%sh\nrm -rf /dbfs/pcb1\nmkdir -p /dbfs/pcb1/labels \ncp -r /tmp/data/pcb1/Data/Images/ /dbfs/pcb1/\ncp /tmp/data/pcb1/image_anno.csv /dbfs/pcb1/labels/\n```\n\n```sql\n%sql\nUSE takaakiyayoi_catalog.pcb;\nDROP TABLE IF EXISTS circuit_board;\nDROP TABLE IF EXISTS circuit_board_gold;\nDROP TABLE IF EXISTS circuit_board_label;\n```\n\n```py\ncloud_storage_path=\"/pcb1\"\n```\n\n### Auto LoaderによるCSVラベルファイルをロード\n\nDatabricksの[Auto Loader](https://docs.databricks.com/ja/ingestion/auto-loader/index.html)を用いることで、CSVファイルを簡単にロードすることができます。\n\n```py\nfrom pyspark.sql.functions import substring_index, col\n\n(\n    spark.readStream.format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"csv\")\n    .option(\"header\", True)\n    .option(\"cloudFiles.schemaLocation\", f\"{cloud_storage_path}/circuit_board_label_schema\")\n    .load(f\"{cloud_storage_path}/labels/\")\n    .withColumn(\"filename\", substring_index(col(\"image\"), \"/\", -1))\n    .select(\"filename\", \"label\")\n    .withColumnRenamed(\"label\", \"labelDetail\")\n    .writeStream.trigger(availableNow=True)\n    .option(\"checkpointLocation\", f\"{cloud_storage_path}/circuit_board_label_checkpoint\")\n    .toTable(\"circuit_board_label\")\n    .awaitTermination()\n)\ndisplay(spark.table(\"circuit_board_label\"))\n```\n![Screenshot 2023-10-15 at 15.18.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a1b03db1-e01d-ab4e-e839-972cc78619a3.png)\n\n### Auto Loaderによるバイナリーファイルのロード\n\nこれで、画像をロードするためにAuto Loaderを用い、ラベルのカラムを作成するためにspark関数を活用することができます。\nまた、テーブルとして画像のコンテンツとラベルを簡単に表示することができます。\n\n```py\nfrom pyspark.sql.functions import substring_index, col, when\n\n(\n    spark.readStream.format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"binaryFile\")\n    .option(\"pathGlobFilter\", \"*.JPG\")\n    .option(\"recursiveFileLookup\", \"true\")\n    .option(\"cloudFiles.schemaLocation\", f\"{cloud_storage_path}/circuit_board_schema\")\n    .load(f\"{cloud_storage_path}/Images/\")\n    .withColumn(\"filename\", substring_index(col(\"path\"), \"/\", -1))\n    .withColumn(\n        \"labelName\",\n        when(col(\"path\").contains(\"Anomaly\"), \"anomaly\").otherwise(\"normal\"),\n    )\n    .withColumn(\"label\", when(col(\"labelName\").eqNullSafe(\"anomaly\"), 1).otherwise(0))\n    .select(\"filename\", \"content\", \"label\", \"labelName\")\n    .writeStream.trigger(availableNow=True)\n    .option(\"checkpointLocation\", f\"{cloud_storage_path}/circuit_board_checkpoint\")\n    .toTable(\"circuit_board\")\n    .awaitTermination()\n)\ndisplay(spark.table(\"circuit_board\"))\n```\n![Screenshot 2023-10-15 at 15.19.40.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/92f4b39d-9e6d-ab77-2c82-326c6ccdb34f.png)\n\n## ラベルと画像テーブルをマージしましょう\n\n```sql\n%sql\nCREATE TABLE circuit_board_gold as (\n  select\n    cb.*,\n    labelDetail\n  from\n    circuit_board cb\n    inner join circuit_board_label cbl on cb.filename = cbl.filename\n);\n```\n\n## 画像テーブルに対する自動最適化を有効化することができます\n\n自動最適化は2つの補完的な機能となります: 最適化書き込みとオートコンパクションです。\n\n```sql\n%sql\nALTER TABLE circuit_board_gold SET TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true);\nALTER TABLE circuit_board SET TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)\n```\n\nこのテーブルに対して任意のSQLコマンドを実行することができます。\n\n```sql\n%sql\nselect\n  *\nfrom\n  circuit_board_gold\nlimit 10\n```\n![Screenshot 2023-10-15 at 15.20.53.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/894ed17e-b5a6-5958-972e-cf751733872e.png)\n\n### データサインスチームが活用するデータセットの準備ができました\n\nこれですべてです！プロダクションレベルのデータ取り込みパイプラインをデプロイしました。\n\n我々の画像はインクリメンタルに取り込まれ、ラベルデータセットと結合されます。\n\n異常検知に必要なモデルを構築するために、データサイエンティストによってこのデータがどのように活用されるのかを見てみましょう。\n\n# 01_ImageClassificationPytorch\n\n**pytorchモデルの実装とデプロイ**\n\nデータサイエンティストとしての次のステップは、画像分類を実行するためにMLモデルを実装することです。\n\nトレーニングデータセットとして以前のデータパイプラインで構築したゴールドテーブルを再利用します。\n\n[torchvision](https://pytorch.org/vision/stable/index.html)を用いることで、このようなモデルの構築が非常にシンプルになります。\n\n## MLOpsのステップ\n\n画像分類モデルの構築は簡単に終わるかもしれませんが、プロダクション環境にモデルをデプロイすることはさらに難しいものとなります。\n\nDatabricksにおいては、以下を提供するMLflowの助けを借りて、このプロセスをシンプルにし、価値創出に至るジャーニーを加速します。\n\n* 進捗を追跡し続ける自動エクスペリメント追跡\n* ベストなモデルを得るためにhyperoptを用いたシンプルかつ分散されたハイパーパラメータチューニング\n* MLフレームワークを抽象化し、MLflowにモデルをパッケージング\n* ガバナンスのためのモデルレジストリ\n* バッチやリアルタイムのサービング(1クリックでのデプロイメント)\n\n```py\nmodel_name = \"cv_pcb_classification_taka\" # 適宜変更\n```\n\n## GPUの有効化\n\nディープラーニングでは、トレーニングでGPUを用いることが合理的です。\n\n```py\nimport torch\n\n# GPUを利用できるかどうかをチェック\nif not torch.cuda.is_available():  # gpuが利用できるかどうか\n    raise Exception(\n        \"Please use a GPU-cluster for model training, CPU instances will be too slow\"\n    )\n```\n\n```py\n# Spark/Pythonバージョンの確認\nimport sys\n\nprint(\n    \"You are running a Databricks {0} cluster leveraging Python {1}\".format(\n        spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\"),\n        sys.version.split(\" \")[0],\n    )\n)\n```\n```\nYou are running a Databricks 13.3.x-gpu-ml-scala2.12 cluster leveraging Python 3.10.12\n```\n\n```py\nfrom petastorm.spark import SparkDatasetConverter, make_spark_converter\nfrom petastorm import TransformSpec\n\nfrom PIL import Image\n\nimport torchvision\nimport torch\n\nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\nimport horovod.torch as hvd\nfrom sparkdl import HorovodRunner\n\nimport mlflow\n\nimport pyspark.sql.functions as f\n\nimport numpy as np\nfrom functools import partial\nimport io\nimport uuid\n\nusername = (\n    dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n)\nmlflow.set_experiment(\"/Users/{}/pcbqi\".format(username))\n\npetastorm_path = (\n    f\"file:///dbfs/tmp/petastorm/{str(uuid.uuid4())}/cache\"  # petastormのキャッシュファイルの格納場所\n)\n```\n\n## データをトレーニング/テストデータセットとして分割\n\n他のMLモデルと同様に、画像をトレーニング/テストデータセットに分割するところからスタートします。\n\n```sql\n%sql\nUSE takaakiyayoi_catalog.pcb;\n```\n```py\n# 対象画像の取得\nimages = spark.table(\"circuit_board_gold\").select(\n    \"content\", \"label\", \"filename\"\n)  # 次のステップではユニークなIDとしてパスを使用します\n\n# 階層化された画像のサンプルの取得\nimages_train = images.sampleBy(\n    \"label\", fractions={0: 0.8, 1: 0.8}\n)  # トレーニングにはそれぞれのクラスの80%をサンプリング\nimages_test = images.join(\n    images_train, on=\"filename\", how=\"leftanti\"\n)  # 残りはテストに使用\n\n# 不要なフィールドを削除\nimages_train = images_train.drop(\"filename\").repartition(\n    sc.defaultParallelism\n)  # パスのIDの削除\nimages_test = images_test.drop(\"filename\").repartition(sc.defaultParallelism)\n\n# サンプリングの検証\ndisplay(\n    images_train.withColumn(\"eval_set\", f.lit(\"train\"))\n    .union(images_test.withColumn(\"eval_set\", f.lit(\"test\")))\n    .groupBy(\"eval_set\", \"label\")\n    .agg(f.count(\"*\").alias(\"instances\"))\n    .orderBy(\"eval_set\", \"label\")\n)\n```\n![Screenshot 2023-10-15 at 15.23.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f29553a8-edd0-d727-fb2c-cf2868b59b78.png)\n\n## DLとpetastormのためのDeltaテーブル\n\n我々のデータは現在Deltaテーブルに格納されており、Sparkデータフレームとして利用することができます。しかし、pytorchでは特定のデータタイプが期待されます。\n\nこれを解決するために、テーブルからデータをモデルに自動で送信するように、PetastormとSparkコンバーターを活用します。このコンバーターは高速処理のために、ローカルキャッシュを用いてデータをインクリメンタルにロードします。詳細は[関連ドキュメント](https://docs.databricks.com/ja/machine-learning/load-data/petastorm.html)をご覧ください。\n\n```py\ntry:\n    dbutils.fs.rm(petastorm_path, True)\nexcept:\n    pass\n```\n\n```py\n# petastoreキャッシュの宛先を設定\nspark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, petastorm_path)\n\n# データセットのおおよそのバイト数を特定\nbytes_in_train = (\n    images_train.withColumn(\"bytes\", f.lit(4) + f.length(\"content\"))\n    .groupBy()\n    .agg(f.sum(\"bytes\").alias(\"bytes\"))\n    .collect()[0][\"bytes\"]\n)\nbytes_in_test = (\n    images_test.withColumn(\"bytes\", f.lit(4) + f.length(\"content\"))\n    .groupBy()\n    .agg(f.sum(\"bytes\").alias(\"bytes\"))\n    .collect()[0][\"bytes\"]\n)\n\n# 画像データのキャッシュ\nconverter_train = make_spark_converter(\n    images_train,\n    parquet_row_group_size_bytes=int(bytes_in_train / sc.defaultParallelism),\n)\nconverter_test = make_spark_converter(\n    images_test, parquet_row_group_size_bytes=int(bytes_in_test / sc.defaultParallelism)\n)\n```\n\n```py\nNUM_CLASSES = 2  # ラベルは2クラス (0 あるいは 1)\n```\n\n## Torchvision\n\nTorchvisionは再利用できる事前学習済みモデルを提供します。\n\n```py\nfrom torchvision.models import (\n    ViT_B_16_Weights,\n    vit_b_16,\n)\n\n\ndef get_model():\n    # 事前学習済みモデルにアクセス\n    weights = ViT_B_16_Weights.DEFAULT\n    model = vit_b_16(weights=weights)\n\n    # 転送学習のために新たな分類レイヤーを追加\n    num_ftrs = model.heads.head.in_features\n\n    # 新たに構成されたモジュールのパラメーターでは、デフォルトで requires_grad=True が設定されています\n    model.heads.head = torch.nn.Linear(num_ftrs, NUM_CLASSES)\n\n    return model, weights\n```\n\n```py\nmodel, weights = get_model()\ntransforms = weights.transforms()\nprint(model.heads)\nprint(transforms)\n```\n\n```\nSequential(\n  (head): Linear(in_features=768, out_features=2, bias=True)\n)\nImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n```\n\n```py\n# 画像変換のロジックを定義\ndef transform_row(is_train, batch_pd):\n\n    # 画像にパイプラインを適用\n    batch_pd[\"features\"] = batch_pd[\"content\"].map(\n        lambda x: np.ascontiguousarray(\n            transforms(Image.open(io.BytesIO(x)).convert(\"RGB\")).numpy()\n        )\n    )\n\n    # ラベルの変換 (我々の評価メトリックは値が float32 であることを期待します)\n    # -----------------------------------------------------------\n    batch_pd[\"label\"] = batch_pd[\"label\"].astype(\"float32\")\n    # -----------------------------------------------------------\n\n    return batch_pd[[\"features\", \"label\"]]\n\n\n# 変換の仕様を取得する関数の定義\ndef get_transform_spec(is_train=True):\n\n    spec = TransformSpec(\n        partial(transform_row, is_train),  # 行を取得/変換するために呼び出す関数\n        edit_fields=[  # 関数によって返却される行のスキーマ\n            (\"features\", np.float32, (3, 224, 224), False),\n            (\"label\", np.float32, (), False),\n        ],\n        selected_fields=[\"features\", \"label\"],  # モデルに送信するスキーマのフィールド\n    )\n\n    return spec\n```\n\n```py\n# petastormのキャッシュにアクセスし、仕様を用いてデータを変換\nwith converter_train.make_torch_dataloader(\n    transform_spec=get_transform_spec(is_train=True), batch_size=1\n) as train_dataloader:\n\n    # キャッシュからレコードを取得\n    for i in iter(train_dataloader):\n        print(i)\n        break\n```\n\n```py\nBATCH_SIZE = 32  # 一度に32画像を処理\nNUM_EPOCHS = 15  # すべての画像を5回繰り返し\n```\n\n```py\nfrom sklearn.metrics import f1_score\n\n\ndef train_one_epoch(\n    model,\n    criterion,\n    optimizer,\n    scheduler,\n    train_dataloader_iter,\n    steps_per_epoch,\n    epoch,\n    device,\n):\n\n    model.train()  # モデルをトレーニングモードに設定\n\n    # 統計情報\n    running_loss = 0.0\n    running_corrects = 0\n    running_size = 0\n\n    # 1つのエポックのデータに対するイテレーション\n    for step in range(steps_per_epoch):\n\n        # petastormから次のバッチを取得\n        pd_batch = next(train_dataloader_iter)\n\n        # 入力の特徴量とラベルを分離\n        inputs, labels = pd_batch[\"features\"].to(device), pd_batch[\"label\"].to(device)\n\n        # トレーニングの履歴を追跡\n        with torch.set_grad_enabled(True):\n\n            # パラメーターの勾配をゼロに\n            optimizer.zero_grad()\n\n            # フォワード\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            probs = torch.nn.functional.softmax(outputs, dim=0)[:, 1]\n            loss = criterion(probs, labels)\n\n            # バックワード + 最適化\n            loss.backward()\n            optimizer.step()\n\n        # 統計情報\n        running_loss += loss.item()\n        running_corrects += torch.sum(preds == labels.data)\n        running_size += inputs.size(0)\n\n    scheduler.step()\n\n    epoch_loss = running_loss / steps_per_epoch\n    epoch_acc = running_corrects.double() / running_size\n\n    print(\"Train Loss: {:.4f} Acc: {:.4f}\".format(epoch_loss, epoch_acc))\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(\n    model, criterion, test_dataloader_iter, test_steps, device, metric_agg_fn=None\n):\n\n    model.eval()  # モデルを評価モードに設定\n\n    # 統計情報\n    running_loss = 0.0\n    running_corrects = 0\n    running_size = 0\n    f1_scores = 0\n\n    # すべての検証データに対してイテレーション\n    for step in range(test_steps):\n\n        pd_batch = next(test_dataloader_iter)\n        inputs, labels = pd_batch[\"features\"].to(device), pd_batch[\"label\"].to(device)\n\n        # メモリーを節約するために評価の際には履歴を追跡しない\n        with torch.set_grad_enabled(False):\n\n            # フォワード\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            probs = torch.nn.functional.softmax(outputs, dim=1)[:, 1]\n            loss = criterion(probs, labels)\n\n        # 統計情報\n        running_loss += loss.item()\n        running_corrects += torch.sum(preds == labels.data)\n        running_size += inputs.size(0)\n        f1_scores += f1_score(labels.cpu().data, preds.cpu())\n\n    # それぞれのミニバッチにおける結果におけるロスを平均\n    epoch_loss = running_loss / test_steps\n    epoc_f1 = f1_scores / test_steps\n    epoch_acc = running_corrects.double() / running_size\n\n    # すべえのワーカーのメトリクスを集計するために分散トレーニングで metric_agg_fn を使用\n    if metric_agg_fn is not None:\n        epoch_loss = metric_agg_fn(epoch_loss, \"avg_loss\")\n        epoch_acc = metric_agg_fn(epoch_acc, \"avg_acc\")\n        epoc_f1 = metric_agg_fn(epoc_f1, \"avg_f1\")\n\n    print(\n        \"Testing Loss: {:.4f} Acc: {:.4f} F1: {:.4f}\".format(\n            epoch_loss, epoch_acc, epoc_f1\n        )\n    )\n    return epoch_loss, epoch_acc, epoc_f1\n```\n\n```py\nimport pandas as pd\n\npd.options.mode.chained_assignment = None  # default='warn'\n\n\ndef train_and_evaluate(lr=0.001):\n\n    # 計算処理でGPUを使えるかどうかをチェック\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # モデルの取得\n    model, _ = get_model()\n\n    # 特定されたプロセッサーデバイスのプロセスにモデルを割り当て\n    model = model.to(device)\n\n    # バイナリークロスエントロピーに最適化\n    criterion = torch.nn.BCELoss()\n\n    # 最終レイヤーのパラメーターのみを最適化\n    filtered_params = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.AdamW(filtered_params, lr=lr)\n\n    # 7エポックごとに0.1の因数でLRを減衰\n    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer, step_size=7, gamma=0.1\n    )\n\n    # petastormキャッシュのデータにアクセス\n    with converter_train.make_torch_dataloader(\n        transform_spec=get_transform_spec(is_train=True), batch_size=BATCH_SIZE\n    ) as train_dataloader, converter_test.make_torch_dataloader(\n        transform_spec=get_transform_spec(is_train=False), batch_size=BATCH_SIZE\n    ) as val_dataloader:\n\n        # データアクセスのためのイテレータと必要なサイクル数を定義\n        train_dataloader_iter = iter(train_dataloader)\n        steps_per_epoch = len(converter_train) // BATCH_SIZE\n\n        val_dataloader_iter = iter(val_dataloader)\n        validation_steps = max(1, len(converter_test) // BATCH_SIZE)\n\n        # それぞれのエポックに対して\n        for epoch in range(NUM_EPOCHS):\n\n            print(\"Epoch {}/{}\".format(epoch + 1, NUM_EPOCHS))\n            print(\"-\" * 10)\n\n            # トレーニング\n            train_loss, train_acc = train_one_epoch(\n                model,\n                criterion,\n                optimizer,\n                exp_lr_scheduler,\n                train_dataloader_iter,\n                steps_per_epoch,\n                epoch,\n                device,\n            )\n            # 評価\n            val_loss, val_acc, val_f1 = evaluate(\n                model, criterion, val_dataloader_iter, validation_steps, device\n            )\n\n    # accで問題のあるタイプを訂正\n    if type(val_acc) == torch.Tensor:\n        val_acc = val_acc.item()\n\n    return model, val_loss, val_acc, val_f1  # テンソルから値を抽出\n\n\n# model, loss, acc, f1 = train_and_evaluate(**{'lr':0.00001})\n```\n\n## Hyperoptによるハイパーパラメータチューニング\n\nモデルの準備ができました。このようなモデルのチューニングは複雑なものです。アーキテクチャ、エンコーダー、学習率のようなハイパーパラメーターに対する選択肢が存在します。\n\n我々のためにベストなハイパーパラメーターを探してもらえるように、Hyperoptを活用しましょう。Hyperoptは分散処理でも動作し、トレーニングプロセスをスピードアップするために、複数インスタンスで並列にトレーニングを実行できることにも注意してください。\n\n```py\n# ハイパーパラメータの探索空間の定義\nsearch_space = {\n    \"lr\": hp.loguniform(\"lr\", np.log(1e-5), np.log(1.2e-5)),\n}\n\n\n# hyperoptが期待する形式の結果を返却するトレーニング関数の定義\ndef train_fn(params):\n\n    # 指定されたハイパーパラメーターの設定を用いてモデルをトレーニング\n    model, loss, acc, f1 = train_and_evaluate(**params)\n\n    # 透明性を確保するためにこのイテレーションをmlflowに記録\n    mlflow.log_metric(\"accuracy\", acc)\n\n    mlflow.log_metric(\"f1\", f1)\n\n    mlflow.pytorch.log_model(model, \"model\")\n    # このイテレーションの結果を返却\n    return {\"loss\": loss, \"status\": STATUS_OK}\n\n\n# 適用する並列度を決定\nif torch.cuda.is_available():  # GPUの場合\n    nbrWorkers = sc.getConf().get(\"spark.databricks.clusterUsageTags.clusterWorkers\")\n    if nbrWorkers is None:  # gcp\n        nbrWorkers = sc.getConf().get(\n            \"spark.databricks.clusterUsageTags.clusterTargetWorkers\"\n        )\n    parallelism = int(nbrWorkers)\n    if parallelism == 0:  # シングルノードのクラスター\n        parallelism = 1\nelse:  # CPUの場合\n    parallelism = sc.defaultParallelism\n\n# 分散ハイパーパラメーターチューニングの実行\nwith mlflow.start_run(run_name=model_name) as run:\n\n    argmin = fmin(\n        fn=train_fn,\n        space=search_space,\n        algo=tpe.suggest,\n        max_evals=1,  # ハイパーパラメーターランの合計数 (通常この値はもっと大きなものにします)\n        trials=SparkTrials(parallelism=parallelism),\n    )  # 並列に実行されるハイパーパラメーターランの数\n```\n\n```py\nargmin\n```\n```\n{'lr': 1.1829944020028542e-05}\n```\n\n### Horovodによる分散ディープラーニング\n\nより多くのエポックでモデルをトレーニングできるようになりました。ランを加速させるには、Sparkクラスターの複数ノードでトレーニングを分散させることができます。\n\n詳細は[Horovod](https://docs.databricks.com/ja/machine-learning/train-model/distributed-training/horovod-runner.html)のドキュメントをご覧ください。\n\n```py\n# モデル評価関数の定義\ndef metric_average_hvd(val, name):\n    tensor = torch.tensor(val)\n    avg_tensor = hvd.allreduce(tensor, name=name)\n    return avg_tensor.item()\n\n\n# 分散トレーニング & 評価の関数の定義\ndef train_and_evaluate_hvd(lr=0.001):\n\n    # Step 1: Horovodの初期化\n    hvd.init()\n\n    # Step 2: 特定のCPUコア、あるいはGPUにHorovodプロセスを割り当て\n\n    # トレーニングに使用するデバイスの特定\n    if torch.cuda.is_available():  # gpu\n        torch.cuda.set_device(hvd.local_rank())\n        device = torch.cuda.current_device()\n    else:\n        device = torch.device(\"cpu\")  # cpu\n\n    # モデルの取得及びデバイスへの割り当て\n    model, _ = get_model()\n    model = model.to(device)\n    criterion = torch.nn.BCELoss()\n\n    # Step 3: Horovodプロセスの数に基づいて学習率をスケール\n    filtered_params = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.AdamW(filtered_params, lr=lr * hvd.size())\n\n    # Step 4: 分散処理のためにオプティマイザをラッピング\n    optimizer_hvd = hvd.DistributedOptimizer(\n        optimizer, named_parameters=model.named_parameters()\n    )\n    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer_hvd, step_size=7, gamma=0.1\n    )\n\n    # Step 5: Horovodプロセスに関連づけられる状態変数の初期化\n    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n    hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n\n    # petastormキャッシュへのアクセスを解放\n    with converter_train.make_torch_dataloader(\n        transform_spec=get_transform_spec(is_train=True),\n        cur_shard=hvd.rank(),\n        shard_count=hvd.size(),\n        batch_size=BATCH_SIZE,\n    ) as train_dataloader, converter_test.make_torch_dataloader(\n        transform_spec=get_transform_spec(is_train=False),\n        cur_shard=hvd.rank(),\n        shard_count=hvd.size(),\n        batch_size=BATCH_SIZE,\n    ) as test_dataloader:\n\n        # それぞれのコア/GPUがバッチを処理します\n        train_dataloader_iter = iter(train_dataloader)\n        train_steps = len(converter_train) // (BATCH_SIZE * hvd.size())\n        test_dataloader_iter = iter(test_dataloader)\n        test_steps = max(1, len(converter_test) // (BATCH_SIZE * hvd.size()))\n\n        # データセットに対するイテレーション\n        for epoch in range(NUM_EPOCHS):\n\n            # エポック情報の表示\n            print(\"Epoch {}/{}\".format(epoch + 1, NUM_EPOCHS))\n            print(\"-\" * 10)\n\n            # モデルのトレーニング\n            train_loss, train_acc = train_one_epoch(\n                model,\n                criterion,\n                optimizer_hvd,\n                exp_lr_scheduler,\n                train_dataloader_iter,\n                train_steps,\n                epoch,\n                device,\n            )\n\n            # モデルの評価\n            test_loss, test_acc, f1_acc = evaluate(\n                model,\n                criterion,\n                test_dataloader_iter,\n                test_steps,\n                device,\n                metric_agg_fn=metric_average_hvd,\n            )\n\n    return test_loss, test_acc, f1_acc, model\n```\n\n```py\n# horovodで利用できる並列度の特定\nif torch.cuda.is_available():  # gpuの場合\n    nbrWorkers = sc.getConf().get(\"spark.databricks.clusterUsageTags.clusterWorkers\")\n    if nbrWorkers is None:  # gcp\n        nbrWorkers = sc.getConf().get(\n            \"spark.databricks.clusterUsageTags.clusterTargetWorkers\"\n        )\n    parallelism = int(nbrWorkers)\n    if parallelism == 0:  # シングルノードのクラスター\n        parallelism = 1\nelse:\n    parallelism = 2  # 小規模なデータでは並列度を2と低く設定。それ以外の場合には、sc.defaultParallelismに設定することも可能\n\n# horovodの実行環境の初期化\nhr = HorovodRunner(np=parallelism)\n\n# 分散トレーニングの実行\nwith mlflow.start_run(run_name=model_name) as run:\n\n    # モデルのトレーニングと評価\n    loss, acc, f1, model = hr.run(\n        train_and_evaluate_hvd, **argmin\n    )  # argminにはチューニングされたハイパーパラメーターが含まれます\n\n    # mlflowにモデルを記録\n    mlflow.log_params(argmin)\n    mlflow.log_metrics({\"loss\": loss, \"accuracy\": acc, \"f1\": f1})\n    mlflow.pytorch.log_model(model, \"model\")\n```\n\n## プロダクション環境にモデルをデプロイ\n\nモデルのトレーニングが完了しました。やらなくてはいけないことは、(`f1`メトリックに基づき)ベストなモデルを入手し、MLflowのレジストリにデプロイするということです。\n\nUI、あるいはいくつかのAPI呼び出しでこれを行うことができます:\n\n```py\n# レジストリからベストモデルを取得\nbest_model = mlflow.search_runs(\n    filter_string=f'attributes.status = \"FINISHED\"',\n    order_by=[\"metrics.f1 DESC\"],\n    max_results=1,\n).iloc[0]\nmodel_registered = mlflow.register_model(\n    \"runs:/\" + best_model.run_id + \"/model\", model_name\n)\n```\n\n```py\nclient = mlflow.tracking.MlflowClient()\nprint(\"registering model version \" + model_registered.version + \" as production model\")\nclient.transition_model_version_stage(\n    name=model_name,\n    version=model_registered.version,\n    stage=\"Production\",\n    archive_existing_versions=True,\n)\n```\n\n```py\ntry:\n    dbutils.fs.rm(petastorm_path, True)\nexcept:\n    pass\n```\n\n## 我々のモデルはデプロイされ、プロダクションでの利用が可能とフラグが立てられました！\n\nモデルレジストリにモデルをデプロイしました。これによって、モデルのガバナンスが提供され、後段でのすべてのパイプライン開発をシンプルにし、加速させます。\n\nこのモデルは、すべてのデータパイプライン(DLT、バッチ、あるいはDatabricksモデルサービングによるリアルタイム)で活用できるようになりました。\n\nそれでは、大規模な推論の実行でこのモデルを活用しましょう。 \n\n# 02_PredictionPCB\n\n**プロダクション環境での推論にモデルを活用**\n\nこれまでのノートブックで、ディープラーニングモデルをトレーニングし、モデルレジストリを用いてデプロイを行いました。ここでは、推論でモデルをどのように活用するのかを見ていきます。\n\n最初のステップでは、MLflowのリポジトリからモデルをダウンロードする必要があります。\n\n```py\nimport os\nimport torch\nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n\nmodel_name = \"cv_pcb_classification_taka\" # 適宜変更\n\n\nlocal_path = ModelsArtifactRepository(\n    f\"models:/{model_name}/Production\"\n).download_artifacts(\n    \"\"\n)\n```\n\n## PCB画像の分類\n\nPCB画像を分類するために使用するUDF関数(ユーザー定義関数)を作成します。\n\n```py\nfrom pyspark.sql.functions import pandas_udf\nimport pandas as pd\nfrom typing import Iterator\nfrom io import BytesIO\nfrom PIL import Image\nfrom torchvision.models import ViT_B_16_Weights\nimport numpy as np\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nloaded_model = torch.load(\n    local_path + \"data/model.pth\", map_location=torch.device(device)\n)\n\nweights = ViT_B_16_Weights.DEFAULT\nfeature_extractor = weights.transforms()\n\nfeature_extractor_b = sc.broadcast(feature_extractor)\nmodel_b = sc.broadcast(loaded_model)\n\n@pandas_udf(\"struct<score: float, label: int, labelName: string>\")\ndef apply_vit(images_iter: Iterator[pd.Series]) -> Iterator[pd.DataFrame]:\n\n    model = model_b.value\n    feature_extractor = feature_extractor_b.value\n    model = model.to(torch.device(\"cuda\"))\n    model.eval()\n    id2label = {0: \"normal\", 1: \"anomaly\"}\n    with torch.set_grad_enabled(False):\n        for images in images_iter:\n            pil_images = torch.stack(\n                [\n                    feature_extractor(Image.open(BytesIO(b)).convert(\"RGB\"))\n                    for b in images\n                ]\n            )\n            pil_images = pil_images.to(torch.device(device))\n            outputs = model(pil_images)\n            preds = torch.max(outputs, 1)[1].tolist()\n            probs = torch.nn.functional.softmax(outputs, dim=-1)[:, 1].tolist()\n            yield pd.DataFrame(\n                [\n                    {\"score\": prob, \"label\": pred, \"labelName\": id2label[pred]}\n                    for pred, prob in zip(preds, probs)\n                ]\n            )\n```\n\n## バッチサイズの設定\n\nSparkのデータパーティションがArrowのレコードバッチに変換される際に、`maxRecordsPerBatch`パラメータを用いてバッチサイズを64に設定しましょう。\n\n```py\nspark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 64)\n```\n\n## 予測テーブル\n\nこれで、すべての画像に対する予測結果を持つ新規テーブルを計算することができます。\n\n```sql\n%sql\nUSE takaakiyayoi_catalog.pcb;\n```\n```py\n\nspark.sql(\"drop table IF EXISTS circuit_board_prediction\")\nspark.table(\"circuit_board_gold\").withColumn(\n    \"prediction\", apply_vit(\"content\")\n).write.saveAsTable(\"circuit_board_prediction\")\n```\n\n## 誤ってラベル付けされた画像を表示\n\nシンプルなSQLで誤ったラベルを持つ画像を表示します。\n\n```sql\n\n%sql\nselect\n  *\nfrom\n  circuit_board_prediction\nwhere\n  labelName != prediction.labelName\n```\n![Screenshot 2023-10-15 at 15.31.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e4d93891-3698-2f4e-868c-0378cb756bae.png)\n\nなお、ここまでで作成されたテーブルはUnity Catalogで管理されているので、リネージも追跡されています。\n![Screenshot 2023-10-15 at 15.06.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d82b51ce-b672-c759-d377-7fb4abb05db0.png)\n\n\n### RESTサーバレスリアルタイム推論エンドポイントにモデルをデプロイ\n\n以降はサーバレスモデルサービングエンドポイントへのデプロイになりますが、今動かしている環境ではサーバレスモデルサービングエンドポイントが有効化されていないので、ここでは割愛します。[ノートブック](https://github.com/taka-yayoi/public_repo_2/blob/master/cv-quality-inspection-jpn/02_PredictionPCB.py)や[ドキュメント](https://docs.databricks.com/ja/machine-learning/model-serving/index.html)を参照ください。\n\n# 結論\n\nこれですべてです！データセットをインクリメンタルに取り込み、クレンジングし、ディープラーニングモデルをトレーニングするエンドツーエンドのパイプラインを構築しました。プロダクションレベルのパイプラインとモデルはデプロイされ、活用できる状態です。\n\nDatabricksレイクハウスは、皆様のチームのスピードを改善し、プロダクションへの移行をシンプルにします:\n\n* Auto Loaderによるユニークなデータ取り込み、データ準備機能は誰でもデータエンジニアリングにアクセスできるようにします\n* 構造化データ、非構造化データを取り込み、処理できることで、すべてのユースケースをサポートします\n* MLトレーニングのための高度なML機能\n* データサイエンティストがオペレーションのタスクではなく、(皆様のビジネスを改善するために)本当に問題になっていることにフォーカスできるようにするMLOpsのカバレッジ\n* 外部ツールなしに、あなたのすべてのユースケースをカバーするように、すべてのタイプのプロダクションデプロイメントをサポート\n* データセキュリティからモデルガバナンスに至る全てをカバーするセキュリティとコンプライアンス\n\nこれによって、Databricksを活用しているチームは、データの取り込みからモデルのデプロイメントに至る高度なMLプロジェクトを数週間でプロダクションに到達させることが可能となり、ビジネスを劇的に加速させます。\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricks AutoMLの時系列予測で各国の休日がサポートされてました", "text": "昨日アップデートを見逃していました。\n\n![Screenshot 2023-10-14 at 21.30.28.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/03a93f70-9d57-a8ac-d0d2-dbc4c3437866.png)\n\nDatabricksランタイム12.0以降であれば、各国の休日を指定できるようになってました。これまでは米国しか指定できなかったのでした。\n\nせっかくですので、[先日試したunerry様のデータ](https://qiita.com/taka_yayoi/items/af35659038f8d77a59a9)を使います。AutoMLで予測する際には、目的変数のカラムがfloat型である必要がありますので、型変換をして新規にテーブルを作成します。\n\n```sql:SQL\nCREATE TABLE main.default.visitor_automl AS\nSELECT\n  date,\n  cast(total_visitor as FLOAT) as total_visitor \nFROM\n  unerry_catalog.default.sample_st_daily_visitor;\n```\n\nこのテーブルを指定します。\n![Screenshot 2023-10-14 at 21.27.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/07505e1b-1082-3045-05ee-41d2f5bb43bf.png)\n\nさらに**国の祝日**で`Japan`を選択します。\n![Screenshot 2023-10-14 at 21.27.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6c45c6ef-1c9d-9a05-cac6-c4c9c8fbafea.png)\n\nAutoMLを実行します。\n![Screenshot 2023-10-14 at 21.34.15.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7689ebda-85aa-c7fd-f872-0ee0ef57effa.png)\n\n**View notebook for best model**のリンクをクリックすることで予測結果も確認できます。黒い点は実績値で、点がない部分が予測結果です。\n![Screenshot 2023-10-14 at 21.35.45.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/69266a61-d80b-db23-72b0-de366ec9b66e.png)\n\n7/1からの10日間の予測なので休日が入ってませんので休日の影響を確認できませんでしたが、これまでより実用性は高まっていると思います。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "LLM推論のパフォーマンスエンジニアリング：ベストプラクティス", "text": "[LLM Inference Performance Engineering: Best Practices \\| Databricks Blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n本記事では、人気のオープンソース大規模言語モデルをプロダクション用途でどのように活用するのかに関して、MosaicMLのエンジニアリングチームがベストプラクティスを共有します。また、ユーザーがモデルやデプロイするハードウェアを選択する際に助けとなるように、これらのモデルに対する推論サービスのデプロイに関するガイドラインも提供します。我々はプロダクション環境で複数のPyTorchベースのバックエンドを取り扱ってきています。これらのガイドラインは、FasterTransformersやまもなくリリースされるNVIDIAの[TensorRT\\-LLM](https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/)などの経験から導き出されています。\n\n# LLMテキスト生成の理解\n\n大規模言語モデル(LLM)は、2ステップのプロセスでテキストを生成します:入力プロンプトのトークンが並列で処理される「prefill」と、自己回帰的な方法で一度に一つの`トークン`が生成され、テキストが生成される「decoding」です。生成されるそれぞれのトークンは入力に追加され、モデルが次のトークンを生成できるようにフィードバックされます。LLMが特殊なストップトークンを出力するか、ユーザーが定義した条件(最大トークン数が生成された場合など)を満たした場合に生成が停止します。LLMがどのようにデコーダーブロックを使用するのかの背景を知りたいのであれば、[こちらのブログ記事](https://jalammar.github.io/illustrated-gpt2/#part-1-got-and-language-modeling)をご覧ください。\n\nトークンは単語あるいは単語の一部となります。テキストをトークンに分割する実際のルールはモデルによって異なります。例えば、[Llamaモデルのテキストのトークン化の方法](https://belladoreai.github.io/llama-tokenizer-js/example-demo/build/)と[OpenAIモデルのトークン化の方法](https://platform.openai.com/tokenizer)を比較することができます。LLM推論プロバイダーは多くの場合、トークンベースのメトリクス(秒あたりのトークン数など)でパフォーマンスを議論しますが、バリエーションのあるモデルにおいて、常にこれらの数値を比較できるわけではありません。具体的な例としては、Anyscaleのチームは[Llama 2のトークン作成はChatGPTよりもさらに19%の時間](https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper)を要することを発見しています(しかし、全体的なコストは以前としてはるかに低いものとなっています)。そして、HuggingGaceの研究者もまた、GPT-4と同じ量のテキストをトレーニングする際に、[Llama 2は約20%のトークンをさらに必要とする](https://twitter.com/Thom_Wolf/status/1701206627859206450?s=20)ことを発見しています。\n\n# LLMサービングにおける重要なメトリクス\n\nだとしたら、実際には我々はどのように推論のスピードを考えるべきなのでしょうか？\n\n我々のチームでは、LLMのサービングにおいて4つのキーメトリクスを使用しています:\n\n1. **最初のトークン出力までに要する時間 Time To First Token (TTFT)**: クエリー入力後にユーザーがどれだけクイックにモデルのアウトプットを参照し始めるのか。レスポンスの待ち時間が短いことはリアルタイムのやり取りで重要ですが、オフラインのワークロードではそれほど重要ではありません。このメトリックは、プロンプトの処理と最初の出力トークンの生成に必要とされる時間による影響を受けます。\n1. **出力トークンごとの時間 Time Per Output Token (TPOT)**: システムに対してクエリーを行う*それぞれの*ユーザーに対して出力トークンを制するのに要する時間です。このメトリックは、それぞれのユーザーがモデルの`スピード`をどのように感じるのかに対応します。例えば、100ms/トークンのTPOTは、ユーザーあたり秒間10トークンとなり、分あたり約450単語であり、通常の人間が読むスピードよりも速いことになります。\n1. **レーテンシー**: モデルがユーザーに対して完全なレスポンスを生成するのに要する合計の時間です。全体的なレスポンスのレーテンシーは、上述の2つのメトリクスを用いて計算することができます: レーテンシー = (TTFT) + (TPOT) * (生成されるトークンの数)。\n1. **スループット**: すべてのユーザーとリクエストに対して推論サーバーが秒間生成できる出力トークンの数です。\n\n我々のゴールは何でしょうか？最初のトークンを可能な限り高速に出力し、最高のスループットを達成し、出力トークンあたりの時間を最小にすることです。言い換えると、我々がサポートできる数多くのユーザーに対して、可能な限りモデルに高速にテキストを生成させたいということになります。\n\n特筆すべきこととして、スループットと出力トークンあたりの時間にはトレードオフが存在します: 16ユーザーのクエリーを同時に処理する場合、クエリーを順次実行するよりも*高い*スループットを実現しますが、それぞれのユーザーに対して出力トークンを生成する時間は*長く*なります。\n\n全体的な推論レーテンシーのターゲットがある場合には、モデル評価に対して有用なヒューリスティックがあります:\n\n- **全体的なレスポンスのレーテンシーにおいては出力の長さが支配的です:** 平均レーテンシーを計算するには、出力トークン長の期待値/最大値と、モデルの出力トークンあたりの全体的な平均時間とで掛け算します。\n- **入力の長さはパフォーマンスにおいて重要ではありませんが、ハードウェア要件では重要となります:** MPTモデルにおいては、512の*入力*トークンの追加によるレーテンシーの増加は8の追加*出力*トークンの積よりも小さくなります。しかし、長いインプットをサポートする場合、モデルのサービングが困難となります。例えば、単体のA100-40GBは、最大のコンテキスト長が2048トークンであるMPT-7Bのサービングには十分なGPUメモリーを提供しません。\n- **全体的なレーテンシーはモデルサイズと劣線形にスケールします:** 同じハードウェアにおいては、大規模なモデルは遅くなりますが、スピードの比率は必ずしもパラメーター数の比率とは一致しません。MPT-30BのレーテンシーはMPT-7Bのレーテンシーの約2.5倍です。Llama2-70Bのレーテンシーは、Llama2-13Bの約2倍です。\n\n多くのケースで我々は見込みのお客様から平均の推論レーテンシーを尋ねられます。特定のレーテンシーターゲット(トークンあたり20ms以下にしたい、など)にフォーカスする前に、期待する入出力長の特徴を把握することに一定の時間を費やすことをお勧めします。\n\n# LLM推論の課題\n\n以下のような一般的なテクニックを用いることで、LLM推論の最適化の助けとなります:\n\n- **オペレータの融合:** 隣接する異なるオペレータを結合することで、多くの場合レーテンシーを改善できます。\n- **量子化:** より少ないビットを使用するようにアクティベーションと重みを圧縮します。\n- **圧縮:** スパーシティあるいはディスティレーション。\n- **並列化:** 複数デバイスに対するテンソル並列化、大規模モデルに対するパイプライン並列化。\n\nこれらの手法の他に、トランスフォーマー固有の数多くの重要な最適化があります。この主要な例はKV(キーバリュー)キャッシングです。デコーダーのみのトランスフォーマーベースのモデルにおける[アテンション](http://jalammar.github.io/illustrated-transformer/)のメカニズムは計算処理的に非効率です。それぞれのトークンは以前に出現したすべてのトークンを処理するため、新たなトークンが生成される都度、数多くの同じ値を再計算します。例えば、N番目のトークンを生成する際には、(N-1)番目のトークンは(N-2)番目、(N-3)番目…最初のトークンを処理します。同様に、(N+1)番目のトークンを生成する際には、N番目のトークンに対するアテンションは、 (N-1)番目、(N-2)番目、(N-3)番目…最初のトークンを再度参照する必要があります。KVキャッシング、すなわち、アテンションレイヤーに対する中間的なキー/バリューの保存は、繰り返しの計算を避け、あとで再利用できるようにこれらの結果を保持するために活用されます。\n\n# メモリー帯域が鍵となります\n\nLLMの計算処理の大部分は、行列・行列の掛け算のオペレーションとなります。次元の小さい行列に対するこれらのオペレーションは通常、多くのハードウェアにおいてメモリー帯域の制限を受けることになります。自己回帰的にトークンを生成する際、(バッチサイズとシーケンスのトークン数によって定義される)アクティベーション行列次元の一つは小規模なバッチサイズにおいて小さなものとなります。このため、このスピードはロードされたデータをどれだけクイックに計算できるのかではなく、モデルのパラメーターをGPUメモリーからローカルのキャッシュ/レジスタにどれだけクイックにロードできるのかに依存します。推論ハードウェアにおいて利用可能なメモリー帯域は、ピークの計算パフォーマンスよりもトークン生成のスピード予測において優れた指標となります。\n\n推論ハードウェアの使用率は、サービングコストの観点で非常に重要です。GPUは高価で、可能な限りそれらを最大限活用する必要があります。共有推論サービスは、多数のユーザーのワークロードをまとめることで低コストを保証し、それぞれのギャップを埋め、重複するリクエストをバッチにまとめます。Llama2-70Bのような大規模なモデルに対しては、大規模なバッチサイズにおいてのみ優れたコストパフォーマンスを達成しています。大規模なバッチサイズに対応できる推論サービングもシステムを持つことは、コスト効率性において重要となります。しかし、大規模なバッチは、大規模なKVキャッシュサイズを意味し、モデルサービングに必要なGPUの数が増加することとなります。このために争奪戦が起きており、共有サービスのオペレータはコストのトレードオフを判断し、システムの最適化を実装する必要に迫られています。\n\n# モデル帯域使用量(MBU)\n\nLLM推論サーバーはどのように最適化できるのでしょうか？\n\nここまでで簡単に説明したように、小規模なバッチサイズなLLMの推論、特にでコード時間においては、デバイスのメモリーから計算ユニットにどれだけ高速にモデルのパラメーターをロードできるのかがボトルネックとなります。メモリーの帯域がどれだけクイックにデータを移動できるのかを示します。背後のハードウェアの使用量を計測するために、Model Bandwidth Utilization (MBU)という新たなメトリックを導入します。MBUは(達成されるメモリー帯域) / (ピークのメモリー帯域)で計算され、達成されるメモリー帯域は((合計のモデルパラメーターサイズ + KVキャッシュサイズ) / TPOT)となります。\n\n例えば、16ビット精度の7BパラメーターのモデルのTPOTが14msの場合、14msで14GBのパラメーターを移動するということは、1TB/秒の帯域使用となります。マシンのピークの帯域が2TB/秒ならば、50%のMBUで処理を行っています。シンプルにするために、この例では小規模なバッチサイズかつ短いシーケンス長において小さいKVキャッシュサイズを無視しています。MBUが100%に近づくことは、推論システムが利用可能なメモリー帯域を高価的に活用していることを意味します。また、MBUは正規化された方法で異なる推論システム(ハードウェア + ソフトウェア)を比較する際にも有用です。MBUは計算処理に制約を受ける環境で重要なメトリックであるModel Flops Utilization (MFU: [PaLMの論文](https://arxiv.org/abs/2204.02311)で導入)を補完するものとなります。\n\n図1では、[roofline plot](https://en.wikipedia.org/wiki/Roofline_model)と同様のプロットで、MBUを図示しています。オレンジの領域の斜線は、メモリー帯域が完全に100%で飽和する際の最大予想スループットとなります。しかし、小さなバッチなサイズにおいては、観測されるパフォーマンスは最大よりも小さなものとなります - どれだけ低いのかがMBUの指標となります。大規模なバッチサイズ(黄色の領域)においては、計算能力による制限を受け、ピークの予測スループットの割合としての達成スループットはModel Flops Utilization (MFU)として計測されます。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-811-blog-img-1.png)\n*図1. MBU (Model Bandwidth Utilization)とMFU (Model Flops Utilization)の図。MBUとMFUはそれぞれ、メモリーの限界と計算能力の限界に対する割合となります。*\n\nMBUとMFUは、特定のハードウェア環境でどれだけ推論スピードを改善できるのかの余地を決定します。図2では我々のTensorRT-LLMベースの推論サーバーにおいて異なるテンソル並列度に対するMBUを計測しています。隣接する大規模なメモリーのチャンクを転送する際に、ピークのメモリー帯域使用量となっています。MPT-7Bのように小規模なモデルが複数のGPUに分散された際、それぞれのGPUで小規模なメモリーのチャンクを移動するため低いMBUを観測しています。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-811-blog-img-2.png)\n*図2: A100-40GのGPUのTensorRT-LLMにおける様々なテンソル並列度のMBUの実験結果。バッチサイズ1で512の入力トークンのシーケンスを使用。*\n\n図3では、NVIDIAのH100 GPUにおけるいくつかのテンソル並列度とバッチサイズにおけるMBUを計測しています。バッチサイズが増加するとMBUは減少します。しかし、GPUをスケールさせると、MBUの相対的な減少はさらに小さなものとなります。より大きなメモリー帯域を持つハードウェアを選択することで、少ないGPUでのパフォーマンスをブーストできることを特記しておきます。バッチサイズ1では、2つのH100-80GBで60%のMBUという、4つのA100-40GB GPUの55%よりも高い値を達成できています。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-811-blog-img-3.png)\n*図3. H100-80GBのGPUびおけるバッチサイズやテンソル並列度に対するMBUの実験結果。リクエスト: 512入力トークンのシーケンス。*\n\n# ベンチマーク結果\n\n## レーテンシー\n\nMPT-7BとLlama2-70Bモデルで様々なテンソル並列度を指定して、最初のトークン出力までに要する時間(TTFT)と出力トークンごとの時間(TPOT)を計測しました。入力プロンプトが長くなると、最初のトークンの生成に要する時間が全体的なレーテンシーの大半を占めるようになりました。複数のGPUに対するテンソル並列化はこのレーテンシーの削減に寄与しました。\n\nモデルのトレーニングとは異なり、より多くのGPUにスケールしても劇的に推論のレーテンシーを削減するわけではありません。例えば、Llama2-70BでGPUを4から8にしても、小規模バッチサイズにおいてはレーテンシーは0.7倍にしかなりません。この利用の一つは、(上述の通り)並列度を高めてもMBUが低くなるということがあります。他の理由は、テンソル並列化によってGPUノード間の通信オーバーヘッドが生じるというものです。\n![Screenshot 2023-10-14 at 10.09.40.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d5b9943a-3acd-293d-fee2-85c0e7377318.png)\n*表1. バッチサイズ1で入力リクエストが512トークンの場合に最初のトークンに要する時間。Llama2 70Bのような大規模なモデルをメモリーに載せるには最低でも4台のA100-40GBのGPUが必要。*\n\n大規模なバッチサイズにおいては、テンソルの並列度を高くするとトークンのレーテーシーは相対的に大きく削減されます。図4ではMPT-7Bにおいて出力トークンごとの時間がどのように変化するのかを示しています。バッチサイズ1では2から4に変えてもトークンのレーテンシーは12%程度しか削減されません。バッチサイズ16では、2から4にすることで33%削減されます。これは、バッチサイズ1と比べてバッチサイズ16におけるテンソルの並列度を高くした場合にMBUの相対的な減少値が小さくなるという以前の観測結果と適合しています。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-811-blog-img-4.png)\n*図4. A100-40GBのGPUにおいてMPT-7Bをスケールした際のユーザーごと出力トークンごとの時間。GPUの数を増やしてもレーテンシーは線形にスケールしません。リクエスト：128の入力トークン、64の出力トークンのシーケンス。*\n\n図5では、4つと8つの違いの相対的な違いが少ないことを除いて同様のLlama2-70Bの結果を示しています。また、我々は2つの異なるハードウェアでのGPUのスケーリングを比較しました。H100-80GBはA100-40GBと比較して、2.15倍のメモリー帯域を持っているので、バッチサイズ1でのレーテンシーは36%低く、4つのシステムでのバッチサイズ16では52%低くなっています。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-811-blog-img-5.png)\n*図5. 複数のGPUにLlama-v2-70Bをスケールさせた際のユーザーごと出力トークンごとの時間(入力リクエスト: 512トークン長)。Llama-v2-70B(float16)は1x40GB GPU、2x40GB、1x80GB GPUにフィットしないので、これらの値がないことに注意してください。*\n\n## スループット\n\nリクエストをバッチにまとめることで、スループットとトークンあたりの時間をトレードオフすることができます。GPU評価の際にクエリーをグルーピングすることで、シーケンシャルにクエリーを処理するよりもスループットを改善することができますが、(キューの効果を無視して)それぞれのクエリーの完了に要する時間は長くなります。\n\n推論リクエストのバッチにはいくつかの共通的なテクニックがあります:\n\n- **静的バッチ:** クライアントで複数のプロンプトをリクエストにパッケージし、バッチにあるすべてのシーケンスが完了したあとにレスポンスが返却されます。我々の推論サーバーはこれをサポートしていますがこれを必要とはしません。\n- **動的バッチ:** サーバー内でオンザフライでプロンプトがバッチ化されます。通常、この手法は静的バッチよりも性能が悪いですが、レスポンスが短い場合あるいは固定長の場合には最適に近いものとなります。リクエストの[パラメーター](https://docs.mosaicml.com/en/latest/inference.html#parameters)が異なる際にはうまく動作しません。\n- **連続バッチ:** リクエストが到着するたびにリクエストのバッチ化を行うというアイデアは、[この素晴らしい論文](https://www.usenix.org/conference/osdi22/presentation/yu)で導入され、現在のSOTA手法となっています。バッチを完了するためにすべてのシーケンスを待つのではなく、イテレーションレベルでシーケンスをグルーピングします。動的バッチよりも10倍から20倍優れたスループットを達成することができます。\n\n![](https://cms.databricks.com/sites/default/files/inline-images/db-811-blog-img-6.png)\n*図6. LLMサービングにおける様々なバッチ手法。バッチ化は推論効率の改善に有効です。*\n\n通常、共有サービスにおいて連続バッチが最適なアプローチですが、他の二つが適しているシチュエーションがあります。低QPSの環境では、動的バッチの方が連続バッチよりも性能が出ることがあります。よりシンプルなバッチフレームワークで低レベルのGPU最適化を実装することの方が簡単な場合があります。オフラインのバッチ推論ワークロードでは、静的バッチでは大きなオーバーヘッドを回避し、優れたスループットを達成することができます。\n\n## バッチサイズ\n\nどれだけうまくバッチ化を行えるのかは、リクエストのストリームに依存します。しかし、同じリクエストを用いて静的バッチをベンチマークすることでパフォーマンスの上限を取得することができます。\n![Screenshot 2023-10-14 at 10.53.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d8d7d239-37bd-d147-7545-39343e3b2d2c.png)\n*表2. 静的バッチとFasterTransformersベースのバックエンドによるMPT-7Bのピークのスループット(リクエスト/秒)。リクエスト: 512の入力トークン、64の出力トークン。大規模な入力においては、小規模バッチサイズではOOMの制限が生じます。*\n\n## レーテンシーのトレードオフ\n\nリクエストのレーテンシーはバッチサイズによって増加します。例えば、1台のNVIDIA A100 GPUで、バッチサイズ64でスループットを最大化する場合、スループットは14倍に増加しますがレーテンシーは4倍に増加します。通常、共有推論サービスではバランスの取れたバッチサイズを選択します。自身のモデルをホスティングしているユーザは、自身のアプリケーションに適したレーテンシー/スループットのトレードオフを決断する必要があります。チャットbotのようにいくつかのアプリケーションにおいては、高速なレスポンスのための低レーテンシーが最大の優先事項となります。非構造化のPDFのバッチ処理のような他のアプリケーションにおいては、すべてを並列に高速に処理するために、個々のドキュメントの処理におけるレーテンシーを犠牲にするかもしれません。\n\n図7では、7Bモデルのスループット対レーテンシーの曲線を示しています。この曲線のそれぞれの線は、バッチサイズを1から256に増加させることで取得しています。これは、様々なレーテンシーの制約においてどれだけバッチサイズを大きくできるのかを特定する際に有用です。上述のroofline譜rっとを思い返すと、我々が期待することとこれらの計測結果に一貫性があることがわかります。特定のバッチサイズを超えた場合、すなわち、計算処理の限界を超えた場合、バッチサイズを倍にしてもスループットを増加させることなしに、単にレーテンシーが増加するだけとなります。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-811-blog-img-7.png)\n*図7. MPT-7Bモデルのスループットとレーテンシーの曲線。これによって、ユーザーはレーテンシーの制約の下でスループット要件を満たすハードウェア設定を選択することができます。*\n\n並列化を活用する際、低レベルのハードウェアの詳細を理解することが重要です。例えば、クラウドが異なる場合、8台のA100インスタンスが同じというわけではありません。いくつかのサーバーはすべてのGPUにおいて高い帯域接続を行いますが、他のペアのGPUではペア間の帯域が狭い場合があります。これによってボトルネックが生じ、上述の曲線から大きく乖離した実際のパフォーマンスをもたらすことになります。\n\n# 最適化ケーススタディ：量子化\n\n量子化は、LLM推論のハードウェア要件の削減で使用される一般的な手法です。推論時にモデルの重みとアクティベーションの精度を削減することで、ハードウェアの要件を劇的に削減することができます。例えば、16ビットの重みを8ビットの重みにスイッチすることで、メモリー制約のある環境で必要ツァれるGPUの数を半分にすることができます(例: A100におけるLlama2-70B)。重みを4ビットにすることで、コンシューマーのハードウェアで推論を実行することも可能です(例: MacbookでLlama2-70Bを実行)。\n\n我々の経験では、量子化は注意して実装する必要があります。不用意な量子化技術のテクニックは、モデルの品質に大きなダメージを与えることがあります。また、量以下のインパクトはモデルのアーキテクチャ(MPT vs Llama)やサイズによって変化します。今後のブログ記事でこの詳細を探索します。\n\n量子化のようなテクニックを実験する際、個別にモデルの品質を評価するのではなく、*推論システム*の品質を評価するために[Mosaic Eval Gauntlet](https://www.mosaicml.com/llm-evaluation)のようなLLM品質ベンチマークを活用することをお勧めします。さらに、より深いシステムの最適化を探索することが重要です。特に、量子化はKVキャッシュをより効率的なものとします。\n\n上述したように、自己再帰のトークン生成では、すべてのステップで再計算するのではなく、[アテンションレイヤーの過去のキー/バリューをキャッシュします](https://kipp.ly/transformer-inference-arithmetic/#kv-cache)。KVキャッシュのサイズは、これらのシーケンスの長さと一度に処理されるシーケンスの数に応じて変化します。さらに、次のトークン生成のそれぞれのイテレーションにおいて、既存のキャッシュに新たなKVアイテムが追加されると、新規トークンが追加されるごとに肥大化していきます。このため、優れた推論パフォーマンスのためには、これらの新規の値を追加する際の効果的なKVキャッシュメモリー管理が重要となります。Llama2モデルでは、Grouped Query Attention (GQA)と呼ばれるアテンションのバリエーションを使用しています。KVのヘッドの数が1の場合、GQAはMulti-Query-Attention (MQA)と同じになることに注意してください。GQAはキー/バリューを共有することで、KVキャッシュのサイズを低く抑える助けとなります。KVキャッシュサイズの計算式は、`batch_size * seqlen * (d_model/n_heads) * n_layers * 2 (K と V) * 2 (Float16ごとのバイト数) * n_kv_heads`となります。\n\n表3では、シーケンス長が1024トークンで様々なバッチサイズで計算されたGQAのKVキャッシュサイズを示しています。Llama2モデルのパラメーターサイズは、70Bモデルでは140GB(Float16)となります。KVキャッシュの量子化は、KVキャッシュのサイズを削減するための(GQA/MQAに加えて)更なるテクニックであり、我々は積極的に生成品質に対するこのインパクトを評価しています。\n![Screenshot 2023-10-14 at 11.24.00.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a78cc4ac-c129-1e9e-ac5d-0f02f3a250eb.png)\n*表3. シーケンス長1024のLlama-2-70BのKVキャッシュサイズ*\n\n上述したように、少ないバッチサイズでのLLMによるトークン生成は、GPUのメモリー帯域に制限を受ける問題となり、生成スピードはモデルパラメーターをどれだけ早くGPUメモリーからチップのキャッシュに移動できるのかに依存します。モデルの重みをFP16(2バイト)からINT8 (1バイト)やINT4(0.5バイト)に変換することで、データする移動が少なくなるので、トークン生成がスピードアップします。しかし、量子化はモデルの生成品質にネガティブなインパクトをもたらすことがああります。我々は現在Model Gauntletを用いてモデル品質へのインパクトを評価しており、今後はフォローアップのブログ記事を公開する予定です。\n\n# 結論及び重要な結果\n\n上で説明した要因のそれぞれは、モデルの構築、デプロイの方法に影響を与えます。ハードウェアのタイプ、ソフトウェアスタック、モデルアーキテクチャ、典型的な使用パターンを検討する際にデータドリブンの意思決定を行えるように、我々はこれらの結果を活用しています。我々の経験から導き出されるいくつかの推奨事項を示します。\n\n**あなたの最適化ターゲットを特定しましょう:** インタラクティブなパフォーマンスを気にしていますか？スループットの最大化ですか？コストの最小化？予測可能なトレードオフが存在します。\n\n**レーテンシーのコンポーネントに注意を払いましょう:** インタラクティブなアプリケーションでは、最初のトークン生成に要する時間はあなたのサービスがどれだけレスポンシブかを決定し、出力トークンごとの時間はそれがどれだけ速いのかを決定します。\n\n**メモリーの帯域が鍵となります:** 最初のトークンの生成は通常計算能力による制限を受けますが、以降のデコーディングのオペレーションはメモリーによる制約を受けるものとなります。LLM推論は多くの場合、メモリーの制約を受ける環境で動作するので、MBUは最適化すべき有用なメトリックであり、推論システムの効率性を比較する際に活用することができます。\n\n**バッチ作成が重要です:** 同時に複数のリクエストのを処理することは、高いスループットを達成し、効果なGPUを効果的に活用するためには重要なこととなります。共有オンラインサービスにおいては、連続バッチが不可欠となり、オフラインバッチ推論ワークロードでは、よりシンプルなバッチ技術で高いスループットを達成することができます。\n\n**深い最適化:** 標準的な推論最適化技術(オペレーターの融合、重みの量子化など)はLLMにおいて重要ですが、システムの最適化を深く探索すること、特にメモリーの使用率を改善するものを探索することが重要となります。その一例がKVキャッシュの量子化です。\n\n**ハードウェア設定:** デプロイメントのハードウェアを決定するためには、モデルのタイプと予測されるワークロードを活用すべきです。例えば、複数のGPUにスケーリングする際、MPT-5のような小規模なモデルでは、Llama2-70Bのような大規模モデルと比較して、MBUは急速に低下します。また、テンソル並列度に対してパフォーマンスは劣線形でスケールする傾向があります。すなわち、トラフィックが多い、あるいはユーザーがさらなる低レーテンシーを望んでいる場合には、テンソル並列度を高くすることには依然として合理性があると言えます。\n\n**データに基づく意思決定:** 理論を理解することは重要ですが、常にエンドツーエンドのパフォーマンスを計測することをお勧めします。推論のデプロイメントが予測したよりも性能が出ないことには多数の理由が存在します。ソフトウェアの非効率性によって、予期せずMBUが低くなることがあります。あるいは、クラウドプロバイダー間のハードウェアの違いによって、驚きをもたらすことがあります(2つのクラウドプロバイダーの8つのA100サーバーで2倍のレーテンシーの違いを観測したことがあります)。\n\nLLM推論をスタートするには、Databricksモデルサービングを使い始めましょう。詳細は[ドキュメント](https://docs.databricks.com/ja/machine-learning/model-serving/llm-optimized-model-serving.html)をチェックしてください。\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksのサインアップおよびワークスペース作成(2023/10版)", "text": "定期的にウォークスルーしています。\n\n- [Databricksフリートライアルへのサインアップ](https://qiita.com/taka_yayoi/items/fb4f57c069e1f272e88a)\n- [Databricksアカウントのセットアップとワークスペースの作成](https://qiita.com/taka_yayoi/items/c53beee557ae4a9643ac)\n- [AWS Marketplace経由でDatabricks環境を構築する](https://qiita.com/taka_yayoi/items/32aa33cd529fdb419c16)\n- [Databricksアカウントのセットアップとワークスペースの作成\\(実践編\\)](https://qiita.com/taka_yayoi/items/98edd2e9d06f5c1029a1)\n\n[Databricksアカウントのセットアップとワークスペースの作成\\(実践編\\)](https://qiita.com/taka_yayoi/items/98edd2e9d06f5c1029a1)と同じ手順を踏むのですが、こちらは2021年の年末なので若干プロセスが変更されています。\n\n# サインアップ\n\n以下のサイトにアクセスして、氏名、メールアドレスなどを入力して**送信**をクリックします。この際のメールアドレスがDatabricksの[アカウントオーナー](https://qiita.com/taka_yayoi/items/090879d59ae74ccaa12e#:~:text=%E3%82%A2%E3%82%AB%E3%82%A6%E3%83%B3%E3%83%88%E3%82%AA%E3%83%BC%E3%83%8A%E3%83%BC%E3%81%AFDatabricks%E3%81%AB%E3%81%8A%E3%81%84%E3%81%A6,%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%8C%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82)になりますので注意して選択してください。\n\nhttps://www.databricks.com/jp/try-databricks#account\n\nクラウドを選択します。ここではAWSを選びます。\n![Screenshot 2023-10-12 at 10.26.12.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2e3e6543-3e5c-729a-74ae-504a61f5cbd4.png)\n\n入力したアドレスにメールが送付されます。\n![Screenshot 2023-10-12 at 10.26.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1b0e410b-fcc8-1cf0-3c66-667dcd0759be.png)\n\n以下のようなメールが届くので**link**をクリックします。\n![Screenshot 2023-10-12 at 10.26.34.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/4265f828-bddd-dbe6-508f-f40b9a4c90eb.png)\n\nパスワードを設定します。\n![Screenshot 2023-10-12 at 10.26.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/34166172-cc8a-a228-742d-d2710a685468.png)\n\n# プランの選択\n\nプランによって利用できる機能と単価が変わります。\n\n![Screenshot 2023-10-12 at 10.27.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/87f35a1b-8e53-f188-373a-fa80e614cda8.png)\n\n# ワークスペースの作成\n\n説明文が表示されるので下にスクロールします。**始めましょう**をクリックします。\n![Screenshot 2023-10-12 at 10.27.25.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1776fad1-dcaa-17dd-de23-821dfc805e38.png)\n![Screenshot 2023-10-12 at 10.27.30.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a2089d42-cbc8-d2cb-0f98-5dd5f72dd1e2.png)\n\n必要な情報が表示されるので準備しておきます。\n![Screenshot 2023-10-12 at 10.27.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3ad87dd3-48a5-d692-cc2a-bc3d92cb7d89.png)\n\nワークスペース名とリージョンを指定します。**クリックスタートを始める**をクリックします。\n![Screenshot 2023-10-12 at 10.28.12.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/40fdf4b4-13bc-c2ba-6616-43abfba69011.png)\n\nAWSマネジメントコンソールにログインするとCloudFormationのスタックが作成されます。\n![Screenshot 2023-10-12 at 10.28.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/546f03f5-ce42-bc21-051a-e4f154e1e7f4.png)\n\n**Databricks account password**に上で設定したパスワードを入力します。\n![Screenshot 2023-10-12 at 10.29.07.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d8c92fcb-76d4-4b57-d20e-e82b331f54f1.png)\n\nスタックの構築をスタートします。\n![Screenshot 2023-10-12 at 10.29.15.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1e8729e6-be07-efd3-006d-f825d6d27b9f.png)\n\n[アカウントコンソール](https://accounts.cloud.databricks.com/login)にアクセスすると、ワークスペースが作成中であることを確認できます。\n![Screenshot 2023-10-12 at 10.32.16.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bd71d0b4-9585-ba80-2709-54d6493e44ba.png)\n\nなんと、この時点で[Unity Catalog](https://qiita.com/taka_yayoi/items/15aede468bdca58ec6a3)のメタストアも作成されます！これまでは手動で作成する必要がありました。\n![Screenshot 2023-10-12 at 10.32.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/65c68bb2-63e8-d287-e03e-eec31a46fccc.png)\n\nワークスペースの作成が完了するとステータスが**Running**になります。\n![Screenshot 2023-10-12 at 10.33.12.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d890ec6e-d2ae-c4b5-db98-1c59c4ae3587.png)\n\n# ワークスペースをメタストアにアタッチ\n\nワークスペースからUnity Catalogの機能を使用するには、ワークスペースをメタストアに紐付け、すなわちアタッチする必要があります。サイドメニューの**データ**からメタストアにアクセスし、**ワークスペース**タブを開き、**ワークスペースに割り当てる**をクリックします。作成されたワークスペースを選択して割り当てを行います。\n![Screenshot 2023-10-12 at 10.33.28.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/882da3f0-5c7a-4c6a-d8a6-107ac2860989.png)\n\nメッセージを確認して有効化します。\n![Screenshot 2023-10-12 at 10.33.34.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d4093c6f-5a0a-8bfa-39f2-fb2198bf9bdb.png)\n\n# ワークスペースへのアクセス\n\nワークスペース一覧からワークスペースにアクセスします。ログインする際にはサインアップの過程で指定したメールアドレスとパスワードを指定します。\n![Screenshot 2023-10-12 at 10.34.18.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1e457dda-912c-43bd-7299-c28524383eeb.png)\n\nサイドメニューの**カタログ**にアクセスすると、カタログ`main`(メタストアのデフォルトカタログ)が表示されているのでUnity Catalogにアクセスできることが確認できました。\n![Screenshot 2023-10-12 at 10.34.34.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/45358530-cc16-4113-b073-30ba02e7f9b5.png)\n\nちなみに、ここまでで10分もかかっていません。是非トライしてみてください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksレイクハウスAIでLlama 2基盤モデルが利用可能に", "text": "[Llama 2 Foundation Models Available in Databricks Lakehouse AI \\| Databricks Blog](https://www.databricks.com/blog/llama-2-foundation-models-available-databricks-lakehouse-ai)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n[Meta AIのLlama 2](https://ai.meta.com/llama/)基盤チャットモデルをファインチューニングし、プライベートモデルサービングエンドポイントにデプロイできるように、[Databricksマーケットプレイス](https://marketplace.databricks.com/details/46527194-66f5-4ea1-9619-d7ec6be6a2fa/Databricks_Llama-2-Models)で利用できるようになったことを発表できて嬉しく思っています。Databricksマーケットプレイスは、クラウド、リージョン、プラットフォーム横断で、データセットやノートブックを含むデータ資産を共有、共有できるオープンなマーケットプレイスです。すでにマーケットプレイスで提供されているデータ資産に加えて、この新たなリスティングでは、Llama 2のチャット指向の70億から700億パラメーターの大規模言語モデル(LLM)、およびUnity Catalogで集中管理されるガバナンスとリネージを提供します。それぞれのモデルはMLflowにラップされているので、Databricksノートブックで[MLflow Evaluation API](https://mlflow.org/docs/latest/models.html#evaluating-with-llms)を容易に活用でき、[LLMに最適化されたGPUモデルサービングエンドポイント](https://qiita.com/taka_yayoi/items/f0ef1fae82197ec51725)に1クリックでデプロイすることができます。\n![](https://cms.databricks.com/sites/default/files/inline-images/image2_6.png)\n*Databricksマーケットプレイス*\n\n# Llama 2とは？\n\nLlama 2は、チャットユースケースに最適化されたMeta AIの生成テキストモデルのファミリーです。このモデルは他のオープンモデルのパフォーマンスを上回っており、ファインチューニングしたモデルはOpenAIのGPT-3.5-turboに匹敵するブレイクスルーを示しています。\n\n# レイクハウスAIにおけるLlama 2\n\nDatabricksレイクハウスのAIプラットフォームで、セキュアかつエンドツーエンドの体験を得ることができます:\n\n1. Databricksマーケットプレイスにアクセスします。ノートブックをプレビューすることができ、[Databricksマーケットプレイス](https://marketplace.databricks.com/details/46527194-66f5-4ea1-9619-d7ec6be6a2fa/Databricks_Llama-2-Models)からすぐにLlama 2モデルのchatファミリーにアクセスすることができます。マーケットプレイスによって、Unity Catalogで管理できる最先端の基盤モデルの発見、評価が容易になります。\n1. Unity Catalogでガバナンスを集中管理します。モデルはカタログに格納されているので、あなたのLlama 2モデルに対してUnity Catalogが提供する集中管理されたガバナンス、監査、リネージ追跡の全てを自動で活用できるようになります。\n1. ご自身のモデルをDatabricksモデルサービングにプライベートにホスティングするために、1クリックでデプロイできるように、Llama 2モデルはMLflowにパッケージングされています。現在パブリックプレビューのGPUモデルサービングは、低レーテンシーと高スループットを提供するために、大規模言語モデルの取り扱いに[最適化](https://qiita.com/taka_yayoi/items/f0ef1fae82197ec51725)されています。これは、センシティブなデータを取り扱ったりするケースや、顧客データをサードパーティに送信できないケースでは優れた選択肢となります。\n1. AIゲートウェイをプライベートエンドポイントと接続します。GPUにプライベートにホストされたモデルは高コストになる場合があるため、組織がエンドポイントへの接続を管理する際、それぞれのユースケースごとのルートを作成、分散できるように、我々は[MLflow AI Gateway](https://www.databricks.com/blog/announcing-mlflow-ai-gateway)の利用をお勧めしています。AIゲートウェイでは、Databricksモデルサービングエンドポイントや外部にホストされたSaaS LLMにおける資格情報管理をセキュアにすることに加え、コストコントロールの制限もサポートしています。\n\n![](https://cms.databricks.com/sites/default/files/inline-images/image3_5.png)\n*マーケットプレイスからLlama 2モデルを選択*\n\n# Databricksで生成AIを始める\n\nDatabricksレイクハウスAIプラットフォームによって、開発者は自信を持って生成AIアプリケーションを迅速に構築、デプロイできるようになります。\n\n- [Marketplace](https://marketplace.databricks.com/details/46527194-66f5-4ea1-9619-d7ec6be6a2fa/Databricks_Llama-2-Models)に直行して、Llama 2チャットモデルを手に入れましょう！\n- [Databricks GPU Model Serving](https://docs.databricks.com/en/machine-learning/model-serving/llm-optimized-model-serving.html)によるLLMのプライベートホスティングをチェックしましょう。Llama 2-70Bをデプロイしたい場合には、[こちらのフォーム](https://docs.google.com/forms/d/1-GWIlfjlIaclqDz6BPODI2j1Xg4f4WbFvBXyebBpN-Y/edit)を提出していただければ、次のステップをご連絡します。\n- MLflow AI Gatewayのプライベートプレビューにサインアップしましょう。\n- [Databricks Demo](https://www.databricks.com/resources/demos/tutorials/data-science-and-ai/lakehouse-ai-deploy-your-llm-chatbot)を用いて、ご自身の企業データでLLMを拡張するRetrieval Augmented Generation (RAG)チャットbotを構築しましょう。\n- [GitHub repository with LLM examples](https://github.com/databricks/databricks-ml-examples)をチェックしましょう。\n\nまもなく発表されるさらにエキサイティングな内容も楽しみにしていてください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksのLLM最適化モデルサービングにLlama2モデルをデプロイしてみる", "text": "こちらのサンプルノートブックをウォークスルーします。\n\nhttps://qiita.com/taka_yayoi/items/f0ef1fae82197ec51725#llama2%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8B%E6%9C%80%E9%81%A9%E5%8C%96llm%E3%82%B5%E3%83%BC%E3%83%93%E3%83%B3%E3%82%B0%E3%83%8E%E3%83%BC%E3%83%88%E3%83%96%E3%83%83%E3%82%AF\n\n# 最適化Llama2サービングのサンプル\n\n最適化LLMサービングによって、最先端のOSSのLLMを活用し、GPUによって改善されたレーテンシーとスループットを提供する自動最適化機能を持つDatabricksモデルサービングにデプロイすることができます。現時点では、Llama2とMosaic MPTクラスのモデルの最適化をサポートしています。\n\nこのサンプルでは以下をウォークスルーします:\n1. Hugging Face `transformers`からモデルをダウンロード\n1. DatabricksのUnity Catalogあるいはワークスペースレジストリに、最適化されたサービングサポートフォーマットでモデルを記録\n1. モデルの最適化サービングの有効化\n\n## 前提条件\n\n- ノートブックを十分なメモリーを持つクラスターにアタッチ\n- MLflowバージョン2.7.0以降がインストールされていること\n- 7B以上のサイズのモデルを取り扱う際には特に**UCでのモデル管理**を行うこと\n\n# ステップ1: 最適化LLMサービングのためにモデルを記録\n\n```py\n# 必要な依存関係を更新/インストール\n!pip install -U mlflow\n!pip install -U transformers\n!pip install -U accelerate\n!pip install -U flash_attn # DBR 13.3 MLでエラーになったので追加\ndbutils.library.restartPython()\n```\n\nHugging faceのトークンを入力します。詳細は[こちら](https://qiita.com/taka_yayoi/items/bc1da42144826da56ab4)。\n\n```py\nimport huggingface_hub\n# 既にhugging faceにログインしている際にはスキップ\nhuggingface_hub.login()\n```\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-chat-hf\", \n    torch_dtype=torch.bfloat16\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"meta-llama/Llama-2-7b-chat-hf\"\n)\n```\n\n```py\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types.schema import ColSpec, Schema\nimport numpy as np\n\n# モデルの入出力スキーマの定義\ninput_schema = Schema([\n    ColSpec(\"string\", \"prompt\"),\n    ColSpec(\"double\", \"temperature\", optional=True),\n    ColSpec(\"integer\", \"max_tokens\", optional=True),\n    ColSpec(\"string\", \"stop\", optional=True),\n    ColSpec(\"integer\", \"candidate_count\", optional=True)\n])\n\noutput_schema = Schema([\n    ColSpec('string', 'predictions')\n])\n\nsignature = ModelSignature(inputs=input_schema, outputs=output_schema)\n\n# 入力サンプルの定義\ninput_example = {\n    \"prompt\": np.array([\n        \"Below is an instruction that describes a task. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n\"\n        \"What is Apache Spark?\\n\\n\"\n        \"### Response:\\n\"\n    ])\n}\n```\n\n最適化サービングを有効化するには、モデルを記録する際に以下のように`mlflow.transformers.log_model`を呼び出して追加のメタデータのディクショナリーを含めます:\n\n```\nmetadata = {\"task\": \"llm/v1/completions\"}\n```\n\nここで、モデルサービングエンドポイントで使用されるAPIシグネチャを指定します。\n\n```py\nimport mlflow\n\n# UCでのモデル管理を行わない場合には以下の行をコメントアウトします\n# 3レベルの名前空間ではなく、シンプルにモデル名を指定します\nmlflow.set_registry_uri('databricks-uc')\nCATALOG = \"quickstart_catalog_taka\"\nSCHEMA = \"llm-catalog\"\nregistered_model_name = f\"{CATALOG}.{SCHEMA}.llama2-7b\"\n\n# 新規MLflowランをスタート\nwith mlflow.start_run():\n    components = {\n        \"model\": model,\n        \"tokenizer\": tokenizer,\n    }\n    mlflow.transformers.log_model(\n        transformers_model=components,\n        task = \"text-generation\",\n        artifact_path=\"model\",\n        registered_model_name=registered_model_name,\n        signature=signature,\n        input_example=input_example,\n        metadata={\"task\": \"llm/v1/completions\"}\n    )\n```\n\nモデルが記録されます。4分弱かかります。\n![Screenshot 2023-10-08 at 17.12.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b5be9e26-2769-7b2e-89e1-8d6ca26504ff.png)\n\nカタログエクスプローラからも確認できます。\n![Screenshot 2023-10-08 at 17.11.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/8f723e93-54e8-e3fe-cbb0-3256730986b6.png)\n\n\n# ステップ2: モデルサービングGPUエンドポイントの設定および作成\n\nエンドポイント名を変更するには以下のセルを修正します。エンドポイントAPIを呼び出した後は、記録されたLlama2モデルは自動的に最適化LLMサービングにデプロイされます。`workload_type`は環境で利用できるスペックのGPUを指定する必要があります。\n\n```py\n# MLflowエンドポイント名の設定\nendpoint_name = \"taka-llama2-7b\"\n\n# 登録されたMLflowモデル名\nmodel_name = registered_model_name\n\n# MLflowモデルの最新バージョンの取得\nmodel_version = 1\n\n# 計算資源タイプの指定 (CPU, GPU_SMALL, GPU_MEDIUM, etc.)\nworkload_type = \"GPU_MEDIUM\"\n\n# 計算資源のスケールアウトサイズの指定 (Small, Medium, Large, etc.)\nworkload_size = \"Small\" \n\n# ゼロへのスケールの指定 (CPUエンドポイントでのみサポート)\nscale_to_zero = False \n\n# 現在のノートブックコンテキストからAPIエンドポイントとトークンを取得\nAPI_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get() \nAPI_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n```\n\n```py\nimport requests\nimport json\n\ndata = {\n    \"name\": endpoint_name,\n    \"config\": {\n        \"served_models\": [\n            {\n                \"model_name\": model_name,\n                \"model_version\": model_version,\n                \"workload_size\": workload_size,\n                \"scale_to_zero_enabled\": scale_to_zero,\n                \"workload_type\": workload_type,\n            }\n        ]\n    },\n}\n\nheaders = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n\nresponse = requests.post(url=f\"{API_ROOT}/api/2.0/serving-endpoints\", json=data, headers=headers)\n\nprint(json.dumps(response.json(), indent=4))\n```\n\n\n# エンドポイントの参照\n\nエンドポイントのより詳細な情報を参照するには、左のナビゲーションバーの**Serving**にアクセスし、エンドポイント名で検索します。\n![Screenshot 2023-10-09 at 15.00.59.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/046677f6-2e71-fae5-0954-b43861ec6b2c.png)\n\n# ステップ3: エンドポイントへのクエリー\n\nエンドポイントの準備ができたら、APIリクエストを行うことでクエリーできるようになります。モデルのサイズや複雑性に依存しますが、準備ができるまでに30分以上を要します。\n\n:::note\n**GPU Medium**の場合、30分程度でReadyになりました。\n:::\n\n```py\ndata = {\n    \"inputs\": {\n        \"prompt\": [\n            \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is Apache Spark?\\n\\n### Response:\\n\"\n        ]\n    },\n    \"params\": {\n        \"max_tokens\": 100, \n        \"temperature\": 0.0\n    }\n}\n\nheaders = {\n    \"Context-Type\": \"text/json\",\n    \"Authorization\": f\"Bearer {API_TOKEN}\"\n}\n\nresponse = requests.post(\n    url=f\"{API_ROOT}/serving-endpoints/{endpoint_name}/invocations\",\n    json=data,\n    headers=headers\n)\n\nprint(json.dumps(response.json(), indent=4))\n```\n\nクエリーできました！\n\n```json:JSON\n{\n    \"predictions\": [\n        {\n            \"candidates\": [\n                {\n                    \"text\": \"Apache Spark is an open-source data processing engine that can handle large-scale data processing tasks. It was developed at the University of California, Berkeley and is now maintained by Apache Software Foundation. Spark provides high-level APIs in Java, Python, Scala, and R, and supports a wide range of data sources, including Hadoop Distributed File System (HDFS), Cassandra, HBase, and Hive. Spark's core features include in-memory computing, fault\",\n                    \"metadata\": {\n                        \"finish_reason\": \"length\"\n                    }\n                }\n            ],\n            \"metadata\": {\n                \"input_tokens\": 41,\n                \"output_tokens\": 100,\n                \"total_tokens\": 141\n            }\n        }\n    ]\n}\n```\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksユーザー会JEDAIのメンバー数を分析する", "text": "Databricks入社以来、ユーザー会JEDAIのステアリングコミッティーメンバーなのです。\n\nhttps://jedai.connpass.com/?gmem=1\n\nかれこれ3年を迎えようとしている中、「メンバー数の変遷を見てみたい」と思った次第です。おかげさまで1400人を超えています。\n\nしかし、connpassでは[メンバー一覧を参照](https://jedai.connpass.com/participation/)することはできますが、データとしては取得ができませんでした。\n\nということで、スクレイピングさせていただきました。\n\n:::note\n**注意**\nスクレイピングする際には、[利用規約](https://connpass.com/about/api/)などに準拠するように注意してください。\n:::\n\n# メンバー数の取得\n\nconnpassのメンバー一覧ページはページネーションされているのですが、何ページまでかがレスポンスコードで判別できなかったので、メンバー数から計算します。\n\n```py\nimport requests, datetime, re\nfrom bs4 import BeautifulSoup\n\nuser_agent = 'Mozilla/5.0'\n\nurl = 'https://jedai.connpass.com/participation/'\nr = requests.get(url, headers={\"User-Agent\": user_agent})\nsoup = BeautifulSoup(r.text, \"html.parser\")\nnum_members = soup.find(\"h3\", \"title inline\").get_text()\nprint(num_members)\n\nnum_members = re.search(r'\\d+', num_members).group()\nprint(num_members)\n```\n\n```\n他のメンバー（1438人）\n1438\n```\n\n# ページ数の計算\n\n管理者数含めてないのでザックリと計算。\n\n```py\nmember_per_page = 100\npage_total = int(int(num_members) / member_per_page) + 1\nprint(page_total)\n```\n\n```\n15\n```\n\n# メンバー加入日の抽出\n\n```py\ndef drug_data(page):\n  url = 'https://jedai.connpass.com/participation/?page=' + str(page)\n  print(url)\n  \n  r = requests.get(url, headers={\"User-Agent\": user_agent})\n  soup = BeautifulSoup(r.text, \"html.parser\")\n        \n  ret_text_list = []\n  elems = soup.find_all(\"td\", \"join_date\")\n  for element in elems:\n    join_date_str = element.get_text()\n    join_date_str = re.sub(\"\\(*.\\)\", \"\", join_date_str)\n    #print(join_date_str)\n    dte = datetime.datetime.strptime(join_date_str, '%Y/%m/%d')\n\n    ret_text_list.append(dte)\n\n  return ret_text_list\n```\n\n```py\nmembers = []\ncount = 1\n\nwhile(count <= page_total):\n  ret_text_list = drug_data(count)\n\n  for element in ret_text_list:\n    members.append(element)\n\n  count = count + 1\n```\n\n```\nhttps://jedai.connpass.com/participation/?page=1\nhttps://jedai.connpass.com/participation/?page=2\nhttps://jedai.connpass.com/participation/?page=3\nhttps://jedai.connpass.com/participation/?page=4\nhttps://jedai.connpass.com/participation/?page=5\nhttps://jedai.connpass.com/participation/?page=6\nhttps://jedai.connpass.com/participation/?page=7\nhttps://jedai.connpass.com/participation/?page=8\nhttps://jedai.connpass.com/participation/?page=9\nhttps://jedai.connpass.com/participation/?page=10\nhttps://jedai.connpass.com/participation/?page=11\nhttps://jedai.connpass.com/participation/?page=12\nhttps://jedai.connpass.com/participation/?page=13\nhttps://jedai.connpass.com/participation/?page=14\nhttps://jedai.connpass.com/participation/?page=15\n```\n\n# メンバー数の確認\n\n```py\nlen(members)\n```\n\n合ってました(管理者も含まれています)。\n\n```\n1445\n```\n\n```py\nimport pandas as pd\ndf_members = pd.DataFrame(members)\ndf_members.columns = ['join_date']\ndf_members = df_members.sort_values(\"join_date\")\ndisplay(df_members)\n```\n\nビルトインの可視化機能で日毎の参加者数は確認できます。\n![Screenshot 2023-10-08 at 21.18.51.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1cbcf1ff-0f18-c7f3-449e-c4a5b6c34b05.png)\n\n# 日毎の加入者の集計\n\n累積値を計算するために日毎の合計加入者を計算します。\n\n```py\ndf_members_groupby_data = df_members.groupby('join_date').size().to_frame('size').reset_index()\ndisplay(df_members_groupby_data)\n```\n![Screenshot 2023-10-08 at 21.20.43.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/f8f3abdb-08ce-4cb9-be73-719fd1180fda.png)\n\n# 累積メンバー数の計算\n\n累積値を計算します。\n\n```py\ndf_members_groupby_data['cum_size'] = df_members_groupby_data['size'].cumsum()\ndisplay(df_members_groupby_data)\n```\n![Screenshot 2023-10-08 at 21.21.42.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/c1b84df8-391c-78de-98c6-f01b2aed85c6.png)\n\n可視化を設定して、累積値と日毎の加入者を表示します。\n![Screenshot 2023-10-08 at 21.22.13.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2bc1208c-07cd-cf63-317c-c4dee71b4145.png)\n![Screenshot 2023-10-08 at 21.22.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b95447f5-363e-1ee6-f121-e9416e35ddc2.png)\n\nできました！\n![Screenshot 2023-10-08 at 21.22.41.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/bca6134b-2dec-c0e4-c01e-ec1247ba1a0a.png)\n\nイベントをマッピングすることで、さらに情報を得ることができます。\n![Screenshot 2023-10-08 at 21.31.54.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/52bf4201-4232-d20c-f735-0054339f3218.png)\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksマーケットプレイスでunerryのショッパーみえーるサンプルを試す", "text": "こちらのニュースを見てからまだ試していませんでした。\n\nhttps://www.unerry.co.jp/news/databricks-marketplace/\n\nマーケットプレイスで`unerry`と検索します。\n![Screenshot 2023-10-07 at 17.34.51.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/3ff3af8a-6455-5b1a-b815-647e22430dba.png)\n\n(当たり前ですが)日本語での説明文でわかりやすいです。\n![Screenshot 2023-10-07 at 17.34.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/2326e681-0f8c-b369-676c-fe6cf6fd80fe.png)\n\n**即時アクセス**を取得をクリックします。そのまま取り込んでも大丈夫ですが、**その他のオプション**を展開して、取り込み先のカタログ名を変更します。\n![Screenshot 2023-10-07 at 17.36.24.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/834bf8fe-4f1e-dd15-1b38-cee5983c57d5.png)\n\nカタログにデータが取り込まれました。\n![Screenshot 2023-10-07 at 17.43.39.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ef4ae90a-a7a0-1f07-e591-6971e50671d7.png)\n\nサンプルノートブックが添付されているので、これもインポートします。\n![Screenshot 2023-10-07 at 17.36.51.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/9cd6fa31-c9ce-9f6a-7f76-aea119925842.png)\n\nこちらもコメントが日本語なので助かります。\n![Screenshot 2023-10-07 at 17.37.08.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/71d121fa-3096-219d-0cb7-5dc8b8e820dd.png)\n\nノートブックを実行していきます。途中のカタログ名は適宜変更します。\n\n```py\nspark_df = spark.sql(\"select * from `unerry_catalog`.default.sample_st_daily_visitor order by date\")\nspark_df.display()\n```\n![Screenshot 2023-10-07 at 17.59.29.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/89496437-4bac-4a0c-e39e-45044aafd1e1.png)\n\n日本の店舗名なのでリアリティが増します。\n![Screenshot 2023-10-07 at 18.00.27.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/6b41d53d-2cdf-5613-b06e-0a850aef4fc0.png)\n![Screenshot 2023-10-07 at 18.00.46.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/da73fb59-5973-b84c-cea9-a82bce132144.png)\n![Screenshot 2023-10-07 at 18.01.12.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/47ef4a45-f897-59b4-4e4c-66bc88015d51.png)\n\nなお、データ自体は[Unity Catalog](https://www.databricks.com/jp/product/unity-catalog)で管理されていますので、[Lakeview](https://qiita.com/taka_yayoi/items/4175cac499b5629de5a3)などその他のDatabricksの機能でも活用できます。\n![Screenshot 2023-10-07 at 18.11.48.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/fb74c2eb-01b3-d94a-9baa-8278c1d1b87f.png)\n\n\nプロバイダー募集中なので、興味がある方はこちらをご覧ください！\n\nhttps://docs.databricks.com/ja/marketplace/get-started-provider.html\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksの結果テーブルが新しくなりました！", "text": "ノートブックでSQLを実行するなどして、データを取得した際に表示されるのが結果テーブル(result table)です。\n\n右上の**New result table**をクリックし、トグルをONにすることで有効化できます。\n![Screenshot 2023-10-05 at 19.53.19.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/ccf20fe1-e480-9b66-b817-d88fd1aab142.png)\n\nデータ型がグラフィカルに確認できるようになっています。\n![Screenshot 2023-10-05 at 19.53.34.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/5ad56660-9b80-1f1a-055c-63cfbc4fcfe2.png)\n\nユーザー設定からも有効化できます。\n![Screenshot 2023-10-05 at 19.55.54.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/99081fa9-99ec-f2a6-f407-505e15175c84.png)\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksマーケットプレイスにおけるソリューションアクセラレータの提供", "text": "個人的には嬉しいです。\n\n# Databricksマーケットプレイスとは\n\nhttps://qiita.com/taka_yayoi/items/ced86daa2606232c308a\n\n> データ製品交換のためのオープンフォーラムであるDatabricksマーケットプレイスをご紹介します。Databricksマーケットプレイスは、データ提供者がセキュアにデータ製品を共有し、データ利用者に自分達の必要とするデータやデータサービスへのアクセスを探索、拡大するパワーを提供するために、Delta Sharingを活用しています。\n\n# ソリューションアクセラレータとは\n\nhttps://www.databricks.com/jp/solutions/accelerators\n\n> Databricks ソリューションアクセラレータは、さまざまな業界・業種に共通の主要なユースケースに対応する、フル機能の Notebook やベストプラクティスを含む目的に特化したガイドです。ソリューションアクセラレータを使用することで、発見、設計、開発、テストにかかる時間を短縮できます。発案から PoC までを 2 週間以内に完了できるよう設計されています。\n\n# ウォークスルー\n\nDatabricksマーケットプレイスで**プロバイダー**からDatabricksを選択します。\n![Screenshot 2023-10-05 at 16.48.32.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/24551586-ced6-9557-b651-a1de30814887.png)\n\nあるいは、**Product**からSolution acceleratorを選びます。\n![Screenshot 2023-10-07 at 17.32.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a3fe5888-e5af-d01f-823f-2ffb33b622fa.png)\n\n\n使用したいアクセラレータにアクセスし、**即時アクセス権を取得**をクリックします。無料です。\n![Screenshot 2023-10-05 at 16.48.06.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e4db23e5-ba7a-5771-17ef-db6ea87d1241.png)\n\nノートブックをリポジトリに取り込みます。\n![Screenshot 2023-10-05 at 16.49.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b5ed7833-27b1-f2f1-cab0-c256acf62b6a.png)\n\n右上のボタンが**開く**に変化するので、リポジトリをクリックします。\n![Screenshot 2023-10-05 at 16.49.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e98723b9-e9e4-57cd-de5a-902514e8f015.png)\n\nこれでソリューションアクセラレータがワークスペースに取り込まれました。\n![Screenshot 2023-10-05 at 16.49.39.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/95a3e686-2139-f306-3c53-469681f416d2.png)\n\n以前は、ファイルやURLからインポートしなくてはなりませんでしたが、よりクイックにソリューションアクセラレータを活用できるようになりました。ぜひお試しください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "大規模言語モデルに最適化されたDatabricksモデルサービング", "text": "[Model serving optimized for large language models \\| Databricks on AWS](https://docs.databricks.com/en/machine-learning/model-serving/llm-optimized-model-serving.html) [2023/9/28時点]の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n:::note info\n**プレビュー**\n本機能は[パブリックプレビュー](https://docs.databricks.com/release-notes/release-types.html)です。\n:::\n\n本書では、GPUを用いた大規模言語モデル(LLM)に最適化されたモデルサービングの有効化方法を説明します。\n\nLLM最適化モデルサービングは、従来のサービングアプローチと比較して3-5倍改善されたスループットとレーテンシーを提供します。現時点では、以下のLLMファミリーおよびプロプライエタリなデータでファインチューニングされた派生モデルをサポートしています:\n\n- MPTファミリーのモデル\n- LLaMA-V2モデル\n\n# 要件\n\n- LLM最適化モデルサービングは、[GPUデプロイメント](https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html#gpu)のパブリックプレビューの一部としてサポートされています。\n- お使いのモデルは、MLflow 2.4以降、Databricks機械学習ランタイム 13.2以降で記録される必要があります。\n- 大規模なモデルのアップロードとダウンロードを高速にするために、Unity Catalogでモデルを管理することをお勧めします。\n- モデルをデプロイする際には、あなたのモデルのパラメーターのサイズに適した計算資源のサイズを選択することが重要です。推奨値に関しては以下の表をご覧ください。500億以上のパラメータのモデルに関しては、必要なGPUにアクセスするためにDatabricks担当者に連絡いただくか、[プレビュー登録フォーム](https://docs.google.com/forms/d/1-GWIlfjlIaclqDz6BPODI2j1Xg4f4WbFvBXyebBpN-Y/edit?ts=65124ee0)を提出してください。\n![Screenshot 2023-10-04 at 9.05.05.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/df54afcf-20ad-8746-d3ce-72ad321798b8.png)\n\n# 大規模言語モデルの記録\n\n最初に、あなたのモデルをMLflowの`transformes`フレーバーで記録し、MLflowのメタデータで`metadata = {\"task\": \"llm/v1/completions\"}`をタスクフィールドに指定します。ここでは、モデルサービングエンドポイントで使用されるAPIのシグネチャを指定します。\n\nDatabricksのLLM最適化モデルサービングは、Databricks AIゲートウェイでサポートされているルートタイプと互換性があります。現時点では、`llm/v1/completions`です。サポートされていないモデルファミリーあるいはタスクタイプがある場合には、Databricks担当者にお問い合わせください。\n\n```py:Python\nmodel = AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b-instruct\",device_map=\"auto\",torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b-instruct\")\nwith mlflow.start_run():\n    components = {\n        \"model\": model,\n        \"tokenizer\": tokenizer,\n    }\n  mlflow.transformers.log_model(\n        \"model\",\n        transformers_model=MPT7BInstruct(),\n        input_example={\"prompt\": np.array([\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is Apache Spark?\\n\\n### Response:\\n\"])},\n        metadata={\"task\": \"llm/v1/completions\"},\n        registered_model_name='mpt'\n  )\n```\n\nモデルが記録されたら、以下のように`CATALOG.SCHEMA.MODEL_NAME`をモデルの3レベルの名前で置き換えて、Unity Catalogにモデルを登録します。\n\n```py:Python\nmlflow.set_registry_uri(\"databricks-uc\")\n\nregistered_model_name=CATALOG.SCHEMA.MODEL_NAME\n```\n\n# モデルサービングエンドポイントの作成\n\n次に、モデルサービングエンドポイントを作成します。あなたのモデルがLLM最適化フレームワークでサポートされているのであれば、サーブしようとした際にDatabricksが自動で最適化モデルサービングエンドポイントを作成します。\n\n```py:Python\nimport requests\nimport json\n\n# Set the name of the MLflow endpoint\nendpoint_name = \"llama2-3b-chat\"\n\n# Name of the registered MLflow model\nmodel_name = \"ml.llm-catalog.llama-13b\"\n\n# Get the latest version of the MLflow model\nmodel_version = 3\n\n# Specify the type of compute (CPU, GPU_SMALL, GPU_MEDIUM, etc.)\nworkload_type = \"GPU_MEDIUM_4\"\n\n# Specify the scale-out size of compute (Small, Medium, Large, etc.)\nworkload_size = \"Small\"\n\n# Specify Scale to Zero (only supported for CPU endpoints)\nscale_to_zero = False\n\n# Get the API endpoint and token for the current notebook context\nAPI_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\nAPI_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\n# send the POST request to create the serving endpoint\n\ndata = {\n    \"name\": endpoint_name,\n    \"config\": {\n        \"served_models\": [\n            {\n                \"model_name\": model_name,\n                \"model_version\": model_version,\n                \"workload_size\": workload_size,\n                \"scale_to_zero_enabled\": scale_to_zero,\n                \"workload_type\": workload_type,\n            }\n        ]\n    },\n}\n\nheaders = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n\nresponse = requests.post(\n    url=f\"{API_ROOT}/api/2.0/serving-endpoints\", json=data, headers=headers\n)\n\nprint(json.dumps(response.json(), indent=4))\n```\n\n# 入出力スキーマのフォーマット\n\n最適化されたLLMサービングエンドポイントには、Databricksがコントロールする入出力スキーマがあります。4つのフォーマットがサポートされています。\n\n- `dataframe_split`は、`split`オリエンテーションのJSONでシリアライズされたPandasデータフレームです。\n\n```json:JSON\n{\n  \"dataframe_split\":{\n    \"columns\":[\"prompt\"],\n    \"index\":[0],\n    \"data\":[[\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instructions:\\nWhat is Apache Spark?\\n\\n### Response:\\n\"]]\n  },\n  \"params\": {\n    \"temperature\": 0.5,\n    \"max_tokens\": 100,\n    \"stop\": [\"word1\",\"word2\"],\n    \"candidate_count\": 1\n  }\n}\n```\n\n- `dataframe_records`は、`records`オリエンテーションのJSONでシリアライズされたPandasデータフレームです。\n\n```json:JSON\n{\n  \"dataframe_records\": [{\"prompt\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instructions:\\nWhat is Apache Spark?\\n\\n### Response:\\n\"}],\n  \"params\": {\n    \"temperature\": 0.5,\n    \"max_tokens\": 100,\n    \"stop\": [\"word1\",\"word2\"],\n    \"candidate_count\": 1\n  }\n}\n```\n\n- `instances`\n\n```json:JSON\n{\n  \"instances\": [\n   {\n     \"prompt\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instructions:\\nWhat is Apache Spark?\\n\\n### Response:\\n\"\n   }\n  ],\n  \"params\": {\n  \"temperature\": 0.5,\n  \"max_tokens\": 100,\n  \"stop\": [\"word1\",\"word2\"],\n  \"candidate_count\": 1\n  }\n}\n```\n\n- `inputs`\n\n```json:JSON\n{\n  \"inputs\": {\n    \"prompt\": [\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instructions:\\nWhat is Apache Spark?\\n\\n### Response:\\n\"],\n  },\n  \"params\": {\n    \"temperature\": 0.5,\n    \"max_tokens\": 100,\n    \"stop\": [\"word1\",\"word2\"],\n    \"candidate_count\": 1\n  }\n}\n```\n\n# エンドポイントへのクエリー\n\nエンドポイントの準備ができると、APIリクエストを用いてクエリーを行うことができます。モデルのサイズと複雑性によりますが、エンドポイントの準備ができるまで30分以上を要します。\n\n```py:Python\n\ndata = {\n  \"inputs\" : [\"Hello, I'm a language model,\"],\n  \"params\" : {\"max_new_tokens\": 100, \"temperature\": 1}\n}\n\nheaders = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n\nresponse = requests.post(\n    url=f\"{API_ROOT}/serving-endpoints/{endpoint_name}/invocations\", json=data, headers=headers\n)\n\nprint(json.dumps(response.json()))\n```\n\n# 制限\n\n- GPUでサービングされるモデルのインストール要件が多いため、GPUサービングのコンテナイメージ作成は、CPUサービングのイメージ作成よりも時間を要します。\n    - また、モデルのサイズがイメージ作成にインパクトを与えます。例えば、300億以上のパラメーターを持つモデルは構築に少なくとも1時間を要します。\n    - 同じバージョンのモデルが次回デプロイされた際には、同じコンテナを再利用するので、以降のデプロイメントの時間は短縮されます。\n- GPUコンピュートでサービングされるモデルはセットアップに時間を要するので、GPUサービングのオートスケーリングはCPUサービングよりも時間を要します。リクエストのタイムアウトを避けるには多めにプロビジョニングすることをお勧めします。\n\n# サンプルノートブック\n\n以下のノートブックでは、最適化サービングエンドポイントの作成方法を説明しています。\n\n## Llama2モデルに対する最適化LLMサービングノートブック\n\nhttps://docs.databricks.com/_extras/notebooks/source/machine-learning/large-language-models/optimized-llama-serving.html\n\n## MPTモデルに対する最適化LLMサービングノートブック\n\nhttps://docs.databricks.com/_extras/notebooks/source/machine-learning/large-language-models/optimized-mpt-serving.html\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "DatabricksモデルサービングによるプライベートLLMのデプロイ", "text": "[Deploy Private LLMs using Databricks Model Serving \\| Databricks Blog](https://www.databricks.com/blog/announcing-gpu-and-llm-optimization-support-model-serving)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n:::note\n**注意**\n現時点(2023/10/4)では日本リージョンではリリースされていません。\n:::\n\nDatabricksモデルサービングにおけるGPUとLLM最適化サポートのパブリックプレビューを発表できることを嬉しく思っています！このローンチによって、レイクハウスプラットフォームにLLMやビジョンモデルを含む全てのタイプのオープンソースやカスタムAIモデルをデプロイすることができます。Databricksモデルサービングは自動で皆様のモデルのLLMサービングを最適化し、設定なしに最高クラスのパフォーマンスを提供します。\n\nDatabricksモデルサービングは、統合データ&AIプラットフォームで開発された最初のサーバレスGPUサービング製品です。これによって、データの取り込みからファインチューニング、モデルデプロイメント、モニタリングに至るGenAIアプリケーションの開発とデプロイすべて単一のプラットフォームで行うことができます。\n\n> Azureユーザーは、モデルサービングのGPUとLLM最適化サポートを有効化するには[サインアップフォーム](https://forms.gle/Lqpx7LhytRQ8eNCy6)を提出する必要があります。\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ve1-slllB8g?si=4E3KZ-XP6WVS6cSI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n# Databricksモデルサービングによる生成AIアプリの構築\n\n> 「Databricksモデルサービングによって、我々は顧客体験と改善し、オペレーションの効率を向上するために、我々のプロセスに生成AIをインテグレーションできるようになりました。モデルサービングによって、我々のデータとモデルに完全なコントロールを維持しながらも、LLMモデルをデプロイできるようになりました。」— Ben Dias, Director of Data Science and Analytics at easyJet - [詳細はこちら](http://www.databricks.com/blog/easyjet-bets-on-databricks-lakehouse-for-gen-ai)\n\n## インフラ管理を心配することなしにAIモデルをセキュアにホスティング\n\nDatabricksモデルサービングは、複雑なインフラストラクチャを理解する必要なしに、いかなるAIモデルをデプロイするための単一のソリューションを提供します。これは、モデルがスクラッチから構築されたのか、オープンソースを活用したのか、プロプライエタリのデータからファインチューニングされたのか、どのようにトレーニングされたのかに関係なく、自然言語、画像、音声、テーブルデータ、カスタムモデルをすべてデプロイできることを意味します。シンプルにMLflowでモデルを記録すうrと、CUDAのようなGPUライブラリを含むプロダクションレディなコンテナを自動で準備し、サーバレスGPUにデプロイします。我々のフルマネージドサービスは、皆様の工数を削減し、インスタンスの管理は不要となり、バージョンの互換性を維持し、バージョンをパッチします。このサービスは、トラフィックパターンに合わせて自動でインスタンスをスケールさせるので、レーテンシーパフォーマンスを最適化しつつもインフラストラクチャのコストを削減します。\n\n> 「Databricksモデルサービングは、意味論に基づく検索アプリケーションからメディアトレンドの予測に至る様々なユースケースにインテリジェンスを注入する我々の能力を加速させます。CUDAやGPUサーバーのスケーリングに関する複雑な作業を抽象化、簡素化することで、我々の真の専門性の領域、すなわち、インフラストラクチャの工数を気にすることなしに、我々のすべてのアプリケーションにおけるCondé NastのAI活用領域の拡大にフォーカスさせてくれました。」— Ben Hall, Sr. ML Engr at Condé Nast\n\n## 最適化LLMサービングによるレーテンシーとコストの削減\n\nDatabricksモデルサービングは、大規模言語モデルを効率的にサービングするための最適化を導入し、レーテンシーとコストを最大3-5倍削減します。最適化LLMサービングを用いることは信じられないほど簡単です: OSSあるいはファインチューンされた重みとモデルを指定するだけです。モデルが最適のパフォーマンスでサービングされるように、残りはすべてDatabricksによってケアされます。これによって、モデルの最適化のために低レベルのライブラリを記述することなしに、皆様のアプリケーションにLLMを組み込むことにフォーカスできるようになります。Databricksモデルサービングは、MPTやLlama2クラスのモデルを自動で最適化し、今後提供されるモデルもサポートされます。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-740-blog-img-1.png)\n*注意: Nvidia 4xA10におけるinput_tokens=512, output_tokens=64を指定したllama2-13bのベンチマーク*\n\n## レイクハウスAIインテグレーションを通じたデプロイメントの加速\n\nLLMをプロダクションに移行する際に行うのはモデルのデプロイだけではありません。検索拡張生成(RAG)やparameter-efficient fine-tuning(PEFT)、標準的なファインチューニングのようなテクニックを用いてモデルを補強する必要があります。さらに、LLLMの品質を評価し、モデルパフォーマンスや安全性を継続的にモニタリングする必要があります。多くの場合、チームは分散したツールの統合に多大なる時間を費やすこととなり、オペレーションの複雑性を増加させ、メンテナンスのオーバーヘッドを引き起こします。\n\nDatabricksモデルサービングは、統合データ&AIプラットフォームに構築されており、単一のプラットフォーム上でデータの取り込みからファインチューニング、デプロイメントに至る全体のLLMOpsの管理を可能とし、開発を加速し、エラーを最小化するAIライフサイクルに対する一貫性のあるビューを構築します。モデルサービングは、以下を含む様々な[レイクハウス](https://www.databricks.com/jp/blog/lakehouse-ai)のLLMサービスと連携します:\n\n- **ファインチューニング:** レイクハウスで皆様のプロプライエタリなデータで基盤モデルをファインチューニングすることで精度を改善し、差別化します。\n- **ベクトル検索インテグレーション:** RAGや意味検索ユースケース向けのベクトル検索をインテグレーションし、シームレスに実行します。[こちら](https://docs.google.com/forms/d/e/1FAIpQLSeeIPs41t1Ripkv2YnQkLgDCIzc_P6htZuUWviaUirY5P5vlw/viewform)からプレビューに登録ください。\n- **ビルトインのLLM管理:** 皆様のすべてのLLM呼び出しにおける中央APIレイヤーとしてのDatabricks AIゲートウェイトのインテグレーション。\n- **MLflow:** MLflowのPromptLab経由のLLMの評価、比較、管理。\n- **品質 & 診断:** モデルを監視、デバッグするためにDeltaテーブルに自動でリクエストとレスポンスをキャプチャ。トレーニングデータセットを生成するために、[Labelbox](https://docs.databricks.com/ja/partners/ml/labelbox.html)とのパートナーシップを通じて、このデータとあなたのラベルを結合することも可能です。\n- **統合ガバナンス:** Unity Catalogを用いて、モデルサービングで利用、生成されるデータ&AIの資産の全てを管理、制御します。\n\n![](https://cms.databricks.com/sites/default/files/inline-images/db-740-blog-img-2.png)\n\n## LLMサービングに信頼性とセキュリティを提供\n\nDatabricksモデルサービングは、データ、モデル、デプロイメント設定に対する完全なコントロールと、大規模な推論を実現する専用の計算リソースを提供します。お好きなクラウドリージョンで専用のリソースを持つことで、低いオーバーヘッドのレーテンシー、予測可能なパフォーマンス、SLAに支えられた保証によるメリットを享受することができます。さらに、皆様のサービングワークロードは複数レイヤーのセキュリティによって保護されており、最もセンシティブなタスクにおいてもセキュアで信頼できる環境を保証します。厳しい規制のある業界固有のコンプライアンス要件に答えるために、いくつかのコントロールを実装しています。詳細に関しては、[こちらのページ](https://www.databricks.com/trust)を参照いただくか、Databricksアカウントチームにお問い合わせください。\n\n# GPUとLLMサービングを使い始める\n\n- 起動しましょう！チュートリアル([AWS](https://docs.databricks.com/en/machine-learning/model-serving/llm-optimized-model-serving.html) | [Azure](https://learn.microsoft.com/azure/databricks/machine-learning/model-serving/llm-optimized-model-serving))を読んで最初のLLMをDatabricksモデルサービングにデプロイしましょう。\n- Azureを利用されているのであれば、GPU、LLMサービングを有効化するためには、[こちら](https://forms.gle/Lqpx7LhytRQ8eNCy6)にサインアップしてください。\n- Databricksモデルサービングの[ドキュメント](https://docs.databricks.com/en/machine-learning/model-serving/index.html)で詳細を確認ください。\n- [こちら](https://www.databricks.com/jp/blog/lakehouse-ai)から生成AIに対するDatabricksのアプローチをご覧ください。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "RAGアプリケーションにおけるLLM評価のベストプラクティス", "text": "[Best Practices for LLM Evaluation of RAG Applications \\| Databricks Blog](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n# DatabricksドキュメントBotのケーススタディ\n\nチャットbotは、大規模言語モデル(LLM)のパワフルなチャットと理由づけの能力を活用した最も広く導入されたユースケースです。検索拡張生成(RAG)アーキテクチャは、ハルシネーションを削減し、最新の情報を維持し、ドメイン固有の知識を活用するために、(ベクトルストアを経由した)知識ベースと生成モデル(GPT-3.5やGPT-4など)を組み合わせるため、すぐにチャットbotの開発における業界標準になりました。しかし、チャットbotのレスポンスの品質の評価は現時点でも未解決のままです。業界標準が定義されていないため、時間を浪費し、スケールしにくい人間による評点(ラベリング)に落ち着くことになりました。\n\n我々は、LLMの自動評価のベストプラクティスを形作る助けとなるプラクティスにセオリーを適用したので、自信を持ってクイックにRAGアプリケーションをプロダクションにデプロイすることができます。本記事では、LLM評価における学びを提供するために、Databricksで実施している一連の調査の最初の部分を説明します。この記事のすべてのリサーチはDatabricksのシニアソフトウェアエンジニアであり、[Databricks Documentation AI Assistant](https://docs.databricks.com/)のクリエーターである[Quinn Leng](mailto:quinn.leng@databricks.com)によって実施されました。\n\n# 自動評価実践における課題\n\n最近では、LLMコミュニティは自分たちのLLMのアウトプットを評価するために、GPT-4のようなパワフルなLLMを用いた自動評価のために「審判としてのLLM」の活用を探索しています。lmsysグループの[研究論文](https://arxiv.org/abs/2306.05685)では、ライティング、数学、世界の知識における審判として様々なLLM(GPT-4, ClaudeV1, GPT-3.5) のpros/consとフィージビリティを探索しています。\n\nこのようなすべての偉大な研究に関わらず、実践としてのLLM審判の適法方法に関しては、依然として数多くの未解答の質問が存在しています:\n\n- **人間の評点とのアラインメント:** ドキュメントのQ&Aチャットbotでは特に、回答の適切性、可読性、網羅性において、LLMの審判による評点は、実際の人間の嗜好をどの程度反映しているのでしょうか？\n- **サンプルを通じた精度:** LLM審判に対していくつかの評点例を提供することの効果はどのようなものでしょうか？異なるメトリクスに対してLLM審判の信頼性と再利用性をどのように増加させるのでしょうか？\n- **適切な評点のスケール:** 様々なフレームワークで様々な評点スケールが使用される(例: [AzureML](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/harness-the-power-of-large-language-models-with-azure-machine/ba-p/3828459)では0から100を使い、[langchain](https://autoevaluator.langchain.com/)ではバイナリースケールを使用)ので、どの評点スケールが推奨となるのでしょうか？\n- **ユースケース横断の適用性:** 同じ評価メトリックを用いて、どの程度まで評価メトリクスを異なるユースケース(例: 日常的なチャット、コンテンツの要約、検索拡張生成)で再利用できるのでしょうか？\n\n# RAGアプリケーションへの効果的な自動評価の適用\n\n上述した質問に対する選択肢をDatabricksにおけるチャットbotの文脈で探索しました。我々の発見は汎化することができ、低コストかつ迅速にRAGベースのチャットbotを効果的に評価する助けになるものと信じています:\n\n- **80%以上の判定で、審判としてのLLMは人間の評点と一致しています。** 我々のドキュメントベースのチャットbotの評価に対する審判としてのLLMは、人間の審判と同じように効果的であり、80%以上の斑点で完全にスコアがマッチし、95%以上の判定では1スコアの距離(0-3スケールを使用)となっています。\n- **サンプルとGPT-3.5を用いてコストを削減します。** それぞれの評点スコアに例を提供した場合、GPT-3.5をLLM審判として活用することができます。コンテキストサイズの制限のため、低精度の評点スケールを用いることが実践的です。GPT-4ではなく、GPT-3.5とサンプルを用いることで、LLM審判のコストを10倍削減し、スピードを3倍以上に高速化します。\n- **解釈を容易にするために低精度の評点スケールを使用します。** 0、1、2、3あるいはバイナリー(0、1)のような低精度の評点スケールでも、0から10.0や0から100.0のような高精度の評点スケールと同様の精度を保ちつつも、人間のアノテーターやLLM審判の両方に評点の指示を行うことが非常に簡単になることを知りました。また、低精度のスケールによって、異なるLLM審判(GPT-4やclaude2など)での評点スケールの一貫性を実現します。\n- **RAGアプリケーションには自身のベンチマークが必要です。** あるモデルは公開された特定のベンチマーク(カジュアルなチャット、数式、クリエイティブな記述など)では良いパフォーマンスを示すかもしれませんが、他のタスク(特定のコンテキストにおけるQ&A)での優れたパフォーマンスを保証するものではありません。ユースケースがマッチする場合にのみベンチマークを使用すべきであり、RAGアプリケーションはRAGベンチマークによってのみ評価されるべきです。\n\n我々の研究に基づき、LLM審判を用いる際には以下の手順を踏むことをお勧めします:\n\n1. 1-5の評点スケールを用いる\n1. 評点ルールを理解するためにサンプルなしにGPT-4をLLM審判として活用する\n1. スコアごとに一つの例を用いてGPT-3.5にLLM審判をスイッチする\n\n# ベストプラクティスを確立する我々の方法論\n\nこの記事の残りでは、これらのベストプラクティスを構成するために実施した一連の実験をウォークスルーします。\n\n## 実験環境\n\n![](https://cms.databricks.com/sites/default/files/inline-images/image10.png)\n\nこの実験では3つのステップがあります:\n\n1. **評価データセットの生成:** Databricksドキュメントから100の質問とコンテキストを作成しました。このコンテキストは質問に適切なドキュメント(の塊)を表現しています。\n![Screenshot 2023-10-03 at 8.09.01.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/de0ce5be-b259-c964-1f74-572e859df018.png)\n1. **回答シートの生成:** 評価データセットを用いて、解答を生成するためにいくつかの言語モデルにプロンプトを支持し、「回答シート」と呼ばれるデータセットに質問・コンテキスト・回答ペアを格納しました。この調査では、GPT-4、GPT-3.5、Claude-v1、Llama2-70b-chat、Vicuna-33b、mpt-30b-chatを使用しました。\n1. **評点の生成:** 回答シートを用いて、評点を生成し、理由づけを行うために様々なLLMを使用しました。評点は、適切性(60%の重み)、包括性(20%の重み)、可読性(20%の重み)から構成されます。生成された回答における適切性に対する嗜好を反映するためにこの重みづけスキームを選択しました。他のアプリケーションにおいては、これらの重みをチューニングするかもしれませんが、依然として適切性が重要な要素になると思います。\n\nさらに、潜在的なバイアスを回避し、信頼性を改善するために以下の技術を用いています:\n\n- 再現性を確保するために低いtemperature(temeratureは0.1)。\n- ペアごとの比較ではなく単一回答の評点。\n- 最終のスコアを出す前に評点プロセスに関する理由づけをLLMに行わせるためのCahin of thoughts。\n- それぞれの要素に対する個々のスコアの評点スコアにおける、いくつかの例がLLMに示されるFew-shots生成。\n\n## 実験1: 人間の評点とのアライン\n\n人間のアノテーターとLLM審判の一致レベルを確認するために、人間によるラベルを得るためにgpt-3.5-turboとvicuna-33bの回答シートをラベリング企業に送付し、GPT-4の評点アウトプットの結果と比較しました。以下が得られた知見です:\n\n- 適切性と可読性スコアにおいて、人間とGPT-4審判は80%以上の一致がありました。そして、要件を引き下げ1スコアの違いを許容するのであれば、一致レベルは95%以上となります。\n![](https://cms.databricks.com/sites/default/files/inline-images/image7_1.png)\n\n包括性メトリックのアライメントは低く、これはビジネスステークホルダーが「包括性」は適切性や可読性よりも主観的であると共有した内容とマッチしています。\n\n## 実験2: サンプルを通じた精度\n\nlmsysの論文では、レスポンスの有用性、適切性、精度、深さ、創造性、詳細レベルに基づいて評価を行うために、LLM審判への指示にこの[プロンプト](https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/data/judge_prompts.jsonl)を使用しています。しかし、この論文では評点指示の詳細は共有されていません。我々の研究では、以下のように最終スコアに大きな影響を与える要素が数多く存在することがわかりました:\n\n- 様々な要素の重要性: 有用性、適切性、精度、深さ、創造性\n- 有用性は曖昧であるというような要因の解釈\n- 回答は有用だが正確でないというように、それぞれの要素の競合\n\n以下をトライすることで、特定の評点スケールに対するLLM審判への指示を開発しました:\n\n1. **オリジナルのプロンプト**: lmsys論文で使用されたオリジナルのプロンプトは以下の通りです:\n\n    > Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format\n\n    我々の適切性、包括性、可読性に関するメトリクスを反映するために、オリジナルのlmsys論文のプロンプトを適合させ、(chain-of-thoughtの理由づけを活用するために)それぞれのスコアを出す前に一行の理由づけを行うように審判にプロンプトを提供しました。以下には、サンプルを指定しないzero-shotバージョンのプロンプトと、それぞれのスコアごとに一つの例を指定するfew-shotバージョンのプロンプトを示します。そして、入力として同じ回答シートを用い、2つのプロンプトタイプの評点結果を比較しました。\n\n1. **ゼロショットの学習**: には、LLM審判は我々の適切性、包括性、可読性を反映することが必要であり、それぞれのスコアごとに1行の理由づけを行うように審判にプロンプトを提供しています。\n\n    > Please act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided question based on a provided context.\n    > \n    >   You'll be given a function grading_function which you'll call for each provided context, question and answer to submit your reasoning and score for the correctness, comprehensiveness and readability of the answer. \n\n1. **数ショットの学習**: 評点スケールにおけるそれぞれのスコアに対する明示的なサンプルを提供するためにゼロショットプロンプトを修正しました:\n\n    >   Please act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided question based on a provided context.\n    >   \n    >   You'll be given a function grading_function which you'll call for each provided context, question and answer to submit your reasoning and score for the correctness, comprehensiveness and readability of the answer. \n    >   \n    >     \n    >   \n    >   Below is your grading rubric: \n    >   \n    >   - Correctness: If the answer correctly answer the question, below are the details for different scores:\n    >   \n    >     - Score 0: the answer is completely incorrect, doesn’t mention anything about the question or is completely contrary to the correct answer.\n    >   \n    >         - For example, when asked “How to terminate a databricks cluster”, the answer is empty string, or content that’s completely irrelevant, or sorry I don’t know the answer.\n    >   \n    >     - Score 1: the answer provides some relevance to the question and answers one aspect of the question correctly.\n    >   \n    >         - Example:\n    >   \n    >             - Question: How to terminate a databricks cluster\n    >   \n    >             - Answer: Databricks cluster is a cloud-based computing environment that allows users to process big data and run distributed data processing tasks efficiently.\n    >   \n    >             - Or answer:  In the Databricks workspace, navigate to the \"Clusters\" tab. And then this is a hard question that I need to think more about it\n    >   \n    >     - Score 2: the answer mostly answer the question but is missing or hallucinating on one critical aspect.\n    >   \n    >         - Example:\n    >   \n    >             - Question: How to terminate a databricks cluster”\n    >   \n    >             - Answer: “In the Databricks workspace, navigate to the \"Clusters\" tab.\n    >   \n    >             Find the cluster you want to terminate from the list of active clusters.\n    >   \n    >             And then you’ll find a button to terminate all clusters at once”\n    >   \n    >     - Score 3: the answer correctly answer the question and not missing any major aspect\n    >   \n    >         - Example:\n    >   \n    >             - Question: How to terminate a databricks cluster\n    >   \n    >             - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\n    >   \n    >             Find the cluster you want to terminate from the list of active clusters.\n    >   \n    >             Click on the down-arrow next to the cluster name to open the cluster details.\n    >   \n    >             Click on the \"Terminate\" button. A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.”\n    >   \n    >   - Comprehensiveness: How comprehensive is the answer, does it fully answer all aspects of the question and provide comprehensive explanation and other necessary information. Below are the details for different scores:\n    >   \n    >     - Score 0: typically if the answer is completely incorrect, then the comprehensiveness is also zero score.\n    >   \n    >     - Score 1: if the answer is correct but too short to fully answer the question, then we can give score 1 for comprehensiveness.\n    >   \n    >         - Example:\n    >   \n    >             - Question: How to use databricks API to create a cluster?\n    >   \n    >             - Answer: First, you will need a Databricks access token with the appropriate permissions. You can generate this token through the Databricks UI under the 'User Settings' option. And then (the rest is missing)\n    >   \n    >     - Score 2: the answer is correct and roughly answer the main aspects of the question, but it’s missing description about details. Or is completely missing details about one minor aspect.\n    >   \n    >         - Example:\n    >   \n    >             - Question: How to use databricks API to create a cluster?\n    >   \n    >             - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the request URL, then you can make the HTTP Request. Then you can handle the request response.\n    >   \n    >         - Example:\n    >   \n    >             - Question: How to use databricks API to create a cluster?\n    >   \n    >             - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the request URL, then you can make the HTTP Request. Then you can handle the request response.\n    >   \n    >     - Score 3: the answer is correct, and covers all the main aspects of the question\n    >   \n    >   - Readability: How readable is the answer, does it have redundant information or incomplete information that hurts the readability of the answer.\n    >   \n    >     - Score 0: the answer is completely unreadable, e.g. fully of symbols that’s hard to read; e.g. keeps repeating the words that it’s very hard to understand the meaning of the paragraph. No meaningful information can be extracted from the answer.\n    >   \n    >     - Score 1: the answer is slightly readable, there are irrelevant symbols or repeated words, but it can roughly form a meaningful sentence that cover some aspects of the answer.\n    >   \n    >         - Example:\n    >   \n    >             - Question: How to use databricks API to create a cluster?\n    >   \n    >             - Answer: You you  you  you  you  you  will need a Databricks access token with the appropriate permissions. And then then you’ll need to set up the request URL, then you can make the HTTP Request. Then Then Then Then Then Then Then Then Then\n    >   \n    >     - Score 2: the answer is correct and mostly readable, but there is one obvious piece that’s affecting the readability (mentioning of irrelevant pieces, repeated words)\n    >   \n    >         - Example:\n    >   \n    >             - Question: How to terminate a databricks cluster\n    >   \n    >             - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\n    >   \n    >             Find the cluster you want to terminate from the list of active clusters.\n    >   \n    >             Click on the down-arrow next to the cluster name to open the cluster details.\n    >   \n    >             Click on the \"Terminate\" button…………………………………..\n    >   \n    >             A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.\n    >   \n    >     - Score 3: the answer is correct and reader friendly, no obvious piece that affect readability.\n    >   \n    >   - Then final rating:\n    >   \n    >       - Ratio: 60% correctness + 20% comprehensiveness + 20% readability\n\nこの実験では、いくつかのことを学びました:\n\n- **GPT-4とFew shotsプロンプトでは、結果の一貫性大きな違いはありませんでした。** サンプルと詳細な評点手順を含めた際、様々なLLMモデルにおいてGPT-4の評点結果において大きな改善は認められませんでした。興味深いことに、スコアのレンジにおいてはわずかなブレを引き起こしました。\n![](https://cms.databricks.com/sites/default/files/inline-images/image4_3.png)\n![](https://cms.databricks.com/sites/default/files/inline-images/image9.png)\n- **GPT-3.5-turbo-16kに対していくつかの例を含めることで、スコアの一貫性を劇的に改善し、結果が有用なものとなりました。** 詳細な評点の手順や例を含めることで、GPT-3.5の評点結果が劇的に改善されました(チャートの右側)。GPT-4とGPT-3.5(スコア3.0 vs スコア2.6)では実際の平均スコアの値はわずかなものですが、ランキングと精度は一貫性を保っています。\n- 一方、評点手順なしのGPT-3.5では結果に一貫性がなく、全く使い物になりません(スクリーンショットの左)。\n- プロンプトが4kトークンよりも大きくなることがあるため、GPT-3.5-turboではなくGPT-3.5-turbo-16kを使用していることに注意してください。\n![](https://cms.databricks.com/sites/default/files/inline-images/image11.png)\n![](https://cms.databricks.com/sites/default/files/inline-images/image5_0.png)\n\n## 実験3: 適切な評点スケール\n\n審判としてのLLMに関する論文では、評点スケールで非整数値(float)の0-10のスケールを用いています。言い換えると、最終スコアで高精度の手順を使用しています。これらの高精度なスケールでは、以下のような問題を後段で引き起こすことがわかりました:\n\n- **一貫性**: 人間とLLM両方の評価者は、高精度の評点を行う際に同じスコアに対して同じ基準を持つことに苦慮しました。このため、低精度から高精度のスケールに移行した際、様々な審判においてアウトプットのスコアの一貫性が低下しました。\n- **説明可能性**: さらに、人間が判断した結果を用いてLLMが判定した結果を交差検証するさいには、回答の評点方法を指示として提供しなくてはなりません。高精度の評点スケールではそれぞれの「スコア」に対する正確な指示を提供することは非常に困難です。例えば、スコア5.6と比較してスコア5.1とスコア付けされる回答の良い例とは何でしょうか？\n\n使用するのに「ベスト」なもののガイドを提供するために、様々な低精度評点スケールで実験を行い、最終的には整数スケール0-3あるいは([Likert](https://en.wikipedia.org/wiki/Likert_scale)スケールが好きなのであれば)0-4をお勧めすることになりました。0-10、1-5、0-3、0-1をトライし、以下を学びました:\n\n- バイナリースケールは「使いやすさ」や「良い/悪い」のようにシンプルなメトリクスでは動作します。\n- 0-10のようなスケールでは、すべてのスコアで区別のつく評価基準を確立することが困難です。\n\n![](https://cms.databricks.com/sites/default/files/inline-images/image1_6.png)\n![](https://cms.databricks.com/sites/default/files/inline-images/image2_4.png)\n\n上のプロットにあるように、GPT-4とGPT-3.5の両方は異なる低精度評点スケールを用いた結果のランキングの一貫性を維持しています。すなわち、0-3、1-5のような低精度の評点スケールを用いることで、説明可能性と精度のバランスを取ることができます。\n\nこのため、人間によるラベルとのアラインを容易にし、スコアリングの評価基準の理由づけ、レンジのそれぞれのスコアの例を提供することを容易にするためには、0-3、1-5を評点スケールとして用いることをお勧めします。\n\n## 実験4: ユースケースに対する適用性\n\n[審判としてのLLM](https://arxiv.org/abs/2306.05685)に関する論文では、LLMと人間の審判の両方において、GPT-3.5に匹敵するモデルとしてVicuna-13Bをランクづけしました:\n![](https://cms.databricks.com/sites/default/files/inline-images/image8.png)\n\n(この図は論文 https://arxiv.org/pdf/2306.05685.pdf の図4です)\n\nしかし、我々のドキュメントQ&Aのユースケースの一連のモデルをベンチマークした際、コンテキストに基づく質問に回答する際、大規模なVicuna-33BモデルがGPT-3.5よりも非常に悪いパフォーマンスを示すことを発見しました。これらの発見は、GPT-4、GPT-3.5、(実験1に示したように)人間の審判によっても、GPT-3.5よりもVicuna-33Bのパフォーマンスが低いという結果によって検証されています。\n![](https://cms.databricks.com/sites/default/files/inline-images/image1_6.png)\n\n論文によって提案されているベンチマークデータセットを細かく見ていくと、[3カテゴリーのタスク](https://arxiv.org/pdf/2306.05685.pdf)(ライティング、数学、知識)が直接反映されておらず、コンテキストに基づく解答を合成するモデルの能力に貢献していませんでした。代わりに、ドキュメントQ&Aユースケースは直感的に、読解力と指示追従力に関するベンチマークを必要とします。このため、**評価結果はユースケース横断で転移することができず**、モデルがどれだけ顧客の要件に合致するのかを適切に評価するためには、ユースケース固有のベンチマークを構築する必要があります。\n\n# ベストプラクティスを活用するためにMLflowを使用\n\n上述の実験によって、我々はさざまな要素がどのようにチャットbotの評価に大きな影響を及ぼすのかを探索し、審判としてのLLMはドキュメントQ&Aユースケースにおいて人間の嗜好を大きく反映することを確認しました。Databricksにおいては、これらの発見に基づいてチームの皆様がLLMアプリケーションを効果的に評価できるように、MLflow Evaluation APIを進化させています。MLflow 2.4では、様々なモデルのテキスト出力を隣り合わせで比較できるように、LLM向けEvaluation APIを導入し、MLflow 2.6では毒性や複雑性のような評価のためのLLMベースのメトリクスを導入しており、近い将来に審判としてのLLMをサポートするように取り組んでいます！\n\n同時に、我々の研究で参照しているリソースを以下にまとめました:\n\n- [Doc\\_qa repository](https://github.com/databrickslabs/doc-qa)\n    - 実験を実行するために使用したコードとデータ\n- [LLM\\-as\\-Judge Research paper from lmsys group](https://arxiv.org/abs/2306.05685)\n    - この論文は、カジュアルチャットのユースケースにおいて審判としてのLLM(GPT-4、ClaudeV1、GPT-3.5)を用いた最初の研究であり、ライティング、数学、世界の知識におけるタスクの審判としてのLLMの活用における実現可能性とprosとconsを積極的に探索しています。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "レイクビューダッシュボードのパブリックプレビューの発表！", "text": "[Announcing the Public Preview of Lakeview Dashboards\\! \\| Databricks Blog](https://www.databricks.com/blog/announcing-public-preview-lakeview-dashboards)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n[レイクビューダッシュボード](https://docs.databricks.com/sql/user/dashboards/lakeview-dashboards.html)と名付けられた次世代Databricks SQLダッシュボードのパブリックプレビューを発表できることを嬉しく思っています。本日から利用できるこの新たなダッシュボード体験は、使いやすさ、広範な配布、ガバナンスとセキュリティに最適化されています。\n\nレイクビューは前世代のダッシュボードと比較して、主に4つの改善点があります:\n\n- **改善されたビジュアライゼーション:** 新たなビジュアライゼーションエンジンは、美しくインタラクティブなチャートを提供し、レンダリングが最大10倍高速です。\n- **共有と配布に最適化:** ドラフト/公開機能によって、利用者は信頼できる安定バージョンを操作しつつも、自由にダッシュボードを編集することができます。Databricksワークスペースに直接アクセスできない組織ないの利用者とセキュアに共有します。\n- **シンプルなデザイン:** WYSIWYGユーザー体験と整理された情報構造によって、レイクビューダッシュボードは背後のデータセットとウィジェットをバンドルします。もはや、参照しているクエリーの権限が同期されないことに心配する必要はありません。\n- **レイクハウスとの統合:** ビルトインのリネージを持つUnity Catalogによって管理されるので、利用者は常に洞察がどこからもたらされたのかを理解することができます。\n\n![](https://cms.databricks.com/sites/default/files/inline-images/db-754-blog-img-1.gif)\n\n# ベストなデータウェアハウスはレイクハウスです\n\n[Databricks SQL](https://www.databricks.com/jp/product/databricks-sql) (DBSQL)はレイクハウスにおけるサーバレスデータウェアハウスであり、従来のクラウドデータウェアハウスよりも少ないコストで、お好きなツールを用いてすべてのSQLやBIアプリケーションを大規模に実行することができます。ローンチ以来、アナリストはビジネスインテリジェンス(BI)ツールを[Databricks SQLウェアハウス](https://docs.databricks.com/ja/sql/admin/create-sql-warehouse.html)に接続し、プラットフォームのビルトインの分析機能(SQLクエリーエディタ、ダッシュボード、アラート)を活用しています。我々は、お客様が両方の機能を積極的に活用するのを目撃しています。\n\n過去一年を通じて、[Databricks SQLのイノベーション](https://www.databricks.com/blog/whats-new-databricks-sql)を継続しており、我々はお客様が組織で洞察をクイックに生成、共有するためにDBSQLダッシュボードを急激に利用するようになっており、サードパーティのBIツールで「プロダクション化」する前に自身のデータを用いてクイックにプロトタイピングしている様子を目撃しています。DBSQLダッシュボードの使用量は急増し、多くのエンハンスリクエストを受け取るようになりました。\n\nこれらのリクエストを受け、それらに対応するソリューションのパブリックプレビューをオープンしました。\n\n# 改善されたビジュアライゼーション\n\nレイクビューを使用する際に最初に気づくのは新たなカラーパレットと豪華なビジュアライゼーションです。アウトオブボックスで使う際にもエレガントであり、大規模なチャートでも最大10倍レンダリングが高速です。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-754-blog-img-2.png)\n\n改善されたビジュアルデザインと高速なレンダリングに加え、新たなビジュアライゼーションライブラリは並び替えられたツールチップや、改善されたnull値ハンドリングのように改善された挙動を提供します。チャートのエキスポートは、プレゼンテーションの埋め込みをよりプロフェッショナルなものにするために透明な背景もサポートしています。\n\n# 共有や配布に最適化\n\nレイクビューダッシュボードは、ドラフト/公開の機能とDatabricksワークスペースへのアクセス権を持たない企業内のユーザーへのセキュアな共有の両方をサポートしています。\n\nドラフト/公開の導入によって、ダッシュボードの作成者は利用者が日々使用するクリーンで整理されたバージョンのダッシュボードを邪魔することなしに、作業中のバージョンに対して変更を繰り返すことができます。\n\nレイクビューダッシュボードの組織への共有機能によって、ダッシュボード作成者は、自身たちのアイデンティティプロバイダー(IdP)に存在しますが特定のワークスペースへのアクセス権を持たないユーザーにセキュアにダッシュボードを共有することができます。レイクハウスの洞察に対するアクションの実行がワークスペースの壁に阻まれることはありません！(注意: これは現状は[統合ログイン](https://docs.databricks.com/ja/administration-guide/users-groups/single-sign-on/index.html#unified-login)とJust-in-Timeのユーザープロビジョニングプレビューに依存していますが、まもなく皆様が利用できるようになります！)\n\n# シンプルなデザイン\n\nレイクビューでは、再設計されたダッシュボード作成体験、ビジュアライゼーション設定体験を導入しました。新たな体験によって、ダッシュボードのデザイナーは完全なデータを用いて、キャンバス上で直接編集を行うことができます。スタートする際のSQLはオプションです。[Unity Catalog](https://www.databricks.com/jp/product/unity-catalog)のテーブルやビューを検索、選択し、ビジュアライゼーション設定で時間のビンや集計を適用します。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-754-blog-img-3.gif)\n\nまた、ダッシュボードの背後の情報構造も整理しています。それぞれのレイクビューダッシュボードは、ダッシュボード自身に背後のデータセット定義をバンドルします。全てがユニットとして作成、パッケージされ、共有されます。これによって、過去のDatabricks SQLダッシュボードと比較した際に、アーティファクトのライフサイクル、権限管理を円滑なものとします。\n\n# レイクハウスとの統合\n\nレイクハウスでダッシュボードを構築する際の最大のメリットの一つは、レイクハウスで構築することで、ダッシュボード作成者はより広範なプラットフォームのメリットを活用できるということを繰り返し聞いてきました。\n\nこの強みを強化する取り組みの一つは、Unity Catalogの自動[データリネージ](https://docs.databricks.com/ja/data-governance/unity-catalog/data-lineage.html)へのインテグレーションです。レイクビューダッシュボードが自動的にキャプチャされるだけではなく、公開されたダッシュボードの利用者体験にリネージ機能を直接組み込みました。\n![](https://cms.databricks.com/sites/default/files/inline-images/db-754-blog-img-4.png)\n\nダッシュボードのユーザーが共通して持つ質問の二つは「このデータを信頼できるのか？」と「データはどこから来ているのか？」です。レイクビューダッシュボードを用いることで、我々はビジネスユーザーがこれらの質問に自分たちで簡単に回答できるようにしています。\n\nもちろん、レイクビューダッシュボードはDatabricksのレイクハウスプラットフォームの一部として直接利用できるので、管理すべき抽出結果はなく、追加で管理するシャドーのデータウェアハウスも不要であり、データサイズに制限はなく、大規模に運用する際に別個のBIサーバーは不要です。\n\n# 次に来るのは\n\n向こう数ヶ月でレイクビューダッシュボードにに対する更なるエキサイティングなアップデートを予定しています。レンダリングと実行パフォーマンスはさらに改善されていきます。Gitサポート、埋め込み、高度なフィルタリング、インタラクティブなコントロールなどを楽しみにしていてください！\n\n新たなレイクビューダッシュボードと、レガシーなDBSQLダッシュボードは一定期間共存しますが、最終的にはレイクビューダッシュボードに統合されます。\n\nレイクビューダッシュボードは、[AWS](https://docs.databricks.com/sql/user/dashboards/lakeview-dashboards.html)や[Azure](https://learn.microsoft.com/azure/databricks/sql/user/dashboards/lakeview-dashboards)でパブリックプレビューとして利用できます。皆様が何を構築するのかを見るのが楽しみです！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksの推論テーブルを試してみる", "text": "こちらの記事を一通り読んだので推論テーブル(inference tables)を試してみます。\n\nhttps://qiita.com/taka_yayoi/items/b6a10ce0ff0d709d2349\n\nhttps://qiita.com/taka_yayoi/items/bce5df57ae5d90bdf971\n\n:::note\n**注意**\nこの機能は[サーバレスモデルサービングのエンドポイント](https://docs.databricks.com/ja/machine-learning/model-serving/index.html)を前提としています。まだ日本リージョンでは利用できません。すみません。\n:::\n\n# 推論テーブルとは\n\n推論テーブルは、お使いの[モデルサービングエンドポイント](https://docs.databricks.com/ja/machine-learning/model-serving/index.html)におけるリクエストとレスポンスを自動で捕捉し、Unity CatalogのDeltaテーブルに記録します。MLモデルを監視、デバッグ、改善するためにこのテーブルのデータを後で活用することができます。\n\n例えば、推論テーブルを活用することで以下のようなことを達成することができます:\n\n- お使いのモデルの次のイテレーションの再トレーニングデータセットの作成。\n- プロダクションのデータとモデルの品質モニタリングの実施。\n- 疑わしい推論の診断とデバッグの実行。\n- 再ロードされるべき誤ったラベルのデータの作成。\n\n# 推論テーブルの有効化\n\nすでに、作成済みのモデルサービングエンドポイントがあったので、設定変更の際に有効化します。この時点では`Not enabled`となっています。\n![Screenshot 2023-10-01 at 17.02.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/48fdb8e6-c54a-27e4-7e14-bca9349ecb97.png)\n\n編集状態にして、**Inference tables (optional)** を展開します。\n![Screenshot 2023-10-01 at 17.03.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a2677f76-f26a-72fe-914f-44b058fe8f65.png)\n\n事前に格納用のカタログとスキーマ(データベース)を作成しておきます。ここでは、`quickstart_catalog_taka`と`inference_tables`となります。指定したら**Update serving endpoint**をクリックします。\n![Screenshot 2023-10-01 at 17.04.10.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e16b090d-0980-5031-bf70-c29c00971644.png)\n\n少し待つと**Inference table**が`Enabled`になります。\n![Screenshot 2023-10-01 at 17.07.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/dcc5e5ad-31dc-0310-6ebd-e40ce991d5bf.png)\n\nその下のテーブル名のリンクをクリックすると、カタログエクスプローラが開き推論テーブルにアクセスすることができます。まだこの時点ではレコードは存在しません。\n![Screenshot 2023-10-01 at 17.08.56.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/1e954b5c-130f-c443-4cc3-1e3f0d424871.png)\n\n# 推論テーブルの動作確認\n\nモデルサービングエンドポイントを呼び出してみます。なお、このエンドポイントはこちらで作成したDatabricks Q&A botのものです。\n\nhttps://qiita.com/taka_yayoi/items/43302d79369a36b66ee4\n\n![Screenshot 2023-10-01 at 17.10.02.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/d40beedc-1a7a-0aa4-83d7-18689b2f0b07.png)\n\n何度か問い合わせを行います。この際にリクエストとレスポンスが推論テーブルに記録されます。\n![Screenshot 2023-10-01 at 17.10.37.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b18288eb-ecf9-934f-cfc8-5c3d2c90926b.png)\n\n数分経ってからテーブルにアクセスするとレコードを確認できました！\n![Screenshot 2023-10-01 at 17.15.47.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/a0b000c4-8e94-093d-c2b6-b26bd9bea3d1.png)\n\n次はパーシングなど行ったり、[こちらのノートブック](https://qiita.com/taka_yayoi/items/3f2ad387de549d64ee74)も試してみます。特に大規模言語モデルを運用する際には、どのようなリクエストが来ていて、それに対して大規模言語モデルがどのようなレスポンスをしているのかをモニタリングすることは品質面でも重要なことかと思います。(日本で利用できるようになったら)推論テーブルを是非ご活用ください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksモデルサービングエンドポイントのモニタリング", "text": "[Monitor model serving endpoints \\| Databricks on AWS](https://docs.databricks.com/en/lakehouse-monitoring/monitor-endpoints.html) [2023/9/25時点]の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n:::note info\n**プレビュー**\n本機能は[パブリックプレビュー](https://docs.databricks.com/release-notes/release-types.html)です。\n:::\n\nDatabricksレイクハウスモニタリングを用いることで、[推論テーブル](https://qiita.com/taka_yayoi/items/b6a10ce0ff0d709d2349)により[Databricksモデルサービング](https://docs.databricks.com/ja/machine-learning/model-serving/index.html)エンドポイントの品質をモニタリングすることができます。\n\n# 要件\n\n推論テーブルを用いてサービングエンドポイントをモニタリングするには、推論テーブルのプレビューにも参加し、両方の機能の要件を満たす必要があります。これらのプレビューに参加するにはDatabricks担当者にお問い合わせください。\n\n# 推論テーブルのモニタリング\n\n以下のステップでエンドポイントをモニタリングすることができます:\n\n1. エンドポイント作成時あるいは更新時に、お使いのエンドポイントの[推論テーブル](https://qiita.com/taka_yayoi/items/bce5df57ae5d90bdf971)を有効化します。\n1. エンドポイントのスキーマに沿って、推論テーブルのJSONペイロードをアンパッキングし、処理するためのワークフローをスケジュールします。\n1. (オプション)モデル品質メトリクスを計算できるように正解ラベルを用いて、リクエストとレスポンスを結合します。\n1. 結果のDeltaテーブルに対するモニターを作成し、メトリクスを更新します。\n\n以下の[スターターノートブック](#推論テーブルモニタリングのスターターノートブック)で、このワークフローを実装しています。\n\n# 推論テーブルモニタリングのスターターノートブック\n\n以下のノートブックでは、推論テーブルからリクエストをアンパッキングし、モニタリングを行うための上述のステップを実装しています。このノートブックはオンデマンドあるいは[Databricksワークフロー](https://docs.databricks.com/ja/notebooks/schedule-notebook-jobs.html)を用いてスケジュール実行することができます。\n\n## 推論テーブル、レイクハウスモニタリングスターターノートブック\n\nhttps://docs.databricks.com/_extras/notebooks/source/monitoring/inference-table-monitor.html\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksにおけるモデルサービングの推論テーブルの有効化", "text": "[Enable inference tables on model serving endpoints \\| Databricks on AWS](https://docs.databricks.com/en/machine-learning/model-serving/enable-model-serving-inference-tables.html) [2023/9/27時点]の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n:::note info\n**プレビュー**\n本機能は[パブリックプレビュー](https://docs.databricks.com/release-notes/release-types.html)です。\n:::\n\n本書では、ご自身のモデルデプロイメントを監視、デバッグできるように、[モデルサービングエンドポイント](https://docs.databricks.com/ja/machine-learning/model-serving/index.html)の推論テーブルの有効化の方法を説明します。\n\n推論テーブルは、モデルサービングエンドポイントの作成時あるいは、既存のエンドポイントの更新の際に、Databricksの機械学習サービングUI、あるいはDatabricks機械学習モデルサービングAPIで有効化することができます。エンドポイントを作成したユーザーがワークスペースから削除された際に、推論テーブルが影響を受けないように、サービスプリンシパルを用いてエンドポイントを作成することをお勧めします。\n\n推論テーブルの有効化後は、エンドポイントの作成者がテーブルの所有者になります。テーブルに対するすべてのアクセスコントロールリスト(ACL)は、標準的なUnity Catalogの権限に従い、テーブル所有者によって変更することができます。\n\n[Databricksにおけるモデルサービングエンドポイントの推論テーブル](https://qiita.com/taka_yayoi/items/b6a10ce0ff0d709d2349)をご覧ください。\n\n# 要件\n\n- この機能はパブリックプレビューでのみ利用できます。パブリックプレビューに参加するには、Databricks担当者に連絡するか、[inference tables preview enrollment form](https://docs.google.com/forms/d/14MGQZxxeLRI7ycmWcU5H1mx6o3AyW7cgucLlfnvha2s/edit)を提出してください。\n- ワークスペースでUnity Catalogが有効化されている必要があります。\n- エンドポイントで推論テーブルを有効化するには、エンドポイントの作成者あるいは変更者は以下の権限が必要です:\n    - エンドポイントに対する`CAN_MANAGE`権限\n    - 指定したカタログに対する`USE CATALOG`権限\n    - 指定したスキーマに対する`USE SCHEMA`権限\n    - スキーマにおける`CREATE TABLE`権限\n\n# UIを用いた推論テーブルの有効化\n\nDatabricks機械学習サービングエンドポイントのUIから推論テーブルを有効化することができます。\n\nエンドポイント作成時に推論テーブルを有効化するには、以下のステップを踏みます:\n\n1. Databricks機械学習UIで**Serving**をクリックします。\n1. **Create serving endpoint**をクリックします。\n1. エンドポイント作成ページで、**Enable inference table**を選択します。\n1. テーブルを格納したいカタログとスキーマを選択します。\n1. オプションとして、最終的なテーブル名のテーブルプレフィクスを指定します。テーブルプレフィクスが指定されない場合、エンドポイント名が使用されます。\n1. エンドポイントの設定を行った後に**Create serving endpoint**をクリックします。エンドポイントページ二委同士、右上に新たに設定された推論テーブルのパスが表示されます。\n\n既存のエンドポイントで推論テーブルを有効化することができます。既存のエンドポイントの設定を変更するには、以下のステップを実施します:\n\n1. エンドポイントページに移動します。\n1. **Edit configuration**をクリックします。\n1. **Enable inference table**を選択します。\n1. テーブルを格納したいカタログとスキーマを選択します。\n1. オプションとして、最終的なテーブル名のテーブルプレフィクスを指定します。テーブルプレフィクスが指定されない場合、エンドポイント名が使用されます。\n1. エンドポイントの設定を行った後に**Update serving endpoint**をクリックします。\n\n# client_request_idの指定\n\n`client_request_id`フィールドは、ユーザーがモデルサービングのリクエストボティに指定できるオプションの値です。これによって、推論テーブルの`client_request_id`に値を埋め込むことができ、正解ラベルなど`client_request_id`を持つ他のテーブルとリクエストのデータを結合するために活用することができます。`client_request_id`を指定するには、リクエストのペイロードのトップレベルのキーとして`client_request_id`を指定します。`client_request_id`が指定されない場合、リクエストに対応する行では値はnullとして表示されます。\n\n```json:JSON\n{\n  \"client_request_id\": \"<user-provided-id>\",\n  \"dataframe_records\": [\n    {\n      \"sepal length (cm)\": 5.1,\n      \"sepal width (cm)\": 3.5,\n      \"petal length (cm)\": 1.4,\n      \"petal width (cm)\": 0.2\n    },\n    {\n      \"sepal length (cm)\": 4.9,\n      \"sepal width (cm)\": 3,\n      \"petal length (cm)\": 1.4,\n      \"petal width (cm)\": 0.2\n    },\n    {\n      \"sepal length (cm)\": 4.7,\n      \"sepal width (cm)\": 3.2,\n      \"petal length (cm)\": 1.3,\n      \"petal width (cm)\": 0.2\n    }\n  ]\n}\n```\n\n`client_request_id`に関連づけられたラベルを持つ他のテーブルがあるのであれば、正解ラベルとの結合で`client_request_id`を活用することができます。\n\n# 推論テーブルでログをチェック\n\nサービングするモデルの準備ができると、あなたのモデルに対するすべてのリクエストは自動で推論テーブルに記録されます。データカタログを開いてエンドポイントにクエリーを行うために、UI上の推論テーブルの下にあるリンクを選択することで、推論テーブルに書き込まれたログを参照することができます。あるいは、エンドポイント呼び出しURLをコールし、APIを用いて自動生成されたUnity Catalogのテーブルにクエリーすることでログを参照することができます。[サービスを提供しているエンドポイントにスコアリングリクエストを送信する](https://docs.databricks.com/ja/machine-learning/model-serving/score-model-serving-endpoints.html)をご覧ください。\n\nエンドポイントを呼び出すと、スコアリングのリクエストを送信してから10分以内に推論テーブルに呼び出しが記録されていることを確認することができます。さらに、Databricksは少なくとも一回ログがデリバリーされることを保証しているので、場合によっては重複したログが送信されることがあります。\n\nエンドポイントをクエリーした後は、エンドポイントのリクエストとレスポンスをDeltaテーブルで確認することができます。以下のSQLクエリーはエンドポイントの入出力を表示します。\n\nDatabricks SQLのUIやノートブックからこのクエリーを実行することができ、`payload_table`は`auto_capture_config`レスポンスの`state`で確認することができます。\n\n```sql:SQL\nSELECT * FROM <catalog>.<schema>.<payload_table>\n```\n\n# APIを用いたエンドポイント作成時の推論テーブルの有効化\n\nAPIを用いてエンドポイントを作成する際に、推論テーブルを有効化することができます。[モデルサービスエンドポイントの作成と管理](https://docs.databricks.com/ja/machine-learning/model-serving/create-manage-serving-endpoints.html)をご覧ください。\n\nAPIでは、リクエストボディに以下を指定するための`auto_capture_config`を含めます:\n\n- Unity Catalogのカタログ: テーブルを格納するカタログ名\n- Unity Catalogのスキーマ: テーブルを格納するスキーマ名\n- (オプション)テーブルのプレフィクス: 推論テーブル名のテーブルプレフィクスを指定します。テーブルプレフィクスが指定されない場合、エンドポイント名が使用されます。\n- (オプション)enabled: 推論テーブルを有効化するか無効化するのかを指定するブール値。デフォルトは`true`です。\n\nカタログ、スキーマ、オプションのテーブルプレフィクスを指定した後は、`<catalog>.<schema>.<table_prefix>_payload`にテーブルが作成されます。このテーブルは自動的に[Unity Catalogのマネージドテーブル](https://docs.databricks.com/ja/data-governance/unity-catalog/create-tables.html)となります。\n\n:::note\n**注意**\n推論テーブルは常にエンドポイントの作成、更新時に作成されるので、既存テーブルの指定はサポートされていません。\n:::\n\n以下の例では、エンドポイント作成時の推論テーブルの有効化の方法を示しています。\n\n```bash:Bash\nPOST /api/2.0/serving-endpoints\n\n{\n  \"name\": \"feed-ads\",\n  \"config\":{\n    \"served_models\": [\n      {\n       \"model_name\": \"ads1\",\n       \"model_version\": \"1\",\n       \"workload_size\": \"Small\",\n       \"scale_to_zero_enabled\": true\n      }\n    ],\n    \"auto_capture_config\":{\n       \"catalog_name\": \"ml\",\n       \"schema_name\": \"ads\",\n       \"table_name_prefix\": \"feed-ads-prod\"\n    }\n  }\n}\n```\n\nレスポンスは以下のようなものとなります:\n\n```json:JSON\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"customer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666829055000,\n  \"state\": {\n    \"ready\": \"NOT_READY\",\n    \"config_update\": \"IN_PROGRESS\"\n  },\n  \"pending_config\": {\n    \"start_time\": 1666718879000,\n    \"served_models\": [\n      {\n        \"name\": \"ads1-1\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"1\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_CREATING\",\n          \"deployment_state_message\": \"Creating\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666829055000\n    }\n   ],\n   \"config_version\": 1,\n   \"traffic_config\": {\n     \"routes\": [\n       {\n         \"served_model_name\": \"ads1-1\",\n         \"traffic_percentage\": 100\n       }\n      ]\n   },\n   \"auto_capture_config\": {\n     \"catalog_name\": \"ml\",\n     \"schema_name\": \"ads\",\n     \"table_name_prefix\": \"feed-ads-prod\",\n     \"state\": {\n       \"payload_table\": {\n         \"name\": \"feed-ads-prod_payload\"\n       }\n     },\n     \"enabled\": true\n   }\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n```\n\n推論テーブルへの記録が有効化されたら、エンドポイントが起動するのを待ちます。その後で呼び出せるようになります。\n\n推論テーブルを作成した後は、システムによってスキーマ進化やデータ追加は管理されます。\n\n:::note alert\n**警告！**\n以下のことを行うとテーブルがデグレードあるいは破損します:\n\n- テーブルスキーマの変更\n- テーブル名の変更\n- テーブルの削除\n- Unity Catalogのカタログ、スキーマへの権限の喪失\n\nテーブルがデグレードした場合、推論テーブルを使い続けるには新規に推論テーブルを作成する必要がある場合があります。エンドポイントのステータスの`auto_capture_config`では、ペイロードデーブルの`FAILED`状態が表示されます。\n:::\n\n以下のオペレーションはテーブルの一貫性に影響を及ぼしません:\n\n- テーブルに対するOPTIMIZE、ANALYZE、VACUUMの実行。\n- 古い未使用データの削除。\n\n`auto_capture_config`を指定しない場合、デフォルトでは以前の設定バージョンの設定が使用されます。例えば、すでに推論テーブルが有効化されているのであれば、次のエンドポイントの更新では同じ設定が使用され、推論テーブルが無効化されている場合には無効化されたままとなります。\n\n```json:JSON\n{\n  \"served_models\": [\n    {\n      \"name\":\"current\",\n      \"model_name\":\"model-A\",\n      \"model_version\":\"1\",\n      \"workload_size\":\"Small\",\n      \"scale_to_zero_enabled\":true\n    }\n  ],\n  \"auto_capture_config\": {\n    \"enabled\": false\n  }\n}\n```\n\n# APIを用いた既存エンドポイントの推論テーブルの有効化\n\nAPIを用いて既存エンドポイントの推論テーブルを有効化することもできます。推論テーブルが有効化されると、推論テーブルを使い続けるためには以降のエンドポイントAPIの`auto_capture_config`ボディには同じものを指定し続けます。\n\n:::note\n**注意**\n推論テーブル有効化後のテーブルロケーションの変更はサポートされていません。\n:::\n\n```bash:Bash\nPUT /api/2.0/serving-endpoints/{name}/config\n\n{\n  \"served_models\": [\n    {\n      \"name\":\"current\",\n      \"model_name\":\"model-A\",\n      \"model_version\":\"1\",\n      \"workload_size\":\"Small\",\n      \"scale_to_zero_enabled\":true\n    },\n    {\n      \"name\":\"challenger\",\n      \"model_name\":\"model-B\",\n      \"model_version\":\"1\",\n      \"workload_size\":\"Small\",\n      \"scale_to_zero_enabled\":true\n    }\n  ],\n  \"traffic_config\":{\n    \"routes\": [\n      {\n        \"served_model_name\":\"current\",\n        \"traffic_percentage\":\"50\"\n      },\n      {\n        \"served_model_name\":\"challenger\",\n        \"traffic_percentage\":\"50\"\n      }\n    ]\n  },\n  \"auto_capture_config\":{\n   \"catalog_name\": \"catalog\",\n   \"schema_name\": \"schema\",\n   \"table_name_prefix\": \"my-endpoint\"\n  }\n}\n```\n\n# 推論テーブルの無効化\n\n推論テーブルを無効化するには、ユーザーは更新APIで`auto_capture_config`を指定してエンドポイント更新を行う必要があります。無効化においては、ユーザーはカタログ、スキーマ、テーブルプレフィクスを指定する必要はありません。必要なフィールドは`enabled: false`のみです。\n\nまた、Databricks機械学習UIで推論テーブルを無効化することができます。\n\n1. エンドポイントページに移動します。\n1. **Edit configuration**をクリックします。\n1. **Enable inference table**の選択を解除します。\n1. エンドポイントの設定が完了したら、**Update serving endpoint**をクリックします。\n\n:::note warn\n**重要！**\nエンドポイントの推論テーブルが無効化された後の当該エンドポイントでの再有効化はサポートされていません。推論テーブルを使い続けるには、推論テーブルを有効化するための新規エンドポイントを作成する必要があります。\n:::\n\n# 次のステップ\n\n推論テーブルを有効化すると、Databricksレイクハウスモニタリングを用いてモデルサービングエンドポイントとサービングされているモデルをモニタリングできるようになります。詳細は[Monitor model serving endpoints](https://docs.databricks.com/en/lakehouse-monitoring/monitor-endpoints.html)をご覧ください。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Preferred NetworksのPLaMo-13BをDatabricksで動かしてみる", "text": "週末の趣味みたいになってますが。\n\nhttps://www.preferred.jp/ja/news/pr20230928/\n\nhttps://tech.preferred.jp/ja/blog/llm-plamo/\n\nhttps://huggingface.co/pfnet/plamo-13b\n\n```py\n%pip install numpy safetensors sentencepiece torch transformers\n```\n\n```py\nimport transformers\npipeline = transformers.pipeline(\"text-generation\", model=\"pfnet/plamo-13b\", trust_remote_code=True)\nprint(pipeline(\"The future of artificial intelligence technology is \", max_new_tokens=32))\n```\n\n最初はこれで以下のエラーになりました。\n\n```\nAttributeError: module 'torch.nn.functional' has no attribute 'scaled_dot_product_attention'\n```\n\nこちらを参考に`torch`のバージョンを上げたら解消しました。\n\nhttps://huggingface.co/tiiuae/falcon-40b/discussions/12\n\n```py\n%pip install --upgrade torch\ndbutils.library.restartPython()\n```\n\n再度実行。\n\n```py\nimport transformers\npipeline = transformers.pipeline(\"text-generation\", model=\"pfnet/plamo-13b\", trust_remote_code=True)\nprint(pipeline(\"The future of artificial intelligence technology is \", max_new_tokens=32))\n```\n\n動きました。\n\n```\n[{'generated_text': 'The future of artificial intelligence technology is fascinating, but it’s also scary. There are many scenarios that could play out, and there’s'}]\n```\n\n日本語でも。\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"pfnet/plamo-13b\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"pfnet/plamo-13b\", trust_remote_code=True)\ntext = \"これからの人工知能技術は\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\ngenerated_tokens = model.generate(\n    inputs=input_ids,\n    max_new_tokens=32,\n    do_sample=True,\n    top_k=50,\n    top_p=0.95,\n    temperature=1.0,\n)[0]\ngenerated_text = tokenizer.decode(generated_tokens)\nprint(generated_text)\n```\n\n```\nこれからの人工知能技術は、その発展が見込まれます。しかしそれだけではなく、人工知能が進化することによって、社会のシステムを根本から変えてしまうようなことも起こり得ると、筆者は\n```\n\n恒例の。\n\n```py\ndef gen_text(text):\n  input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n  generated_tokens = model.generate(\n    inputs=input_ids,\n    max_new_tokens=32,\n    do_sample=True,\n    top_k=50,\n    top_p=0.95,\n    temperature=1.0,\n  )[0]\n  generated_text = tokenizer.decode(generated_tokens)\n  return generated_text\n```\n\n```py\nprint(gen_text(\"Databricksとは\"))\n```\n\n> Databricksとは、AIと分析を、ビジネスの最前線で活用するためのソフトウェアとサポートを提供する企業である。Databricksは、世界をリードするテクノロジーとデータ・プロバイダ\n\n```py\nprint(gen_text(\"What is Databricks?\"))\n```\n\n> Databricks is a privately held software company which was founded in 2013 by the creators of Apache Spark, and is based in\n\n日英対応というのはいいですね。活用の幅が広がりそうです。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "Databricksにおけるモデルサービングエンドポイントの推論テーブル", "text": "[Inference tables for monitoring model serving endpoints \\| Databricks on AWS](https://docs.databricks.com/en/machine-learning/model-serving/inference-tables.html) [2023/9/27時点]の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\n:::note info\n**プレビュー**\n本機能は[パブリックプレビュー](https://docs.databricks.com/release-notes/release-types.html)です。\n:::\n\n本書では、モデルサービングエンドポイントをモニタリングするための推論テーブルを説明し、期待されるアウトプットと特徴量の制限の詳細を説明します。\n\n# 推論テーブルとは？\n\n推論テーブルは、お使いの[モデルサービングエンドポイント](https://docs.databricks.com/ja/machine-learning/model-serving/index.html)におけるリクエストとレスポンスを自動で捕捉し、Unity CatalogのDeltaテーブルに記録します。MLモデルを監視、デバッグ、改善するためにこのテーブルのデータを後で活用することができます。\n\n例えば、推論テーブルを活用することで以下のようなことを達成することができます:\n\n- お使いのモデルの次のイテレーションの再トレーニングデータセットの作成。\n- プロダクションのデータとモデルの品質モニタリングの実施。\n- 疑わしい推論の診断とデバッグの実行。\n- 再ロードされるべき誤ったラベルのデータの作成。\n\nパブリックプレビューに参加するには、[inference tables preview enrollment form](https://docs.google.com/forms/d/14MGQZxxeLRI7ycmWcU5H1mx6o3AyW7cgucLlfnvha2s/edit)を提出してください。\n\n以下の図では、モデルサービングのワークフローにどのように推論テーブルがフィットするのかを示しています。\n![](https://docs.databricks.com/en/_images/inference-table-model-serving-workflow.png)\n\n# Unity Catalog推論テーブルのスキーマ\n\n推論テーブルに記録されるそれぞれのリクエストとレスポンスは、以下のスキーマでDeltaテーブルに書き込まれます:\n\n:::note\n**注意**\nバッチ入力でエンドポイントを呼び出した場合には、すべてのバッチは1行として記録されます。\n:::\n\n| カラム名 | 説明 | 型 |\n|:--|:--|:--|\n| `databricks_request_id`  | すべてのモデルサービングリクエストに付与されるDatabricks生成のリクエストID  | `STRING`  |\n| `client_request_id`  | モデルサービングのリクエストボディで指定できるオプションのクライアント生成のリクエストID。詳細は[client_request_idの指定](https://qiita.com/taka_yayoi/items/bce5df57ae5d90bdf971#client_request_id%E3%81%AE%E6%8C%87%E5%AE%9A)をご覧ください。  | `STRING`  |\n| `date`  | モデルサービングのリクエストを受信したUTC日付。  |  `DATE` |\n| `timestamp_ms`  | リクエストがモデルサービングエンドポイントに到達した時間を表現するエポックでのタイムスタンプ(ms)  | `LONG`  |\n| `status_code`  | モデルから返却されたHTTPステータスコード。  |  `INT` |\n| `sampling_fraction`  | リクエストがダウンサンプリングされた際に使用されるサンプリング率。0と1の値を取り、1が入力リクエストの100%が含まれることを意味します。  | `DOUBLE`  |\n| `execution_time_ms`  | モデルが推論を行う際に要した実行時間(ms)。ネットワークのレーテンシーオーバーヘッドは含まれず、モデルが予測を行う際に要した時間のみが含まれます。  |`LONG`   |\n| `request`  |  モデルサービングエンドポイントに送信された生のJSONボディ。 |  `STRING` |\n| `response`  | モデルサービングエンドポイントから返却された生のレスポンスJSONボディ。  | `STRING`  |\n| `request_metadata`  | リクエストに関連付けらえたモデルサービングエンドポイントに関するメタデータのマップ。このマップには、エンドポイント名、エンドポイントで使用されているモデル名、モデルバージョンが含まれます。  | `MAP<STRING, STRING>`  |\n\n# 制限\n\nこの機能には以下の制限があります:\n\n- 顧客管理キーはサポートされていません。\n- 推論テーブルはCPUエンドポイントでのみ動作します。モデルサービングGPUのパブリックプレビューに登録している場合には、カスタムソリューションに関してDatabricks担当者にお問い合わせください。\n- AWS PrivateLinkはデフォルトではサポートされません。有効化するにはDatabricks担当者にお問い合わせください。\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "JEDAI勉強会：生成AIとは何か？", "text": "こちらのイベントで私が話した内容のサマリーです。参加いただいた皆様ありがとうございました！\n\nhttps://jedai.connpass.com/event/293793/\n\n資料はこちらにアップロードしています。\n\nhttps://sajpstorage.blob.core.windows.net/yayoi/JEDAI%E5%8B%89%E5%BC%B7%E4%BC%9A%20%E7%AC%AC4%E5%9B%9E%20%E7%94%9F%E6%88%90AI%E3%81%A8%E3%81%AF%E4%BD%95%E3%81%8B(%E9%85%8D%E5%B8%83%E7%94%A8).pdf\n\nまた、途中で触れたQ&A botのサンプルノートブックです。\n\nhttps://github.com/taka-yayoi/public_repo_2/tree/master/diy-llm-qa-bot-jpn\n\n# 生成AIの基礎\n\n## 生成AIとは\n\n生成AIの前に人工知能、機械学習、ディープラーニングを説明させてください。\n![Screenshot 2023-09-28 at 8.20.09.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e6f9bd25-0ca8-a721-835a-1b3cfdf817ce.png)\n\n**人工知能**とは、人間レベルの知能を模倣し、人間を上回る能力を持つシステムの作成を狙いとした学際的なコンピューターサイエンスの領域です。\n\n**機械学習**とは、明示的にプログラムすることなしに既存のデータから学習し、予測を行う技術です。\n\n**ディープラーニング**とは、データからの学習に「人工ニューラルネットワーク」を活用する技術です。第三次AIブームのきっかけにもなりました。\n\nそして、**生成AI**はディープラーニングの一領域であり、さまざまなコンテンツの生成にフォーカスしています。\n![Screenshot 2023-09-28 at 8.21.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/743d4f34-ecba-44cf-9ec9-24659551af00.png)\n\n生成AIモデルは、数式的に世界を近似する機械学習モデリングの分派となります。\n![Screenshot 2023-09-28 at 8.22.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/0ebc8655-4b0e-3701-776e-4d5343d3634a.png)\n\nなぜ、今になって生成AIが実現されたのでしょうか？大きく3つの要因があります。\n![Screenshot 2023-09-28 at 8.26.26.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/7b24c6a4-0e1c-b9b3-bc00-99aa8a17c4c3.png)\n\n1. 大規模データセット\n    - LLMの学習で活用できるデータが潤沢に存在しています。\n1. 計算パワー\n    - かつては簡単にGPUを利用できませんでしたが、クラウドで簡単にさまざまなスペックのGPU、CPUを活用できるようになりました。\n1. 革新的なDLモデル\n    - GAN、トランスフォーマーアーキテクチャ、RLHFなどによって高い精度を示す生成モデルが実現されました。\n\n## なぜ、今生成AIを機にする必要があるのか？\n\n**生成AIモデルの精度や効果が転換点を超えました**\n\n- 1年前には不可能だったユースケースを実現するのに十分にパワフルです\n- 技術者ではないビジネスユーザーが利用するにも十分に経済的です\n\n**生成AIモデルとツールがすでに利用可能です**\n\n- 多くのモデルはオープンソースでありカスタマイズできます \n- パワフルなGPUが必要ですが、クラウドで利用できます\n\n## 生成AIのユースケース\n\n知性のある会話、クリエイティブなテキスト、コード生成などが可能です。\n\n- コンテンツ生成\n- 質問/回答\n- バーチャルアシスタント \n- コンテンツのパーソナライゼーション\n- 言語スタイルの転換\n- ストーリーテリング、詩、クリエイティブなライティング\n- 翻訳\n- コード生成 / オートコンプリート\n\n\n# LLMと生成AI\n\n## LLMとは\n\n![Screenshot 2023-09-28 at 8.33.45.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/b1f320f1-e60c-dc32-5341-7d7a1db75b55.png)\n\n**大規模言語モデル(LLM)** とは、高度な言語処理能力を達成するために膨大なデータセットでトレーニングされたモデルです。\n\n**基盤モデル**とは、膨大な量のデータでトレーニングされ、特定の言語理解や生成タスクのためにファインチューンされた大規模MLモデルです。\n\nトレーニング対象のテキストをトークナイズ(分かち書き)し、エンべディング(多次元ベクトル)に変換します。その後、ある単語の次に出現する単語を予測するようにトレーニングを行いLLMを構築します。\n![Screenshot 2023-09-28 at 8.34.51.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/e8f143f1-8e2a-e936-3949-1c7f7794d7f8.png)\n\nLLMは言語にまつわる様々なタスクで活用できます。\n![Screenshot 2023-09-28 at 8.37.16.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/4a32a10d-d5d8-6a7a-aa87-4554acb529d2.png)\n\n単にやり取りを行うだけでなく、システムに組み込むことで自動化、効率化が可能です。\n![Screenshot 2023-09-28 at 8.37.55.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/440b11f2-1cd7-037a-5c24-25fdd7987cb1.png)\n\n# LLMの活用\n\n田辺三菱製薬様に以下の事例をご紹介いただきました。すべてがシステムでの活用を前提としており非常に興味深く、セキュリティやコストなどに関して多くの質問をいただきました。\n\n1. テーブル自動作成→メタデータ自動設定\n1. インシデント通知内容の翻訳および要約、今後の攻撃想定\n1. ヘルプデスクQABot化\n1. リアルタイム翻訳・要約\n\nまた、上記取り組みでは弊社[Databricks](https://www.databricks.com/jp)を積極的にご活用いただいており、非常に感謝です。QABotであれば1日で稼働まで持っていけるとの発言も。\n\n# 生成AIの潜在的なリスクと課題\n\n以下のような点をカバーしました。こちらは技術的な話から外れますので、詳細は割愛します。[資料](https://sajpstorage.blob.core.windows.net/yayoi/JEDAI%E5%8B%89%E5%BC%B7%E4%BC%9A%20%E7%AC%AC4%E5%9B%9E%20%E7%94%9F%E6%88%90AI%E3%81%A8%E3%81%AF%E4%BD%95%E3%81%8B(%E9%85%8D%E5%B8%83%E7%94%A8).pdf)をご覧ください。\n\n- 法律的な問題\n    - プライバシー\n    - セキュリティ\n    - 知的財産の保護\n- 倫理的な問題\n    - バイアス\n    - 間違った情報\n- 社会的/環境的問題\n    - 労働力へのインパクト\n    - 環境への影響\n\n# まとめ\n\n私からは生成AIの概要を説明させていただきましたが、田辺三菱製薬様からは非常に実践的な取り組みをご紹介いただき多くの質疑応答があり、大変有意義な会となりました。また、当日は投票機能を用いて、参加者のロール、生成AIの活用状況、開発形態などをヒアリングさせていただき、こちらも大変興味深い結果となりました。\n![Screenshot 2023-09-28 at 8.42.57.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1168882/505db2b4-f5cf-70ae-f3ad-22062591f71d.png)\n\n次回も楽しみにしていてください！\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
{"title": "メディア & エンターテイメントにおける大規模言語モデルの活用", "text": "[Large Language Models in Media & Entertainment \\| Databricks Blog](https://www.databricks.com/blog/large-language-models-media-entertainment)の翻訳です。\n\n:::note warn\n本書は抄訳であり内容の正確性を保証するものではありません。正確な内容に関しては原文を参照ください。\n:::\n\nメディア & エンターテイメント業界は、データを中心とした変革の真っ只中におり、お客様を全ての体験の中心に据えようとしています。すべてのサイズのすべての企業は、大規模な1対1体験を支援する次世代の破壊的イノベーションを提供するクエストの中にいます。特にある技術はこのゲームを根本的に変革するパワーを持っています。それが大規模言語モデル(LLM)です。LLMはユニークなコンテンツを生成する能力を持つだけでなく、複雑な情報を深く理解し、人間のようなインタラクションを模する能力の有しています。この記事では、現実世界のインパクトをもたらし、新たな収益のストリームを解放するためにメディア、エンターテイメント業界のデータ、AIリーダーが実現できる3つの主要な領域、パーソナライゼーション、マネタイゼーション、コンテンツ生成の領域におけるLLMの革新的ポテンシャルを探索します。\n\n# パーソナライゼーションの再定義\n\nパーソナライゼーションは、どの場所、いつでも、いかなるチャネルでよりスマートな体験を提供するため、メディア & エンターテイメントのリーダーの成功には欠かせないものとなっています。LLMはリアルタイムかつ非常に適切な体験で聴衆を魅了する、高度にパーソナライズされた体験を提供するための革新的なアプローチを提供します。例えば、ストリーミングメディアを例に取りましょう。LLMで強化された会話型のインタフェースによって、コンテンツプラットフォームは自然言語のプロンプトを通じてユーザーにエンゲージすることができ、よりパーソナライズされたレコメンデーション、インテリジェントな検索機能、リアルタイムで行えるテーラーメードのコンテンツ発見を実現します。ユーザーの嗜好、挙動、文脈のヒントを理解することで、LLMはユーザーが今後も戻ってくるように、よりスマートで没入感のあるメデイア体験を提供できるよう企業を支援します。\n\nまた、LLMはお客様がカスタマーサービスやサポートとエンゲージする方法を変革させる際に重要な役割を担います。AIでガイドされたインタラクションを通じて、顧客の問い合わせをより効率的に解決するために、LLMはコールセンターのエージェントをアシストすることができます。これは、顧客満足度を改善し、対応平均時間を削減し、膨大なコストを要する人間によるサービスからよりセルフサービスなものへと移行しようとしている通信会社の大きなフォーカスエリアとなっています。オープン/未解決の問題を分析し、以前のやり取りから洞察を導き出すことで、LLMは顧客満足度を高めるための適切な情報と推奨事項でエージェントを支援することができます。カスタマーサポートプロセスを円滑にするこの能力は、オペレーションの効率性を改善するだけではなく、顧客のロイヤリティも強化します。\n\n# インテリジェントなコンテンツの生成とキュレーション\n\nメディア & エンターテイメントにおいては、コンテンツは重力の中心に存在し、文字通りそれらの組織が顧客に売るものとなります。魅力的なコンテンツを作成し、キュレーションすることは、面白い瞬間を自分たちのより広範なコンテンツ作成戦略に取り込むために、コンテンツの作成者は社会的なシグナルのようなより多くのデータを活用するため、非常に多くの変更を体験することになります。そして、LLMはコンテンツを作成、キュレーションするためにデータとAIを活用する企業のアプローチを根本的に書き換える能力を有しています。例えば、LLMは興味深いヘッドラインの生成、魅力的なコピーの記述、コンテンツ品質に対するリアルタイムのフィードバックすら行うことが可能です。メディア企業は、コンテンツ作成プロセスを円滑にし、編集ワークフローを改善し、全体的なコンテンツの品質を改善し、すべてのピースが意図した聴衆に響くようにします。\n\nさらに、LLMはコンテンツのキュレーション、提案システムの自動化に長けています。ユーザーの嗜好、消費パターン、ソーシャルメディアのシグナルを分析することで、LLMはインテリジェントにパーソナライズされたコンテンツのプレイリスト、ニュースのダイジェスト、設られたレコメンデーションをキュレーションすることができます。これは、ユーザー満足度を改善するだけでなく、ユーザーの維持やロイヤリティを改善し、継続的なエンゲージメントやマネタイゼーションの可能性を高めます。\n\n# 強化されたエンゲージメントとマネタイゼーション\n\nコンテンツが王だとすれば、エンゲージメントは王国です。LLMはメディア & エンターテイメント企業がインタラクティブな体験を実現できるようにするだけでなく、既存の体験も強化します。例えば、LLMによって強化されたインタラクティブなストーリーテリングは参照者をパーソナライズされ、没入感のあるジャーニーに誘い、彼らの選択や嗜好がナラティブを形成します。ゲーミング、バーチャルリアリティ、スポーツ、インタラクティブな広告であろうとも、LLMはユーザーエンゲージメントやマネタイゼーションに対する新たな収益の扉を開きます。\n\nさらに、LLMはターゲットされた広告やマネタイゼーションの大きな機会をもたらします。膨大な量のユーザーデータを分析することで、LLMは正確な広告ターゲティングやコンテンツのレコメンデーションを伝える洞察を生成することができます。このレベルのパーソナライゼーションは、広告キャンペーンの効果を高め、より高いエンゲージメントを実現し、収益を最大化します。さらに、LLMは顧客の挙動、市場のトレンド、競争優位性のダイナミクスを分析することで、メディア企業による値付けの戦略の最適化を支援することができ、個々の嗜好と予算と協調するターゲティングされたサブスクリプションプランを提供することができます。\n\n# 始めるには\n\nこれらの例は、LLMがこの業界で価値を解放することができる数多くある領域の一部です。いつもと同じように、ビジネスが解き放つのは、企業がこの技術を競合との差別化を図るために、どのようにこの技術をオーケストレーションするのかに依存します。ここで、LLMに対するオープンソースのアプローチが、企業が自身のデータと知的財産のコントロールを維持できるようにすることで価値創生に至る持続性のあるパスを提供し、業界固有の文脈やユースケースにモデルを最適化する柔軟性を提供し、関連ビジネスの成果でスケールするそれらの機能を提供するためのアーキテクチャを設計を可能にします。\n\n詳細を知りたいですか？メディア & エンターテイメント向けレイクハウスに関しては[我々のサイト](https://www.databricks.com/solutions/industries/media-and-entertainment)を訪問してください。あるいはウェビナー[Build Your Own Large Language Model Like Dolly](https://www.databricks.com/resources/webinar/build-your-own-large-language-model-dolly)でどのようにご自身でLLMを活用できるのかについて学んでください。\n\n詳細は[Lakehouse for Media & Entertainment](https://www.databricks.com/solutions/industries/media-and-entertainment)をご覧ください。\n\n\n### Databricksクイックスタートガイド\n\n[Databricksクイックスタートガイド](https://www.amazon.co.jp/dp/B09V1YXFVQ/)\n\n\n### Databricks無料トライアル\n\n[Databricks無料トライアル](https://databricks.com/jp/try-databricks)\n"}
